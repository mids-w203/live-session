# Summarizing Distributions

In the last live session, we introduced random variables; probability density and cumulative density; and, made the connection between joint, marginal, and conditional distributions. All of these concepts work with the **entire** distribution. 

Take, for example, the idea of conditional probability. We noted that conditional probability is defined to be: 

\[
  f_{Y|X}(y|x) = \frac{f_{Y,X}(y,x)}{f_{X}(x)}
\]

This is a powerful concept that shows a lot of the range of the reasoning system that we've built to this point! The probability distribution of $Y$ might change as a result of changes in $X$. If you unpack that just a little bit more, we might say that $f_{Y}(y)$ -- the probability density of $Y$ -- which is itself a function, is *also* a function of $X$. To say it again, to be very explicit: the function is a function of another input. That might sound wild, but it is all perfectly consistent with the world that we've built to this point. 

This concept is **very** expressive. Knowing $f_{Y}(y)$ gives a full information representation of a variable; knowing $f_{Y|X}(y|x)$ lets you update that information to make an even more informative statement about $Y$. In *Foundations* and at this point in the class, we deal only with conditional probability conditioning on a single variable, but the process generalizes. So, if there were four random variables, $A, B, C, D$, we could make a statement about $A$ that conditions on $B, C, D$: 

\[
  f_{A|\{B,C,D\}}(a|\{b,c,d\}) = \frac{f_{A,B,C,D}(a,b,c,d)}{f_{B,C,D}(b,c,d)}
\]

In this week's materials we are going to go in the *opposite* direction: Rather than producing a very expressive system of probabilities, we're going to attempt to summarize all of the information contained in a pdf into lower-dimensional representations. Our first task will be summarizing a single random variable in two ways: 

1. Where is the "center" of the random variable; and, 
2. How dispersed, "on average" is the random variable from this center. 

After developing the concepts of *expectation* and *variance* (which are 1 & 2 above, respectively), we will develop a summary of a joint distribution: the *covariance*. The particular definitions that we choose to call expectation, variance, and covariance require justification. Why should we use these *particular* formulae as measures of the "center" and "dispersion"?

We ground these summaries in the **Mean Squared Error** evaluative metric, as well as justifying this metric.

```{r load unit 03 packages, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
```

![a majestic valley](./images/yosemite.jpg)

## Learning Objectives 

At the end of the live session and homework this week, students will be able to: 

1. **Understand** the importance of thinking in terms of random variables, while; 
2. Being able to **appreciate** that it is not typically possible to fully model the world with a single function. 
3. **Articulate** why we need a target for a model, and propose several possible such targets.
4. **Justify** why expectation is a good model, why variance is a reasonable model, and how covariance relates two-random variables with a common joint distribution.
4. **Produce** summaries of location and relationship given a particular functional form for a random variable.

## Class Announcements 

Where have we come from, and where are we going? 

### What is in the rearview mirror? 

::: {.slide}
- Statisticians create a population model to represent the world; random variables are the building blocks of such a model.
- We can describe the distribution of a random variable using:
    - A *CDF* for all random variables
    - A *PMF* for discrete random variables
    - A *PDF* for continuous random variables
- When we have multiple random variables,
    - The joint PMF/PDF describes how they behave together
    - The marginal PMF/PDF describes one variable in isolation
    - The conditional PMF/PDF describes one variable given the value of another
:::    

### Today's Lesson

What might seem frustrating about this probability theory system of reasoning is that we are building a castle in the sky -- a fiction. We're supposing that there is some function that describes the probability that values are generated. In reality, there is no such generative function; it is *extremely unlikely* (though we'll acknowledge that it is possible) that the physical reality we believe we exist within is just a complex simulation that has been programmed with functions by some unknown designer. 

Especially frustrating is that we're supposing this function, and then we're further saying, 

> "If only we had this impossible function; and if only we also had the ability to take an impossible derivative of this impossible function, then we could..." 

#### Single number summaries of a single random variable

But, here's the upshot! 

**What we are doing today is laying the baseline for models that we will introduce next week.** Here, we are going to suggest that there are radical simplifications that we can produce that hold specific guarantees, no matter how complex the function that we're reasoning about. 

In particular, in one specific usage of the term *best* we will prove that the Expectation operation is the best one-number summary of any distribution. To do so, we will define a term, *variance*, which is the squared deviations from the expectation of a variable that describes how "spread out" is a variable. Then, we will define a concept that is the *mean squared error* that is the square of the distance between a model prediction and a random variable's realization. The key realization is that when the model predicts the expectation, then the MSE is equal to the variance of the random variable, which is the smallest possible value it could realize. 

#### Single number summaries of relationships between random variables

Although the single number summaries are **incredibly** powerful, that's not enough for today's lesson! We're also going to suggest that we can create a measure of linear dependence between two variables that we call the "covariance", and a related, re-scaled version of this relationship that is called the correlation. 

### Future Attractions 

- A predictor is a function that provides a value for one variable, given values of some others.
- Using our summary tools, we will define a predictor's error and then minimize it.
- This is a basis for linear regression

## Discussion of Terms 

::: {.discussion-question} 
With your instructor, talk about what each of the following definitions mean in your own words. For key concepts, you might also formalize this intuition into a formula that can be computed. 

- Expected Value, or Expectation 
- Central Moments $\rightarrow$ Variance $\rightarrow$ Standard Deviation
- Set aside for later: Chebyshev's Inequality and the Normal Distribution 
- Mean Squared Error and its alternative formula 
- Covariance and Correlation
:::

## Computing Examples 

### Expected Value of Education [discrete random variable]

- The expected value of a discrete random variable $X$ is the weighted average of the values in the range of $X$.
- Suppose that $X$ represents the number of years of education that someone has completed, and so has a support that ranges from $0$ years of education, up to $28$ years of education. (Incidentally, Mark Labovitz has about 28 years of education.) 
- You can then think of 

```{r, echo = FALSE}
set.seed(1)

population_education <- sample(
  x    = 0:28, 
  size = 2000, 
  replace = TRUE, 
  prob = ((c(1:10, rep(10, 8), 11:1))^2 / sum((29:1)^2))
  ## pretty fancy probability distribution, eh? ^^ 
)

ggplot() + 
  aes(x=population_education) + 
  geom_histogram(bins = 28, fill = '#003262') + 
  labs(
    title = 'Years of Education', 
    x     = 'Years', 
    y     = 'Number of Instances')
```

without using specific numbers, describe the process you would use to calculate the expected value of this distribution. 

### Using a formula 

How does the following formula match with your concept of expectation from that last section?

\[
  \begin{aligned}
  E[X] &= \sum_{x \in \{years\ of\ edu\}} x \cdot f(x) \\ 
       &= \sum_{x=0}^{x=28} x\cdot P(X=x)
  \end{aligned}
\]

## Computing by Hand
### Compute the Expected Value 

Let $X$ represent the result of one roll of a 6 sided die. 

- Calculating by hand, what is the expected value $X$? 

> Fill this in by hand. 

```{r solution, echo = FALSE}
## For a fair die, we know that the pmf is: 
##  $f(x) = 1/6$ for x in {1,2,3,4,5,6}. 
## The $E[X]$ is therefore 1/6*(1+2+3+4+5+6) = 3.5.
```

### Compute the Variance 

Let $X$ represent the result of one roll of a 6 sided die. 

- Calculating by hand, what is the variance of $X$? 

## Expected Value by Code 

### Expected Value of a Six-Sided Die

Let $X$ represent the result of one roll of a 6 sided die. 

- Build an object to represent the whole sample space, $\Omega$ of a six sided die. 
- Determine what probabilities to assign to each value of that object. 
- Write the code to run the expectation algorithm that you just performed by hand. 

```{r}
die <- data.frame(
    value = 'fill this in',
    prob  = 'fill this in'
  )
```

### Variacne of a Six-Sided Die

Let $X$ represent the result of one roll of a 6 sided die. 

5. Using what you know about the definition of variance, write a function that will compute the variance of your `die` object. 

```{r}
variance_function <- function(die) { 
  ## fill this in
  mu = 'fill this in'   ## you should index to the correct column
  var = 'fill this in'  ## for each, and use the correct function
  
  return(var)
}

variance_function(die)
```

Suppose that you had to keep the values the same on the die, that is. That is the domain of the outcome still had to be the countable set of integers from one to six. 

- How would you change the probability distribution to decrease the variance of this random variable? 
- What is the smallest value that you can generate for this random variable? Use the `variance_function` from above to actually compute this variance. 
- What is the largest value of variance that you can generate for this random variable? Use the `variance_function` from above to actually compute this variance. 

Now suppose that you again had an equal probability of every outcome, but you were to apply a function to the number of spots that are showing on the die. Rather that each dot contributing one value to the random variable, instead the random variable's outcome is the square of the number of spots. 

- How would this change the mean?  
- How would this change the variance? 

## Practice Computing 

### Single Variable 

Suppose that $X$ has the following density function: 

\[ 
  f_{X}(x) = \begin{cases} 
    6x(1 - x), & 0 < x < 1 \\ 
    0, & otherwise \\ 
  \end{cases}
\]

- Find $E[X]$.
- Find $E[X^2]$. 
- Find $V[X]$. 

### Joint Density 

Suppose that $X$ and $Y$ have joint density $f_{X,Y}(x,y) = 10 x^2y$ for $0 < y < x < 1. $ 

- Find $E[X]$
- Find $E[EY]$
- Find $\rho[X,Y]$

## Write Code 

Suppose that you have a random variable with a **gnarly** probability distribution function: 

\[ 
  f_{X}(x) = \frac{3*\left(x - 2x^2 + x^3\right)}{2}, 0\leq x\leq 2
\]

If you had to pick a single value that minimizes the MSE of this function, what would it be? 

- First, how would you approach this problem *analytically*. By this, we mean, "how would you solve this with the closed form answer? 
- Second, how might you approach this problem *computationally*. By this, we mean, "how might you write code that would produce a numeric approximation of the closed form solution?" 

```{r}
pdf_func <- function(x) { 
  3*(x - (2*x^2) + x^3)/2
}
```

```{r}
input_range <- seq(from=0, to=2, by=0.01)

mean_absolute_error <- Vectorize(
  function(c) { 
    x_values <- pdf_func(input_range)
    mae_     <- mean(abs(x_values - c))
    }
)

mean_square_error <- Vectorize(
  function(c) { 
    x_values <- pdf_func(input_range)
    mse_     <- mean((x_values - c)^2)
    return(mse_)
  }
)

mean_cubic_error <- Vectorize(
  function(c) { 
    x_values <- pdf_func(input_range) 
    mce_     <- mean((x_values - c)^3)
    }
)

mean_quadratic_error <- Vectorize(
  function(c) { 
    x_values <- pdf_func(input_range)
    mqe_     <- mean((x_values - c)^4)
    return(mqe_)
  }
)

mean_power_error <- Vectorize(
  function(c, power) { 
    x_values <- pdf_func(input_range)
    m_power_e_     <- mean((x_values - c)^power)
    return(m_power_e_)
    }
)

mae_ <- mean_absolute_error(
  c = input_range
)
mse_ <- mean_square_error(
  c = input_range
  )
mce_ <- mean_cubic_error(
  c = input_range
)
mqe_ <- mean_quadratic_error(
  c = input_range
)

plot(
  x = input_range,
  y = mse_,
  type = 'l'
)
lines(
  x = input_range, 
  y = mae_, 
  type = 'l', 
  col = '#003262'
)
lines(
  x = input_range, 
  y = mqe_, 
  type = 'l', 
  col = '#FDB515'
)
# lines(
#   x = input_range, 
#   y = mce_, 
#   type = 'l', 
#   col = '#FDB515'
# )
abline(v=1.5, col = 'darkred')
```

```{r}
optim(
  par = 0, 
  fn = mean_square_error, 
  method = 'Brent', 
  lower = 0, upper = 2
)
```

