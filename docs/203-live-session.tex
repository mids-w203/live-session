% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Statistics for Data Science},
  pdfauthor={D. Alex Hughes, Paul Laskowski \& The 203 Teaching Team},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{color}
\usepackage{tcolorbox}

\definecolor{berkeleyblue}{HTML}{003262}
\definecolor{berkeleygold}{HTML}{FDB515}
\definecolor{bayfog}{HTML}{DDD5C7}
\definecolor{laplane}{HTML}{00A598}
\definecolor{ion}{HTML}{CFDD45}
\definecolor{sathergate}{HTML}{B9D3B6}
\definecolor{stonepine}{HTML}{584F29}

\newtcolorbox{breakout}{
  colback=berkeleyblue,
  colframe=berkeleygold,
  coltext=white,
  boxsep=5pt,
  arc=4pt
}

\newtcolorbox{discussion-question}{
  colback=bayfog,
  colframe=laplane,
  coltext=laplane,
  boxsep=5pt,
  arc=4pt
}

% \newtcolorbox{definition}{
%   colback=ion,
%   colframe=stonepine,
%   coltext=stonepine,
%   boxsep=5pt,
%   arc=4pt
% }

% \newtcolorbox{theorem}{
%   colback=sathergate,
%   colframe=stonepine,
%   coltext=stonepine,
%   boxsep=5pt,
%   arc=4pt
% }
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi

\title{Statistics for Data Science}
\author{D. Alex Hughes, Paul Laskowski \& The 203 Teaching Team\footnote{UC Berkeley, School of Information}}
\date{2023-01-17}

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
\newtheorem{corollary}{Corollary}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\newtheorem{conjecture}{Conjecture}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\theoremstyle{definition}
\newtheorem{example}{Example}[chapter]
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}[chapter]
\theoremstyle{definition}
\newtheorem{hypothesis}{Hypothesis}[chapter]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{live-session}{%
\chapter*{Live Session}\label{live-session}}
\addcontentsline{toc}{chapter}{Live Session}

\includegraphics{./images/campus.jpeg}

This is the live session work space for the course. Our goal with this repository, is that we're able to communicate \emph{ahead of time} our aims for each week, and that you can prepare accordingly.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(mids203)}
\end{Highlighting}
\end{Shaded}

\hypertarget{probability-spaces}{%
\chapter{Probability Spaces}\label{probability-spaces}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{source}\NormalTok{(}\StringTok{\textquotesingle{}./src/blank\_lines.R\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Probability is a system of reasoning about the world in the face of incomplete information. In this course, we're going to develop an understanding of the implications of core parts of this theory, how this theory was developed, and how these implications relate to every other part of the practice of data science.

\begin{figure}
\centering
\includegraphics{./images/webb.jpg}
\caption{probability, the final frontier}
\end{figure}

\hypertarget{learning-objectives}{%
\section{Learning Objectives}\label{learning-objectives}}

At the end of this week's learning, student will be able to:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Find} and \emph{access} all of the course materials;
\item
  \textbf{Develop} a course of study that is builds toward success;
\item
  \textbf{Apply} the axioms of probability to make a valid statement;
\item
  \textbf{Solve} word problems through the \emph{application} of probability and math rules.
\end{enumerate}

\hypertarget{course-learning-objectives}{%
\section{Course Learning Objectives}\label{course-learning-objectives}}

At this point in the course, there is so much that is before us! As we settle in to study for the semester, it is useful to have a point of view of where we're trying to go, and what we are going to see along the way.

Allow a justification by analogy:

\begin{quote}
Suppose that you decide that you would like to be a chef -- all of the time watching cooking shows has revealed to you that this is your life's true calling -- and so you enroll in a culinary program.

One does not begin such a program by baking croissants and souffle. They begin the program with knife skills, breaking down ingredients and the basic techniques that build up to produce someone who is not a \emph{cook}, but a \emph{chef} -- someone who can combine ingredients and techniques to produce novel ideas.

At the same time, however, one has not gone to school just to become a cucumber slicer. The knife skills are instrumental to the eventual goal -- of being a chef -- but not the goal itself.
\end{quote}

At the beginning of the program, we're teaching these core, fundamental skills. How to read and reason with mathematical objects, how to use conditional probability with the goal of producing a model, and eventually, \textbf{eventually} to create novel work as a data scientist.

At the end of this course, students will be able to:

\hypertarget{understand-the-building-blocks-of-probability-theory-that-prepare-learners-for-the-study-of-statistical-models}{%
\subsection{Understand the building blocks of probability theory that prepare learners for the study of statistical models}\label{understand-the-building-blocks-of-probability-theory-that-prepare-learners-for-the-study-of-statistical-models}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Understand the mathematical objects of probability theory and be able to apply their properties.
\item
  Understand how high-level concepts from calculus and linear algebra are related to common procedures in data science.
\item
  Translate between problems that are defined in business or research terms into problems that can be solved with math.
\end{enumerate}

\hypertarget{understand-and-apply-statistical-models-in-common-situations}{%
\subsection{Understand and apply statistical models in common situations}\label{understand-and-apply-statistical-models-in-common-situations}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Understand the theory of statistics to prepare students for inferrential statements.
\item
  Understand model parameters and high level strategies to estimate them: means, least squares, and maximum likelihood.
\item
  Choose an appropriate statistic, and conduct a hypothesis test in the Neyman-Pearson framework.
\item
  Interpret the results of a statistical test, including statistical significance and practical significance.
\item
  Recognize limitations of the Neyman-Pearson hypothesis testing framework and be a conscientious participant in the scientific process
\end{enumerate}

\hypertarget{analyze-a-research-question-using-a-linear-regression-framework}{%
\subsection{Analyze a research question using a linear regression framework}\label{analyze-a-research-question-using-a-linear-regression-framework}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Explore and wrangle data with the intention of understanding the information and relationships that are (and are not) present
\item
  Identify the goals of your analysis
\item
  Build a model that achieves the goals of an analysis
\end{enumerate}

\hypertarget{interpret-the-results-of-a-model-and-communicate-them-in-manner-appropriate-to-the-audience}{%
\subsection{Interpret the results of a model and communicate them in manner appropriate to the audience}\label{interpret-the-results-of-a-model-and-communicate-them-in-manner-appropriate-to-the-audience}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Identify their audience and report process and findings in a manner appropriate to that audience.
\item
  Construct regression oriented reports that provide insight for stakeholders.
\item
  Construct technical documents of process and code for collaboration and reproducability with peer data scientists.
\item
  Read, understand, and assess the claims that are made in technical, regression oriented reports
\end{enumerate}

\hypertarget{contribute-proficient-basic-work-using-industry-standard-tools-and-coding-practices-to-a-modern-data-science-team.}{%
\subsection{Contribute proficient, basic work, using industry standard tools and coding practices to a modern data science team.}\label{contribute-proficient-basic-work-using-industry-standard-tools-and-coding-practices-to-a-modern-data-science-team.}}

Demonstrate programming proficiency by translating statistical problems into code.
1. Understand and incorporate best practices for coding style and data carpentry
2. Utilize industry standard tooling for collaboration

\hypertarget{introductions}{%
\section{Introductions}\label{introductions}}

\hypertarget{instructor-introductions}{%
\subsection{Instructor Introductions}\label{instructor-introductions}}

The instructors for the course come to the program, and to statistics from different backgrounds. Instructors hold PhDs in statistics, astrophysics, biology, political science, computer science, and information.

\hypertarget{what-does-a-statistician-look-like-you}{%
\subsection{What does a statistician look like? You!}\label{what-does-a-statistician-look-like-you}}

Identity shapes how people approach and understand their world.

We would like to acknowledge that we have limited diversity of identity among the instructors for this course. We each have been fortunate to be able to study, but we want to acknowledge that the education system in the US has systematically benefited the hegemonic groups and marginalized others voices.

Every one of the instructors shares a core identity as an empathetic educator that wants to understand your strengths, areas for growth, and unique point of view that is shaped by who you are. We want to see a field of data scientists who embrace each others voices, and respects people for the identies that they hold.

\begin{itemize}
\tightlist
\item
  It doesn't matter if you've never taken a stats class before, or if you're reviewing using this class. There will be challenges for everyone to overcome.
\item
  It doesn't matter how old or young you are. We will all be learning frequentist statistics which is timeless.
\item
  The color of your skin doesn't matter; nor does whether you identify as a woman or a man or trans or non-binary; neither does your sexual orientation. There are legacies of exclusion and discrimination against people due to these identities. We will not continue to propagate those legacies and instead will work to controvert those discriminations to build a diverse community of learning in line with the University's \href{https://diversity.berkeley.edu/principles-community}{Principles of Community}.
\end{itemize}

\hypertarget{student-introductions-breakout-one}{%
\section{Student Introductions {[}Breakout One{]}}\label{student-introductions-breakout-one}}

In a breakout room of between three and four students introduce yourself!

\begin{breakout}
\textbf{Breakout One.} A \emph{name story} is the unique, and individual story that describes how you came to have the name that you do. While there may be many people are called the same thing, each of their name stories is unique.

Please share: \emph{What is your name story?}

\end{breakout}

\hypertarget{student-introductions-breakout-two}{%
\section{Student Introductions {[}Breakout Two{]}}\label{student-introductions-breakout-two}}

In the same breakout room:

\begin{breakout}
\textbf{Breakout Two.}
Like our names, the reasons that we joined this program, our goals and our histories are different.

Please share: \emph{What is your data science story? How did you wind up here, in this room today?}

\end{breakout}

\hypertarget{probability-theory}{%
\section{Probability Theory}\label{probability-theory}}

\textbf{Probability}

Probability is a system of reasoning that we use to model the world under incomplete information. This model underlies virtually \emph{every} other model you'll ever use as a data scientist.

\begin{figure}
\centering
\includegraphics{./images/picard.jpg}
\caption{told you this would be spacey}
\end{figure}

In this course, probability theory builds out to random variables; when combined with sampling theory we are able to develop p-values (which are also random variables) and an inferential paradigm to communicate what we know and how certain a statement we can make about it.

In introduction to machine learning, literally the first model that you will train is a naive bayes classifier, which is an application of Bayes' Theorem, trained using an iterative fitting algorithm. Later in machine learning, you'll be fitting non-linear models, but at every point the input data that you are supplying to your models are generated from samples from random variables. That the world can be represented by random variables (which we will cover in the coming weeks) means that you can transform -- squeeze and smush, or stretch and pull -- variables to heighten different aspects of the variables to produce the most useful \emph{information} from your data.

As you move into NLP, you might think of generative text as a conditional probability problem: given some particular set of words as an input, what is the most likely \emph{next} word or words that someone might type?

Beyond the direct instrumental value that we see working with probability, there are two additional aims that we have in starting the course in the manner.

First, because we are starting with the axioms of probability as they apply to data science statistics, students in this course develop a \emph{much} fuller understanding of classical statistics than students in most other programs. Unfortunately, it is very common for students and then professionals to see statistics as a series of rules that have to be followed absolutely and without deviation. In this view of statistics, there are distributions to memorize; there are repeated problems to solve that require the rote application of some algebraic rule (i.e.~compute the sample average and standard deviation of some vector); and, there are myriad, byzantine statistical tests to memorize and apply. In this view of statistics, if the real-world problem that comes to you as a data scientist doesn't clearly fit into a box, there's no way to move forward.

\begin{quote}
Statistics like this is not fun.
\end{quote}

In the way that we are approaching this course, we hope that you're able to learn \emph{why} certain distributions (like the normal distribution) arise repeatedly, and why we can use them. We also hope that because you know how sampling theory and random variables combine, that you can be more creative and inventive to solve problems that you haven't seen before.

The second additional aim that we have for this course is that it can serve as either an introduction or a re-introduction to reading and making arguments using the language of math. For some, this will be a new language; for others, it may have been some years since they have worked with the language; for some, this will feel quite familiar. New algorithms and data science model advancements \emph{nearly always} developed in the math first, and then applied into algorithms second. In our view, being a literate reader of graduate- and professional-level math is a necessary skill for any data scientist that is going to keep astride of the field as it continues to develop and these first weeks of the course are designed to bring everyone back into reading and reasoning in the language.

\hypertarget{axiomatic-probability}{%
\section{Axiomatic Probability}\label{axiomatic-probability}}

The book makes a point of defining our axioms of probability, calling them them

\begin{definition}
\emph{Kolmogorov Axioms}

Let \(\Omega\) be a sample space, \(S\) be an event space, and \(P\) be a probability measure. Then, \((\Omega, S, P)\) is a \emph{probability space} if it satisfies the following:

\begin{itemize}
\tightlist
\item
  Non-negativity: \(\forall A \in S, P(A) \geq 0\), where \(P(A)\) is finite and real.
\item
  Unitarity: \(P(\Omega)=1\).
\item
  Countable additivity: if \(A_1, A_2, A_3, \dots \in S\) are pairwise disjoint, then
\end{itemize}

\[
P(A_1 \cup A_2 \cup A_3 \cup \dots) = P(A_1) + P(A_2) + P(A_3) = \sum_{i}P(A_{i})
\]
\end{definition}

There is a lot going on in this definition!

First things first, these are the \textbf{axioms of probability} (read aloud in the booming voice of a god).

This means that these are things that we begin from, sort of the foundational principles of the entire system of reasoning that we are going to use. In the style of argument that we're going to make, these are things that are sort of off-limits to question. Instead, these serve as the grounding assumptions, and we see what happens as we flow forward from these statements.

Second, and importantly, from these axioms there are a \emph{very large} set of things that we can build. The first set of things that we will build are probability statements about atomic outcomes (Theorem 1.1.4 in the book), and collections of events. But, these statements, are not the only thing that we're limited to. We can also build \emph{Frequentist Statistics}, and \emph{Bayesian Statistics} and \emph{Language Models}.

In many ways, these axioms are the fundamental particles that hold our system of probabilistic reasoning together. These are to probability what the \emph{fermions} and and \emph{bosons} are to physics.

\hypertarget{definition-vs.-theorem}{%
\section{Definition vs.~Theorem}\label{definition-vs.-theorem}}

What is the difference between a definition and a theorem? On pages 10 and 11 of the textbook, there is a rapid fire collection of pink boxes. We reproduce them here (notice that they may have different index numbers than the book -- this live session book autoindexes and we're not including every theorem and definition in this live session discussion guide).

\begin{definition}
\emph{Conditional Probability} For \(A, B \in S\) with \(P(B) > 0\), the \emph{conditional probablity} of \(A\) given \(B\) is \[P(A|B) = \frac{P(A\cap B)}{P(B)}.\]
\end{definition}

\begin{theorem}
\emph{Multiplicative Law of Probability} For \(A, B \in S\) with \(P(B) > 0\), \[P(A|B)P(B) = P(A \cap B)\]
\end{theorem}

\begin{theorem}
\emph{Baye's Rule} For \(A, B \in S\) with \(P(A) > 0\) and \(P(B) > 0\), \[P(A|B) = \frac{P(B|A)P(A)}{P(B)}.\]
\end{theorem}

\begin{discussion-question}

\begin{itemize}
\tightlist
\item
  What would happen to the statement of the \emph{Multiplicative Law of Probability} if we did not have the definition of \emph{Conditional Probability}?
\item
  How does one get from the definition, to the law?
\item
  Can one get to \emph{Baye's Rule} wihtout using the \emph{Multiplicative Law of Probability}?
\end{itemize}

\end{discussion-question}

\hypertarget{working-with-a-sample-space}{%
\section{Working with a Sample Space}\label{working-with-a-sample-space}}

As a way to begin lets define terms that we will use for the next activities.

\begin{discussion-question}

\textbf{Group Discussion Question}

\begin{itemize}
\tightlist
\item
  What is the definition of a sample space?
\item
  What is the definition of an event?
\item
  How are sample spaces, and event spaces related?
\end{itemize}

\end{discussion-question}

\hypertarget{working-with-a-sample-space-part-i}{%
\subsection{Working with a Sample Space, Part I}\label{working-with-a-sample-space-part-i}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{You roll two six-sided dice}:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    How would you define an appropriate sample space, \(\Omega\)?
  \item
    How many elements exist in \(\Omega\)?
  \item
    What is an appropriate event space, and how many elements does it have?
  \item
    Give an example of an event.
  \end{enumerate}
\end{enumerate}

\vspace{5cm}

\hypertarget{working-with-a-sample-space-part-ii}{%
\subsection{Working with a Sample Space, Part II}\label{working-with-a-sample-space-part-ii}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{For a random sample of 1,000 Berkeley students}:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    How would you define an appropriate sample space, \(\Omega\)?
  \item
    How big is \(\Omega\)? How many elements does it contain?
  \item
    What is an example of an event for this scenario?
  \item
    Can a single person be represented in the space twice? Why or why not?
  \end{enumerate}
\end{enumerate}

\vspace{5cm}

\hypertarget{independence}{%
\section{Independence}\label{independence}}

The book provides a (characteristically) terse statement of what it means for two events to be independent of one another.

\begin{definition}
\emph{Independence of Events} Events \(A, B \in S\) are \emph{independent} if \[P(A \cap B) = P(A)P(B)\].
\end{definition}

In your own words:

\begin{itemize}
\tightlist
\item
  What does it mean for two events to be independent of one another?
\item
  How do you \textbf{know} if two events are independent of one another?
\item
  How do you \textbf{test} if two events are independent of one another?
\end{itemize}

Try using this idea of independent in two places:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Suppose that you are creating a model to predict an outcome. Further, suppose that two events \(A\) and \(B\) are independent of one another. \emph{Can you use \(B\) to predict \(A\)}?
\item
  If two events, \(A\) and \(B\) are independent, then what happens if you work through a statement of conditional probability, \(P(A|B)\)?
\end{enumerate}

\hypertarget{a-practice-problem}{%
\section{A practice problem}\label{a-practice-problem}}

The last task for us to complete today is working through a practice problem on the course practice problem website. Please, click the link below, and follow us over to the the course's practice problem website.

\href{https://w203.herokuapp.com/problems/106}{link here}

\hypertarget{student-tasks-to-complete}{%
\section{Student Tasks to Complete}\label{student-tasks-to-complete}}

Before next live session, please complete the homework that builds on this unit. There are two parts, an \emph{applied} and a \emph{proof} part. You can submit these homework as many times as you like before the due date (you will not receive feedback), and you can access this homework through bCourses.

The \emph{applied} homework will be marked either \texttt{Correct} or \texttt{Incorrect} without partial credit applied. These are meant to be problems that you solve, and that have a single straightforward solution concept. The \emph{proof} homework will be marked for partial credit (out of three points) that evaluates your argument for your solution concept.

\hypertarget{defining-random-variables}{%
\chapter{Defining Random Variables}\label{defining-random-variables}}

\begin{figure}
\centering
\includegraphics{./images/yosemite.jpg}
\caption{yosemite valley}
\end{figure}

\hypertarget{learning-objectives-1}{%
\section{Learning Objectives}\label{learning-objectives-1}}

At the end of this week's course of study (which includes the async, sync, and homework) students should be able to

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Remember} that random variable are neither random, or variables, but instead that they are a foundational object that we can use to reason about a world.
\item
  \textbf{Understand} that the intuition developed by the use of set-theory probability maps into the more expressive space of random variables
\item
  \textbf{Apply} the appropriate mathematical transformations to move between joint, marginal, and conditional distributions.
\end{enumerate}

This week's materials are theoretical tooling to build toward one of the first notable results of the course, \textbf{conditional probability}. This is the idea that, if we know that one event has occurred, we can make a conditional statement about the probability distribution for another, dependent distribution.

\hypertarget{introduction-to-the-materirals}{%
\section{Introduction to the Materirals}\label{introduction-to-the-materirals}}

From the axioms of probability, it is possible to build a whole, expressive modeling system (that need not be grounded \textbf{at all} in the minutia of the world). With this probability model in place, we can describe how frequently events in the random variable will occur. When variable are dependent upon each other, we can utilize information that is encoded in this dependence in order to make predictions that are \emph{closer to the truth} than predictions made without this information.

There is both a beauty and a tragedy when reasoning about random variables: we describe random variables using their joint density function.

The \textbf{beauty} is that by reasoning with such general objects -- the definitions that we create, and the theorems that we derive in this section of the course -- produce guarantees that hold in every case, no matter the function that stands in for the joint density function. We will compute several examples of \emph{specific} functions to provide a chance to reason about these objects and how they ``work''.

The \textbf{tragedy} is that in the ``real world'', the world where we are going to eventually going to train and deploy our models, we are never provided with this joint density function. Perhaps this is the creation myth for probability theory: in a perfect world, we can produce a perfect result. But, in the ``fallen'' world of data, we will only be able to produce approximations.

\hypertarget{class-announcements}{%
\section{Class Announcements}\label{class-announcements}}

\hypertarget{homework}{%
\subsection*{Homework}\label{homework}}
\addcontentsline{toc}{subsection}{Homework}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  You should have turned in your first homework. The solution set for this homework is scheduled to be released to you in two days. The solution set contains a full explanation of how we solved the questions posed to you. You can expect that feedback for this homework will be released back to you within seven days.
\item
  You can start working on your second homework when we are out of this class.
\end{enumerate}

\hypertarget{study-groups}{%
\subsection*{Study Groups}\label{study-groups}}
\addcontentsline{toc}{subsection}{Study Groups}

It is a \textbf{very} good idea for you to create a recurring time to work with a set of your classmates. Working together will help you solve questions more effectively, quickly, and will also help you to learn how to communicate what you do and do not understand about a problem to a group of collaborating data scientists. And, working together with a group will help you to find people who share data science interests with you.

\hypertarget{course-resources}{%
\subsection*{Course Resources}\label{course-resources}}
\addcontentsline{toc}{subsection}{Course Resources}

There are several resources to support your learning. A learning object last week was that you would be introduced to each of these systems. Please continue to make sure that you have access to the:

\begin{itemize}
\tightlist
\item
  \href{https://www.lib.berkeley.edu/using-the-libraries/vpn}{Library VPN} to read all of the scholarly content in the known universe, including the course textbook.
\item
  \href{https://www.bcourses.berkeley.edu}{Course LMS Page}
\end{itemize}

\hypertarget{using-definitions-of-random-variables}{%
\section{Using Definitions of Random Variables}\label{using-definitions-of-random-variables}}

\hypertarget{random-varaible}{%
\subsection{Random Varaible}\label{random-varaible}}

What is a random variable? Does this definition help you?

\begin{definition}[Random Variable]
A random variable is a function \(X : \Omega \rightarrow \mathbb{R},\) such that \(\forall r \in \mathbb{R}, \{\omega \in \Omega: X(\omega) \leq r\} \in S\).
\end{definition}

Someone, please, read that without using a single ``omega'', \(\mathbb{R}\), or other jargon terminology. Instead, someone read this aloud and tell us what each of the concepts mean.

The goal of writing with math symbols like this is to be \emph{absolutely} clear what concepts the author does and does not mean to invoke when they write a definition or a theorem. In a very real sense, this is a language that has specific meaning attached to specific symbols; there is a correspondence between the mathematical language and each of our home languages, but exactly what the relationship is needs to be defined into each student's home language.

\begin{itemize}
\tightlist
\item
  What are the key things that random variables allow you to accomplish?

  \begin{itemize}
  \tightlist
  \item
    Suppose that you were going to try to make a model that predicts the probability of winning ``big money'' on a slot machine. Big money might be that you get :cherries: :cherries: :cherries:. Can you do \emph{math} with :cherries:?
  \item
    Suppose that you wanted to build a chatbort that uses a language model so that you don't have to do your homework anymore. How would you go about it?
  \item
    Suppose you want to direct class support to students in 203, but their grades are scored \texttt{{[}A,\ A-,\ ...,\ {]}} and features include prior statistics classes grades, also scored \texttt{A,\ A-,\ ...{]}}
  \end{itemize}
\end{itemize}

\hypertarget{pieces-of-a-random-variable}{%
\section{Pieces of a Random Variable}\label{pieces-of-a-random-variable}}

\begin{definition}[Random Variable, Suite]
A random variable is a function \(X : \Omega \rightarrow \mathbb{R},\) such that \(\forall r \in \mathbb{R}, \{\omega \in \Omega\}: X(\omega) \leq r\} \in S\).
\end{definition}

There are two key pieces that must exist for every random variable. What are these pieces? The first of these pieces is provided to us in \textbf{Definition 1.2.1} \emph{Random Variable} (on page 16). The second is provided to us in \textbf{Definition 1.2.5} \emph{Probability Mass Function} (on page 18).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
\item
\end{enumerate}

Suppose that a random variable is simple and discrete. For concreteness, you could think of this random variable as the answer to the question, ``Is the grass wet outside?''.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What is the sample space?
\item
  What is a sensible function that you might use to map from the sample space to real values?
\item
  What is a insensible function that you might use to map from the sample space to real values? (A student well-seasoned in Maths might use (and define for the rest of the class) the concept of a \emph{bijective function}).
\item
  If you simply had the values that the random variable function maps to are you guaranteed to be able to describe the entire sample space? Why or why not?
\item
  How would you go about determining the probability mass function for this random variable?
\end{enumerate}

\hypertarget{functions-of-functions}{%
\subsection{Functions of Functions}\label{functions-of-functions}}

Why do we say that random variables are functions? Is there some useful property of these being functions rather than any other quantity? What else \emph{could} they be if not a function?

What about a function of a random variable, which is a function of a function.

\begin{definition}[Function of a Random Variable]
Let \(g : U \rightarrow \mathbb{R}\) be some function, where \(X(\Omega) \subset U \subset \mathbb{R}\). Then, if \(g \circ X : \Omega \rightarrow \mathbb{R}\) is a random variable, we say that \(g\) is a \emph{function} of X and write \(g(X)\) to denote the random variable \(g \circ X\).
\end{definition}

If a random variable is a function from the real world, or the sample space, or the outcome space to a real number, then what does it mean to define a function of a random variable?

\begin{itemize}
\tightlist
\item
  At what point does this function work? Does this function change the sample space that is possible to observe? Or, does this function change the real-number that each outcome points to?
\end{itemize}

\begin{example}[MNIST]

Suppose that you are doing some image processing work. To keep things simple, that you are doing image classification in the style of the MNIST dataset.

\begin{itemize}
\tightlist
\item
  Can someone describe what this task is trying to accomplish?
\item
  Has anyone done work like this?
\end{itemize}

However, suppose that rather than having good clean indicators for whether a pixel is on or off, instead you have weak indicators -- there's a lot of grey. A lot of the cells are marked in the range \(0.2 - 0.3\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  How might creating a function that re-maps this grey into more extreme values help your model?
\item
  Is it possible to ``blur'' events that are in the outcome space? Does this ``blurring'' meet the requirements of a function of a random variable, as provided above?
\end{enumerate}

\end{example}

\hypertarget{probability-density-functions-and-cumulative-density-functions}{%
\subsection{Probability Density Functions and Cumulative Density Functions}\label{probability-density-functions-and-cumulative-density-functions}}

\begin{itemize}
\tightlist
\item
  What is a probability mass function?
\item
  What do the \textbf{Kolmogorov Axioms} mean must be true about any probability mass function (\emph{pmf})?
\end{itemize}

\begin{example}[Berkeley Drivers, No Survivors]

You should try driving in Berkeley some time. It is a \textbf{trip}! Without being deliberately ageist, the city is full of ageing hippies driving Subaru Outbacks and making what seem to be stochastic right-or-left turns to buy incense, pottery, or just sourdough bread.

Suppose that you are walking to campus, and you have to cross 10 crosswalks, each of which are spaced a block apart. Further, suppose that as you get closer to campus, there are fewer aging hippies, and therefore, there is decreasing risk that you're hit by a Subaru as you cross the street. Specifically, and fortunately for our math, the risk of being hit decreases linearly with each block that you cross.

Finally, campus provides you with the safety reports from last year, and reports that there were 120 student-Subaru incidents last year, out of 10,000 student-crosswalk crossings.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What is the \emph{pmf} for the probability that you are involved in a student-Subaru incident as you walk across these 10 blocks? What sample space, \(\Omega\) is appropriate to represent this scenario?
\item
  Suppose that you don't leave your house -- this is a remote program after all! What is your cumulative probability of being involved in a student-subaru incident?\\
\item
  What is the cumulative probability \emph{cmf} for the probability that you are involved in a student-Subaru incident?
\item
  Suppose that you live three blocks from campus, but your classmate lives five blocks from campus. What is the difference in the cumulative probability?
\item
  How would you describe the cumulative probability of being hit as you walk closer to campus? That is, suppose that you start 10 blocks away from campus, and are walking to get closer. Is your cumulative probability of being hit on your way to campus increasing or decreasing as you get closer to campus?
\item
  How would you describe the cumulative probability of being hit as you walk \textbf{further} from campus? That is, suppose that you start on campus, and you're walking to a bar after classes. Is your cumulative probability of being hit on your way away from campus increasing or decreasing as you get further from campus?
\end{enumerate}

\end{example}

\hypertarget{discrete-continuous-random-variables}{%
\section{Discrete \& Continuous Random Variables}\label{discrete-continuous-random-variables}}

What, if anything is fundamentally different between discrete and continuous random variables? As a way of starting the conversation, consider the following cases:

\begin{itemize}
\tightlist
\item
  Suppose \(X\) is a random variable that describes the time a student spends on w203 homework 1.

  \begin{itemize}
  \tightlist
  \item
    If you have only granular measurement -- i.e.~the number of nights spent working on the homework -- is this discrete or continuous?
  \item
    If you have the number of hours, is it discrete or continuous?
  \item
    If you have the number of seconds? Or milliseconds?
  \end{itemize}
\item
  Is it possible that \(P(X = a) = 0\) for every point \(a\)? For example, that \(P(X = 3600) = 0\).
\item
  Does one of these measures have more \emph{information} in it than another?

  \begin{itemize}
  \tightlist
  \item
    How are measurement choices that we make as designers of information capture systems -- i.e.~the machine processes, human processes, or other processes that we are going to work with as data scientists -- reflected in both the amount of information that is gathered, the type of information that is gathered, and the types of random variables that are manifest as a result?
  \end{itemize}
\end{itemize}

\hypertarget{moving-between-pdf-and-cdf}{%
\section{Moving Between PDF and CDF}\label{moving-between-pdf-and-cdf}}

The book defines \emph{pmf} and \emph{cmf} first as a way of developing intuition and a way of reasoning about these concepts. It then moves to defining continuous density functions, which is many ways are easier to work with although they lack the means of reasoning about them intuitively. Continuous distributions are defined in the book, and more generally, in terms of the \emph{cdf}, which is the cumulative density function. There are technical reasons for this choice of definition, some of which are signed in the footnotes on the page where the book presents it.

More importantly for this course, in \textbf{Definition 1.2.15} the book defines the relationship between \emph{cdf} and \emph{pdf} in the following way:

\begin{definition}[Probability Density Function (PDF)]
For a continuous random variable \(X\) with CDF \(F\), the \emph{probability density function} of \(X\) is

\[
  f(x) = \left. \frac{d F(u)}{du} \right|_{u=x}, \forall x \in \mathbb{R}.
\]
\end{definition}

\begin{itemize}
\tightlist
\item
  How does this definition, which relates \emph{pdf} and \emph{cdf} by a means of differentiation and integration, fit with the ideas that we just developed in the context of walking to and from campus?
\end{itemize}

\begin{example}[Working with a continuous pdf and cdf]

Suppose that you learn than a particular random variable, \(X\) has the following function that describes its \emph{pdf}, \(f_{x}(x) = \frac{1}{10}x\). Also, suppose that you know that the smallest value that is possible for this random variable to obtain is 0.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What is the CDF of \(X\)?
\item
  What is the maximum possible value that \(x\) can obtain? How did you develop this answer, using the Kolmogorov axioms of probability?
\item
  What is the cumulative probability of an outcome up to 0.5?
\item
  What is the probability of an outcome between 0.25 and 0.75? Produce an answer to this in two ways:
\item
  Using the \(pdf\)
\item
  Using the \(cdf\)
\end{enumerate}

\end{example}

\hypertarget{joint-density}{%
\section{Joint Density}\label{joint-density}}

Working with a single random variable helps to develop our understanding of how to relate the different features of a \emph{pdf} and a \emph{cdf} through differentiation and integration. However, there's not really \emph{that} much else that we can do; and, there is probably very little in our professional worlds that would look like a single random variable in isolation.

We really start to get to something useful when we consider joint density functions. Joint density functions describe the probability that \emph{both} of two random variables. That is, if we are working with random variables \(X\) and \(Y\), then the joint density function provides a probability statement for \(P(X \cap Y)\).

In this course, we might typically write this joint density function as \(f_{X,Y}(x,y) = f(\cdot)\) where \(f(\cdot)\) is the actual function that represents the joint probability. The \(f(\cdot)\) means, essentially, ``some function'' where we just have not designated the specifics of the function; you might think of this as a generic function.

\hypertarget{example-uniform-joint-density}{%
\subsection{Example: Uniform Joint Density}\label{example-uniform-joint-density}}

Suppose that we know that two variables, \(X\) and \(Y\) are jointly uniformly distributed within the the \emph{support} \(x \in [0,4], y \in [0,4]\). We have a requirement, imposed by the \emph{Kolmogorov Axioms} that all probabilities must be non-zero, and that the total probability across the whole support must be one.

\begin{itemize}
\tightlist
\item
  Can you use these facts to determine answers to the following:

  \begin{itemize}
  \tightlist
  \item
    What kind of shape does this joint \emph{pdf} have?
  \item
    What is the specific function that describes this shape?
  \item
    If you draw this shape on three axes, and \(X\), and \(Y\), and a \(P(X,Y)\), what does this plot look like?
  \item
    How do you get from the joint density function, to a marginal density function for \(X\)?
  \item
    How do you get form the joint density function, to a marginal density function for \(Y\)?
  \item
    How do you get from these marginal density functions of \(X\) and \(Y\) back to the joint density? Is this always possible?
  \end{itemize}
\end{itemize}

\hypertarget{examples-thinking-through-many-plots}{%
\subsection{Examples: Thinking Through Many Plots}\label{examples-thinking-through-many-plots}}

An alumni of the MIDS program, and a former instructor of this course, \href{https://www.linkedin.com/in/dtoddyoung/}{Todd Young} built this nifty tool that lets us consider several different joint probability functions.

As a class, lets consider a few of these PDFs, beginning with this ``triangle'' distribution.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{include\_app}\NormalTok{(}\StringTok{\textquotesingle{}http://www.statistics.wtf/PDF\_Explorer/\textquotesingle{}}\NormalTok{, }\AttributeTok{height=}\StringTok{"1000px"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{triangle-math}{%
\subsection{Triangle Math}\label{triangle-math}}

After considering the intuition for the triangle distribution, do the following: Write down the function that accords with the figure that you're seeing above.\footnote{Notice, that in general, this kind of \emph{curve fitting} isn't really a common data science task. Instead, this is just a learning task that lets the class assess their understanding of the definitions of random variables.}

\begin{itemize}
\tightlist
\item
  What is a full statement of the PDF of this image?
\item
  What is the marginal distribution of \(X\), \(f_{X}(x)\)?
\item
  What is the marginal distribution of \(Y\), \(f_{Y}(y)\)?
\item
  Using the definition of independence, are \(X\) and \(Y\) independent of each other?
\item
  What is the CDF of \(X\), \(F_{X}(x)\)?
\end{itemize}

\hypertarget{saddle-sores}{%
\subsection{Saddle Sores}\label{saddle-sores}}

Suppose that you know that two random variables, \(X\) and \(Y\) are jointly distributed with the following \emph{pdf}:

\[
f_{X,Y}(x,y) = 
  \begin{cases}
    a * x^{2} * y^{2} & 0 < x < 1, 0 < y < 1 \\
    0 & otherwise
  \end{cases}
\]

This joint pdf is similar to the pdf that you can visualize above, under the distribution called ``saddle''. The difference between this function and the image above is that the function bounds the with support of \(x\) and \(y\) on the range \([0,1]\). This is to make the math easier for us in the next step.

\begin{itemize}
\tightlist
\item
  Can you use these facts to determine the following?

  \begin{itemize}
  \tightlist
  \item
    What value of \(a\) makes this a valid joint pdf?
  \item
    What is the marginal pdf of \(x\)? That is, what is \(f_{x}(x)\)?
  \item
    What is the conditional pdf of \(X\) given \(Y\)? That is, what is \(f_{x|y}(x,y)\)?
  \item
    Given these facts, would you say that \(X\) and \(Y\) are dependent or independent?
  \item
    If the support for this joint distribution were instead \([0,4]\) (rather than \([0,1]\)), how would the shape of the distribution change?
  \end{itemize}
\end{itemize}

\hypertarget{computing-different-distributions.}{%
\section{Computing Different Distributions.}\label{computing-different-distributions.}}

Suppose that random variables \(X\) and \(Y\) are jointly continuous, with joint density function given by,

\[
f(x,y) = 
  \begin{cases}
    c, & 0 \leq x \leq 1, 0 \leq y \leq x \\
    0, & otherwise
\end{cases}
\]

where \(c\) is a constant.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Draw a graph showing the region of the X-Y plane with positive probability density.
\item
  What is the constant \(c\)?
\item
  Compute the marginal density function for \(X\). (Be sure to write a complete expression)
\item
  Compute the conditional density function for \(Y\), conditional on \(X=x\). (Be sure to specify for what values of \(x\) this is defined)
\end{enumerate}

\hypertarget{conditional-probability}{%
\section{Conditional Probability}\label{conditional-probability}}

Conditional probability is \textbf{incredible}. In fact, without exaggeration, almost \textbf{all} of data science is an exercise in making statements about conditional probability distributions. \emph{Don't believe us?}

\begin{itemize}
\tightlist
\item
  What is the goal of a ``customer churn'' model or a conversion model?
\item
  What is the goal of a language-completion model?
\item
  What is the goal of flight-departures model?
\end{itemize}

\textbf{If} we possessed the whole information about a process; \textbf{if} we had the CDF that governed probability of occurrences, what kinds of statements would we be able to make? Would we even need data?

Using the distribution above, produce a statement of conditional probability, \(f_{Y|X}(y|x)\).

\hypertarget{visualizing-distributions-via-simulation}{%
\section{Visualizing Distributions Via Simulation}\label{visualizing-distributions-via-simulation}}

To this point in the course, we have focused on concepts in ``the population'' with no reference to samples. This is on purpose! We want to develop the theory that defines the \textbf{best possible} predictor if we knew \textbf{everything} (if we know formula of the function that maps from \(\omega \rightarrow \mathbb{R}\), and we know the probability of each \(\omega \in \Omega\) then we know everything). Beginning in week 5 of the course, we will talk about ``approximating'' (which we will call estimating) this best possible predictor with a limited sample of data.

However, at this point, to help build your working understanding, or intuition, for what is happening, we are going to work on a way to \emph{simulate} draws from a population. In some places, people might refer to these as \emph{Monte Carlo} methods -- this is because the method was developed by von Neumann \& Ulam during World War II, and they needed a way to talk about it using a code name. They chose \emph{Monte Carlo} after a famous casino in Monaco.

\hypertarget{example-the-uniform-distribution}{%
\subsection{Example: The Uniform Distribution}\label{example-the-uniform-distribution}}

\begin{quote}
You: ``Gosh. There sure are a lot of examples that use the uniform distribution. That must be a really important statistical distribution.''

Instructor: ``Nah. Not really. We're just using the uniform a bunch so that we don't get too lost in doing math while we're working with these concepts.''
\end{quote}

We'll start with a simple uniform distribution, but then we'll make it a little more complex in a moment.

We can use R to simulate draws from a probability distribution function by providing it with the name of the distribution that we're considering, the support of that distribution, or other features of the distribution. In the case of the uniform, the entire distribution is can be described just from it support.

So, suppose that you had a uniform distribution that had positive probability on the range \([1.1, 4.3]\). Why these? No particular reason. That is, suppose

\[
  f_{X}(x) = \begin{cases} 
    a & 1.1 \leq x \leq 4.3 \\ 
    0 & otherwise
  \end{cases}
\]

What does this distribution ``look like''? Because it is a uniform, you might have a sense that it will be a horizontal line. But, what is the height of that line? Aha! We could do the math to figure it out, or we could generate an approximation using a simulation.

In the code below, we are going to create an object called \texttt{samples\_uniform} that stores the results of the \texttt{runif} function call.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{samples\_uniform }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(}\AttributeTok{n=}\DecValTok{1000}\NormalTok{, }\AttributeTok{min=}\FloatTok{1.1}\NormalTok{, }\AttributeTok{max=}\FloatTok{4.3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

What is happening inside \texttt{runif}?

When you're writing you own code, you can pull up the documentation for this (and any) function using a question mark, i.e.~\texttt{?}, followed by the function name -- \texttt{?runif}.

But, we can speed this up slightly by simply telling you that \texttt{n} is the number of samples to take from the population; \texttt{min} is the low-end of the support, and \texttt{max} is the high-end of the support.

If we look into this object, we can see the results of the function call. Below, we will show the first \(20\) elements of the \texttt{samples\_uniform} object.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{samples\_uniform[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{20}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 2.066562 2.011425 3.309678 1.344357 1.760204 4.136672 3.603350 1.657201 3.096724 2.120201 3.245458
## [12] 2.069459 3.087195 2.838074 1.732300 1.183500 1.988982 3.980934 3.331974 1.609208
\end{verbatim}

(Notice that R is a \(1\) index language (python is a zero-index language).)

With this object created, we can plot a density of the data and then learn from this histogram what the pdf looks like.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plot\_full\_data }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(samples\_uniform), }\AttributeTok{y=}\NormalTok{samples\_uniform) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{()  }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{title =} \StringTok{\textquotesingle{}Showing the Data\textquotesingle{}}\NormalTok{, }
    \AttributeTok{y     =} \StringTok{\textquotesingle{}Sample Value\textquotesingle{}}\NormalTok{, }
    \AttributeTok{x     =} \StringTok{\textquotesingle{}Index\textquotesingle{}}\NormalTok{)}

\NormalTok{plot\_density }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{samples\_uniform) }\SpecialCharTok{+} 
  \FunctionTok{geom\_density}\NormalTok{(}\AttributeTok{bw=}\FloatTok{0.1}\NormalTok{)   }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{title =} \StringTok{\textquotesingle{}Showing the PDF\textquotesingle{}}\NormalTok{, }
    \AttributeTok{y     =} \StringTok{\textquotesingle{}Probability of Drawing Value\textquotesingle{}}\NormalTok{, }
    \AttributeTok{x     =} \StringTok{\textquotesingle{}Sample Value\textquotesingle{}}\NormalTok{)}

\NormalTok{(plot\_full\_data }\SpecialCharTok{|}\NormalTok{ (plot\_density }\SpecialCharTok{+} \FunctionTok{coord\_flip}\NormalTok{())) }\SpecialCharTok{/} 
\NormalTok{  plot\_density }
\end{Highlighting}
\end{Shaded}

\includegraphics{203-live-session_files/figure-latex/plot uniform samples-1.pdf}

Interesting. From what we can see here, there does not appear to be any discernible pattern. This leaves us with two options: either, we might reduce the resolution that we're using to view this pattern, or we might take more samples and hold the resolution constant. Below, two different plots show these differing approaches, and are \emph{very} explicit about the code that creates them.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{samples\_uniform\_moar }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(}\AttributeTok{n=}\DecValTok{1000000}\NormalTok{, }\AttributeTok{min=}\FloatTok{1.1}\NormalTok{, }\AttributeTok{max=}\FloatTok{4.3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plot\_low\_res }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{()  }\SpecialCharTok{+} 
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{samples\_uniform)  }\SpecialCharTok{+} 
  \FunctionTok{geom\_density}\NormalTok{(}\AttributeTok{bw=}\FloatTok{0.1}\NormalTok{)    }\SpecialCharTok{+} 
  \FunctionTok{lims}\NormalTok{(}\AttributeTok{y=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\FloatTok{0.4}\NormalTok{))        }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{\textquotesingle{}Low Res, Low Data\textquotesingle{}}\NormalTok{)}

\NormalTok{plot\_high\_res }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{()     }\SpecialCharTok{+} 
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{samples\_uniform\_moar) }\SpecialCharTok{+} 
  \FunctionTok{geom\_density}\NormalTok{(}\AttributeTok{bw=}\FloatTok{0.01}\NormalTok{)       }\SpecialCharTok{+} 
  \FunctionTok{lims}\NormalTok{(}\AttributeTok{y=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\FloatTok{0.4}\NormalTok{))            }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{\textquotesingle{}High Res, More Data\textquotesingle{}}\NormalTok{)}

\NormalTok{plot\_low\_res }\SpecialCharTok{|}\NormalTok{ plot\_high\_res}
\end{Highlighting}
\end{Shaded}

\includegraphics{203-live-session_files/figure-latex/plot uniform distributions-1.pdf}

\hypertarget{example-the-normal-distribution}{%
\subsection{Example: The Normal Distribution}\label{example-the-normal-distribution}}

Folks might have some prior beliefs about the Normal distribution. Don't worry, we'll cover this later in the course. But, this is the distribution that you have in mind when you're thinking of a ``bell curve''.

We can use the same method to visualize a normal distribution as we did for a uniform distribution. In this case, we would issue the call \texttt{rnorm}, together with the population parameters that define the population. At this point in the course, we do not expect that you will know these (and, actually memorizing these facts are not a core focus of the course), but you can \href{https://en.wikipedia.org/wiki/Normal_distribution}{look them up} if you like. Truthfully, statistics wikipedia is \emph{very} good.

Do do you notice anything about the \texttt{runif} and the \texttt{rnorm} calls that we have identified? Both seem to name the distribution: \(unif \approx uniform\) and \(norm \approx normal\), but prepened with a \texttt{r}? This is for ``random draw''.

Base R is loaded with a \emph{pile} of basic statistics distributions, which you can look into using \texttt{?distributions}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{samples\_normal }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n=}\DecValTok{100000}\NormalTok{, }\AttributeTok{mean=}\DecValTok{18}\NormalTok{, }\AttributeTok{sd=}\DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Like before, we could look at the first \(20\) of these samples.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{samples\_normal[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{20}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 22.115919 16.483990 22.410488 22.100870 12.543764 17.786928 19.138007 12.190335 13.530546 22.450998
## [11] 16.761041 15.800745 20.773784 12.851562 23.500219 18.330674 17.632022  7.697596 18.466610 22.645725
\end{verbatim}

And, from here we could visualize this distribution.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{samples\_normal) }\SpecialCharTok{+} 
  \FunctionTok{geom\_density}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title=}\StringTok{\textquotesingle{}Visualization of this Normal Distribution\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{203-live-session_files/figure-latex/plot normal-1.pdf}

\hypertarget{combining-this-ability}{%
\subsubsection{Combining This Ability}\label{combining-this-ability}}

Consider three random variables \(A, B, C\). Suppose,

\[ 
  \begin{aligned}
    A & \sim Uniform(min=1.1, max=4.3) \\ 
    B & \sim Normal(mean=18, sd=4)     \\ 
    C = A + B
  \end{aligned}
\]

And, suppose that \(B\) is a random variable that is described by the normal density that we considered earlier. Suppose that \(A\) and \(B\) are independent of each other.

Finally, suppose that \(C = A + 2B\).

What does \(C\) look like?

Although this is a simple function applied to a random variable -- a legal move -- the math would be tedious. What if, instead, one used this simulation method to get a sense for the distribution?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{samples\_A }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(}\AttributeTok{n=}\DecValTok{10000}\NormalTok{, }\AttributeTok{min=}\FloatTok{1.1}\NormalTok{, }\AttributeTok{max=}\FloatTok{4.3}\NormalTok{)}
\NormalTok{samples\_B }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n=}\DecValTok{10000}\NormalTok{, }\AttributeTok{mean=}\DecValTok{18}\NormalTok{, }\AttributeTok{sd=}\DecValTok{4}\NormalTok{)}

\NormalTok{samples\_C }\OtherTok{\textless{}{-}}\NormalTok{ samples\_A }\SpecialCharTok{+}\NormalTok{ samples\_B}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plot\_C }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{samples\_C) }\SpecialCharTok{+} 
  \FunctionTok{geom\_density}\NormalTok{()}

\NormalTok{plot\_C\_and\_A\_and\_B }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{()   }\SpecialCharTok{+} 
  \FunctionTok{geom\_density}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{samples\_A), }\AttributeTok{color =} \StringTok{\textquotesingle{}\#003262\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_density}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{samples\_B), }\AttributeTok{color =} \StringTok{\textquotesingle{}\#FDB515\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_density}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{samples\_C), }\AttributeTok{color =} \StringTok{\textquotesingle{}darkred\textquotesingle{}}\NormalTok{)}

\NormalTok{plot\_C\_and\_A\_and\_B}
\end{Highlighting}
\end{Shaded}

\includegraphics{203-live-session_files/figure-latex/plot C-1.pdf}

\hypertarget{review-of-terms}{%
\section{Review of Terms}\label{review-of-terms}}

Remember some of the key terms we learned in the async:

\begin{itemize}
\tightlist
\item
  Joint Density Function
\item
  Conditional Distribution
\item
  Marginal Distribution
\end{itemize}

Explain each of these three in terms of the cake metaphor.

\hypertarget{summarizing-distributions}{%
\chapter{Summarizing Distributions}\label{summarizing-distributions}}

\begin{figure}
\centering
\includegraphics{./images/yosemite.jpg}
\caption{a majestic valley}
\end{figure}

\hypertarget{learning-objectives-2}{%
\section{Learning Objectives}\label{learning-objectives-2}}

At the end of the live session and homework this week, students will be able to

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Understand} the importance of thinking in terms of random variables, while;
\item
  \textbf{Appreciating} that it is not typically possible to fully model the world with a single function.
\item
  \textbf{Articulate} why we need a target for a model, and propose several possible such targets.
\item
  \textbf{Produce} summaries of location and relationship given a particular functional form for a random variable.
\end{enumerate}

\hypertarget{class-announcements-1}{%
\section{Class Announcements}\label{class-announcements-1}}

Where have we come from, and where are we going?

\hypertarget{what-is-in-the-rearview-mirror}{%
\subsection{What is in the rearview mirror?}\label{what-is-in-the-rearview-mirror}}

\begin{itemize}
\tightlist
\item
  Statisticians create a population model to represent the world; random variables are the building blocks of such a model.
\item
  We can describe the distribution of a random variable using:

  \begin{itemize}
  \tightlist
  \item
    A \emph{CDF} for all random variables
  \item
    A \emph{PMF} for discrete random variables
  \item
    A \emph{PDF} for continuous random variables
  \end{itemize}
\item
  When we have multiple random variables,

  \begin{itemize}
  \tightlist
  \item
    The joint PMF/PDF describes how they behave together
  \item
    The marginal PMF/PDF describes one variable in isolation
  \item
    The conditonal PMF/PDF describes one variable given the value of another
  \end{itemize}
\end{itemize}

\hypertarget{todays-lesson}{%
\subsection{Today's Lesson}\label{todays-lesson}}

What might seem frustrating about this probability theory system of reasoning is that we are building a castle in the sky -- a fiction. We're supposing that there is some function that describes the probability that values are generated. In reality, there is no such generative function; it is \emph{extremely unlikely} (though we'll acknowledge that it is possible) that the physical reality we belive we exist within is just a complex simulation that has been programmed with functions by some unknown designer.

Especially frustrating is that we're supposing this function, and then we're further saying,

\begin{quote}
``If only we had this impossible function; and if only we also had the ability to take an impossible derivative of this impossible function, then we could\ldots{}''
\end{quote}

\hypertarget{single-number-summaries-of-a-single-random-variable}{%
\subsubsection{Single number summaries of a single random variable}\label{single-number-summaries-of-a-single-random-variable}}

But, here's the upshot!

\textbf{What we are doing today is laying the baseline for models that we will introduce next week.} Here, we are going to suggest that there are radical simplifications that we can produce that hold specific guarantees, no matter how complex the function that we're reasoning about.

In particular, in one specific usage of the term \emph{best} we will prove that the Expectation operation is the best one-number summary of any distribution. To do so, we will define a term, \emph{variance}, which is the squared deviations from the expectation of a variable that describes how ``spread out'' is a variable. Then, we will define a concept that is the \emph{mean squared error} that is the square of the distance between a model prediction and a random variable's realization. The key realization is that when the model predicts the expectation, then the MSE is equal to the variance of the random variable, which is the smallest possible value it could realize.

\hypertarget{single-number-summaries-of-relationships-between-random-variables}{%
\subsubsection{Single number summaries of relationships between random variables}\label{single-number-summaries-of-relationships-between-random-variables}}

Although the single number summaries are \textbf{incredibly} powerful, that's not enough for today's lesson! We're also going to suggest that we can create a measure of linear dependence between two variables that we call the ``covariance'', and a related, rescaled version of this relationship that is called the correlation.

\hypertarget{future-attractions}{%
\subsection{Future Attractions}\label{future-attractions}}

\begin{itemize}
\tightlist
\item
  A predictor is a function that provides a value for one variable, given values of some others.
\item
  Using our summary tools, we will define a predictor's error and then minimize it.
\item
  This is a basis for linear regression
\end{itemize}

\hypertarget{discussion-of-terms}{%
\section{Discussion of Terms}\label{discussion-of-terms}}

Together with your instructor, you will talk about what each of the following definitions mean in your own words, for the key concepts, you might also formalize this intuition into a formula that can be computed.

\begin{itemize}
\tightlist
\item
  Expected Value, or Expectation
\item
  Central Moments \(\rightarrow\) Variance \(\rightarrow\) Standard Deviation
\item
  Set aside for later: Chebyshev's Inequality and the Normal Distribution
\item
  Mean Squared Error and its alternative formula
\item
  Covariance and Correlation
\end{itemize}

\hypertarget{computing-examples}{%
\section{Computing Examples}\label{computing-examples}}

\hypertarget{expected-value-of-a-die-discrete-random-variable}{%
\subsection{Expected Value of a Die {[}discrete random variable{]}}\label{expected-value-of-a-die-discrete-random-variable}}

\begin{itemize}
\tightlist
\item
  The expected value (or population mean) of a discrete random variable \(X\) is the weighted average of the values in the range of \(X\).
\item
  Suppose that \(X\) represents the number of years of education that someone has completed, and so has a support that ranges from \(0\) years of education, up to \(28\) years of education. (Incidentally, Mark Labovitz has about 28 years of education).
\end{itemize}

\includegraphics{203-live-session_files/figure-latex/unnamed-chunk-5-1.pdf}

without using specific numbers, describe the process you would use to calculate the expected value of this distribution.

\hypertarget{using-a-formula}{%
\subsection{Using a formula}\label{using-a-formula}}

How does the following formula match with your concept of expectation from that last section?

\[
  E(X) = \sum_{x \in \{years\}} x \cdot P(X=x)
\]

\hypertarget{computing-by-hand}{%
\section{Computing by Hand}\label{computing-by-hand}}

\hypertarget{compute-the-expected-value}{%
\subsection{Compute the Expected Value}\label{compute-the-expected-value}}

Let \(X\) represent the result of one roll of a 6 sided die.

\begin{itemize}
\tightlist
\item
  Calculating by hand, what is the expected value \(X\)?
\end{itemize}

\begin{quote}
Fill this in by hand.
\end{quote}

\hypertarget{compute-the-variance}{%
\subsection{Compute the Variance}\label{compute-the-variance}}

Let \(X\) represent the result of one roll of a 6 sided die.

\begin{itemize}
\tightlist
\item
  Calculating by hand, what is the variance of \(X\)?
\end{itemize}

\hypertarget{expected-value-by-code}{%
\section{Expected Value by Code}\label{expected-value-by-code}}

\hypertarget{expected-value-of-a-six-sided-die}{%
\subsection{Expected Value of a Six-Sided Die}\label{expected-value-of-a-six-sided-die}}

Let \(X\) represent the result of one roll of a 6 sided die.

\begin{itemize}
\tightlist
\item
  Build an object to represent the whole sample space, \(\Omega\) of a six sided die.
\item
  Determine what probabilities to assign to each value of that object.
\item
  Write the code to run the expectation algorithm that you just performed by hand.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{die }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
    \AttributeTok{value =} \StringTok{\textquotesingle{}fill this in\textquotesingle{}}\NormalTok{,}
    \AttributeTok{prob  =} \StringTok{\textquotesingle{}fill this in\textquotesingle{}}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\hypertarget{variacne-of-a-six-sided-die}{%
\subsection{Variacne of a Six-Sided Die}\label{variacne-of-a-six-sided-die}}

Let \(X\) represent the result of one roll of a 6 sided die.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Using what you know about the definition of variance, write a function that will compute the variance of your \texttt{die} object.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{variance\_function }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(die) \{ }
  \DocumentationTok{\#\# fill this in}
\NormalTok{  mu }\OtherTok{=} \StringTok{\textquotesingle{}fill this in\textquotesingle{}}   \DocumentationTok{\#\# you should index to the correct column}
\NormalTok{  var }\OtherTok{=} \StringTok{\textquotesingle{}fill this in\textquotesingle{}}  \DocumentationTok{\#\# for each, and use the correct function}
  
  \FunctionTok{return}\NormalTok{(var)}
\NormalTok{\}}

\FunctionTok{variance\_function}\NormalTok{(die)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "fill this in"
\end{verbatim}

Suppose that you had to keep the values the same on the die, that is. That is the domain of the outcome still had to be the countable set of integers from one to six.

\begin{itemize}
\tightlist
\item
  How would you change the probability distribution to decrease the variance of this random variable?
\item
  What is the smallest value that you can generate for this random variable? Use the \texttt{variance\_function} from above to actually compute this variance.
\item
  What is the largest value of variance that you can generate for this random variable? Use the \texttt{variance\_function} from above to actually compute this variance.
\end{itemize}

Now suppose that you again had an equal probability of every outcome, but you were to apply a function to the number of spots that are showing on the die. Rather that each dot contributing one value to the random variable, instead the random variable's outcome is the square of the number of spots.

\begin{itemize}
\tightlist
\item
  How would this change the mean?\\
\item
  How would this change the variance?
\end{itemize}

\hypertarget{practice-computing}{%
\section{Practice Computing}\label{practice-computing}}

\hypertarget{single-variable}{%
\subsection{Single Variable}\label{single-variable}}

Suppose that \(X\) has the following density function:

\[ 
  f_{X}(x) = \begin{cases} 
    6x(1 - x), & 0 < x < 1 \\ 
    0, & otherwise \\ 
  \end{cases}
\]

\begin{itemize}
\tightlist
\item
  Find \(E[X]\).
\item
  Find \(E[X^2]\).
\item
  Find \(V[X]\).
\end{itemize}

\hypertarget{joint-density-1}{%
\subsection{Joint Density}\label{joint-density-1}}

Suppose that \(X\) and \(Y\) have joint density \(f_{X,Y}(x,y) = 10 x^2y\) for \$0 \textless{} y \textless{} x \textless{} 1. \$

\begin{itemize}
\tightlist
\item
  Find \(E[X]\)
\item
  Find \(E[EY]\)
\item
  Find \(\rho[X,Y]\)
\end{itemize}

\hypertarget{conditional-expectation-and-the-blp}{%
\chapter{Conditional Expectation and The BLP}\label{conditional-expectation-and-the-blp}}

\begin{figure}
\centering
\includegraphics{./images/tam-view.jpeg}
\caption{mt. tamalpais}
\end{figure}

One of our most fundamental goals as data scientists is to produce predictions that are \emph{good}. In this week's async, we make a statement of performance that we can use to evaluate how good a job a predictor is doing, choosing Mean Squared Error.

With the goal of minimizing MSE, then we then present, justify, and prove that the conditional expectation function (\emph{the CEF}) is the globally best possible predictor. This is an incredible powerful result, and one that serves as the backstop for \textbf{every} other predictor that you will ever fit, whether that predictor is a ``simple'' regression, or that predictor is a machine learning algorithms (e.g.~a random forest) or a deep learning algorithm. Read that again -- even the most technologically advanced algorithms \emph{cannot possibly} perform better than the conditional expectation function.

Why does the CEF do so well? Because it can contain a \emph{vast} amount of complex information and relationships; in fact, the complexity of the CEF is a product of the complexity of the underlying probability space. If that is the case, then why don't we just use the CEF as our predictor every time?

Well, this is one of the core problems of applied data science work: we are never given the function that describes the behavior of the random variable. And so, we're left in a world where we are forced to produce predictions from simplifications of the CEF. A very strong simplification, but one that is useful for our puny human brains, is to restrict ourselves to predictors that make predictions from a linear combination of input variables.

Why should we make such a strong restriction? After all, the conditional expectation function might be a fantastically complex combination of input features, why should we entertain functions that are only linear combinations? Essentially, this is because we're limited in our ability to reason about anything more complex than a linear combination.

\hypertarget{thunder-struck}{%
\section{Thunder Struck}\label{thunder-struck}}

\begin{figure}
\centering
\includegraphics{./images/conditional_risk.png}
\caption{thunder struck}
\end{figure}

\hypertarget{learning-objectives-3}{%
\section{Learning Objectives}\label{learning-objectives-3}}

At the end of this weeks learning, which includes the asynchronous lectures, reading the textbook, this live session, and the homework associated with the concepts, student should be able to

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Recognize} that the conditional expectation function, the \emph{CEF}, is a the pure-form, best-possible predictor of a target variable given information about other variables.
\item
  \textbf{Recall} that all other predictors, be they linear predictors, non-linear predictors, branching predictors, or deep learning predictors, are an attempt to approximate the CEF.
\item
  \textbf{Produce} the conditional expectation function as a predictor, given joint densities of random variables.
\item
  \textbf{Appreciate} that the best linear predictor, which is a restriction of predictors to include only those that are linear combinations of variables, can produce reasonable predictions, and \textbf{anticipate} that the BLP forms the target of inquiry for regression.
\end{enumerate}

\hypertarget{class-announcements-2}{%
\section{Class Announcements}\label{class-announcements-2}}

\hypertarget{test-1-is-releasing-to-you-today.}{%
\subsection{Test 1 is releasing to you today.}\label{test-1-is-releasing-to-you-today.}}

The first test is releasing today. There are review sessions scheduled for this week, practice tests available, and practice problems available. The format for the test is posted in the course discussion channel. In addition to your test, your instructor will describe your responsibilities that are due next week.

\hypertarget{roadmap}{%
\section{Roadmap}\label{roadmap}}

\hypertarget{rearview-mirror}{%
\subsection{Rearview Mirror}\label{rearview-mirror}}

\begin{itemize}
\tightlist
\item
  Statisticians create a population model to represent the world.
\item
  \(E[X], V[X], Cov[X,Y]\) are ``simple'' summaries of complex joint distributions, which are hooks for our analyses.
\item
  They also have useful properties -- for example, \(E[X + Y] = E[X] + E[Y]\).
\end{itemize}

\hypertarget{this-week}{%
\subsection{This week}\label{this-week}}

\begin{itemize}
\tightlist
\item
  We look at situations with one or more ``input'' random variables, and one ``output.''
\item
  Conditional expectation summarizes the output, given values for the inputs.
\item
  The conditional expectation function (CEF) is a predictor -- a function that yields a value for the output, give values for the inputs.
\item
  The best linear predictor (BLP) summarizes a relationship using a line / linear function.
\end{itemize}

\hypertarget{coming-attractions}{%
\subsection{Coming Attractions}\label{coming-attractions}}

\begin{itemize}
\tightlist
\item
  OLS regression is a workhorse of modern statistics, causal analysis, etc

  \begin{itemize}
  \tightlist
  \item
    It is also the basis for many other models in classical stats and machine learning
  \end{itemize}
\item
  The target that OLS estimates is exactly the BLP, which we're learning about this week.
\end{itemize}

\hypertarget{conditional-expectation-function-cef}{%
\section{Conditional Expectation Function (CEF),}\label{conditional-expectation-function-cef}}

\hypertarget{part-i}{%
\subsection{Part I}\label{part-i}}

Think back to remember the definition of the expectation of \(Y\):

\[
  E[Y] = \int_{-\infty}^\infty y \cdot f_{Y}(y) dy
\]

This week, in the async reading and lectures we added a new concept, the conditional expectation of \(Y\) given \(X=x \in \text{Supp}[X]\):

\[
  E[Y|X=x] =  \int_{-\infty}^\infty y \cdot f_{Y|X}(y|x) dy
\]

\hypertarget{part-ii}{%
\subsection{Part II}\label{part-ii}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What desirable properties of a predictor does the expectation possess (note, this is thinking \emph{back} by a week)? What makes these properties desirable?
\item
  Turning to the content from this week, how, if at all, does the conditional expectation improve on these desirable properties?
\end{enumerate}

\hypertarget{part-iii}{%
\subsection{Part III}\label{part-iii}}

\begin{itemize}
\item
  Compare and contrast \(E[Y]\) and \(E[Y|X]\). For example, when you look at how these operators are ``shaped'', how are their components similar or different?\footnote{Note, when we say ``shaped'' here, we're referring to the deeper concept of a statistical functional. A statistical functional is a function of a function that maps to a real number. So, if \(T\) is the functional that we're thinking of, \(\mathcal{F}\) is a family of functions that it might operate on, and \(\mathbb{R}\) is the set of real numbers, a statistical functional is just \(T: \mathcal{F} \rightarrow \mathbb{R}\). The Expectation statistical functional, \(E[X]\) always has the form \(\int x f_{X}(x)dx\).)}
\item
  What is \(E[Y|X]\) a function of? What are ``input'' variables to this function?
\item
  What, if anything, is \(E[E[Y|X]]\) a function of?
\end{itemize}

\hypertarget{computing-the-cef}{%
\section{Computing the CEF}\label{computing-the-cef}}

\begin{itemize}
\tightlist
\item
  Suppose that random variables \(X\) and \(Y\) are jointly continuous, with joint density function given by,
\end{itemize}

\[
f(x,y) = 
  \begin{cases}
    2, & 0 \leq x \leq 1, 0 \leq y \leq x \\
    0, & otherwise
\end{cases}
\]

What does the joint PDF of this function look like?

\includegraphics{203-live-session_files/figure-latex/create blank plot for pdf-1.pdf}

To begin with, let's compute the simplest quantities:

\begin{itemize}
\tightlist
\item
  What is the expectation of \(X\)?
\item
  What is the expectation of \(Y\)?
\item
  How would you compute the variance of \(X\)? (We're not going to do it live).
\end{itemize}

And then, let's think about how to compute the conditional quantites:

\begin{itemize}
\tightlist
\item
  What is the conditional expectation of \(Y\), given that \(X=x=0\)?
\item
  What is the conditional expectation of \(Y\), given that \(X=x=0.5\)?
\item
  What is the conditional expectation of \(X\), given that \(Y=y=0.5\)?
\item
  Which of the two of these has a lower conditional variances?

  \begin{itemize}
  \tightlist
  \item
    \(V[Y|X=0.25]\); or,
  \item
    \(V[Y|X=0.75]\).
  \end{itemize}
\item
  How does \(V[Y]\) compare to \(V[Y|X=1]\)? Which is larger?
\end{itemize}

\hypertarget{conditional-functionals}{%
\section{Conditional Functionals}\label{conditional-functionals}}

Suppose that you were told that \(X\) and \(Y\) are jointly distributed according to the following joint density function:

\[ 
  f_{X,Y}(x,y) = 
    \begin{cases}
      2 & 0 \leq x \leq 1, 0 \leq y \leq x \\ 
      0 & otherwise
    \end{cases}
\]

\hypertarget{conditional-expectation}{%
\subsection{Conditional Expectation}\label{conditional-expectation}}

\textbf{What is the conditional expectation function?} To get started, you can use the fact that in week two, we already computed the conditional probability density function:

\[
f_{Y|X}(y|x) = \begin{cases}
  \frac{1}{x}, & 0 \leq y \leq x \\ 
  0,           & \text{otherwise.}
\end{cases}
\]

\vspace{10cm}

\hypertarget{conditional-variance}{%
\subsection{Conditional Variance}\label{conditional-variance}}

\begin{itemize}
\tightlist
\item
  What is the conditional variance function?\footnote{Take a moment to strategize just a little bit before you get going on this one. There is a way to compute this value that is easier than another way to compute this value.}
\end{itemize}

\vspace{20cm}

\hypertarget{breakout-activity}{%
\section{Breakout Activity}\label{breakout-activity}}

\hypertarget{minimizing-mse}{%
\subsection{Minimizing MSE}\label{minimizing-mse}}

Theorem 2.2.20 states,

\begin{quote}
The CEF \(E[Y|X]\) is the ``best'' predictor of \(Y\) given \(X\), where ``best'' means it has the smallest mean squared error (MSE).
\end{quote}

Oh yeah? As a breakout group, \emph{ride shotgun} with us as we prove that the conditional expectation is the function that produces the smallest possible Mean Squared Error.

Specifically, \textbf{you group's task} is to justify every transition from one line to the next using concepts that we have learned in the course: definitions, theorems, calculus, and algebraic operations.

\hypertarget{the-pudding-aka-where-the-proof-is}{%
\subsection{The pudding (aka: ``Where the proof is'')}\label{the-pudding-aka-where-the-proof-is}}

We need to find such function \(g(X): \mathbb{R} \to \mathbb{R}\) that gives the smallest mean squared error.

First, let MSE be defined as it is in \textbf{Definition 2.1.22}.

\begin{quote}
For a random variable \(X\) and constant \(c \in \mathbb{R}\), the \emph{mean squared error} of \(X\) about \(c\) is \(E[(x-c)^2]\).
\end{quote}

Second, let us note that since \(g(X)\) is just a function that maps onto \(\mathbb{R}\), that for some particular value of \(X=x\), \(g(X)\) maps onto a constant value.

\begin{itemize}
\tightlist
\item
  Deriving a Function to Minimize MSE
\end{itemize}

\[
\begin{aligned}
  E[(Y - g(X))^2|X]
      &= E[Y^2 - 2Yg(X) + g^2(X)|X]                                \\
      &= E[Y^2|X] + E[-2Yg(X)|X] + E[g^2(X)|X]                     \\
      &= E[Y^2|X] - 2g(X)E[Y|X] + g^2(X)E[1|X]                     \\
      &= (E[Y^2|X] - E^2[Y|X]) + (E^2[Y|X] - 2g(X)E[Y|X] + g^2(X)) \\
      &= V[Y|X] + (E^2[Y|X] - 2g(X)E[Y|X] + g^2(X))                \\
      &= V[Y|X] + (E[Y|X] - g(X))^2                                \\
\end{aligned} 
\]

Notice too that we can use the \emph{Law of Iterated Expectations} to do something useful. (This is a good point to talk about how this theorem works in your breakout groups.)

\[
\begin{aligned}
  E[(Y-g(X))^2] &= E\big[E[(Y-g(X))^2|X]\big]     \\ 
    &=E\big[V[Y|X]+(E[Y|X]-g(X))^2\big]           \\
    &=E\big[V[Y|X]\big]+E\big[(E[Y|X]-g(X))^2\big]\\
\end{aligned}
\]

\begin{itemize}
\tightlist
\item
  \(E[V[Y|X]]\) doesn't depend on \(g\); and,
\item
  \(E[(E[Y|X]-g(X))^2] \geq 0\).
\end{itemize}

\(\therefore g(X) = E[Y|X]\) gives the smallest \(E[(Y-g(X))^2]\)

\hypertarget{the-implication}{%
\subsection{The Implication}\label{the-implication}}

If you are choosing some \(g\), you can't do better than \(g(x) = E[Y|X=x]\).

\hypertarget{working-with-the-blp}{%
\section{Working with the BLP}\label{working-with-the-blp}}

Why Linear?

\begin{itemize}
\item
  In some cases, we might try to estimate the CEF. More commonly, however, we work with linear predictors. Why?
\item
  We don't know joint density function of \(Y\). So, it is ``difficult'' to derive a suitable CEF.
\item
  To estimate \emph{flexible} functions requires considerably more data. Assumptions about distribution (e.g.~a linear form) allow you to leverage those assumptions to learn `more' from the same amount of data.
\item
  Other times, the CEF, even if we \emph{could} produce an estimate, might be so complex that it isn't useful or would be difficult to work with.
\item
  And, many times, linear predictors (which might seem trivially simple) actually do a very good job of producing predictions that are `close' or useful.
\end{itemize}

\#\#Joint Distribution Practice

\hypertarget{professorial-mistakes-discrete-rvs}{%
\subsection{Professorial Mistakes (Discrete RVs)}\label{professorial-mistakes-discrete-rvs}}

\begin{itemize}
\item
  Let the number of questions that students ask be a RV, \(X\).\\
\item
  Let \(X\) take on values: \(\{1, 2, 3\}\), each with probability \(1/3\).\\
\item
  Every time a student asks a question, the instructor answers incorrectly with probability \(1/4\), independently of other questions.
\item
  Let the RV \(Y\) be number of incorrect responses.
\item
  \textbf{Questions:}

  \begin{itemize}
  \tightlist
  \item
    Compute the expectation of \(Y\), conditional on \(X\), \(E[Y|X]\)
  \item
    Using the law of iterated expectations, compute \(E[Y] = E\big[E[Y|X]\big]\).
  \end{itemize}
\end{itemize}

\hypertarget{learning-from-random-samples}{%
\chapter{Learning from Random Samples}\label{learning-from-random-samples}}

\begin{figure}
\centering
\includegraphics{./images/south_hall.jpeg}
\caption{south hall}
\end{figure}

\hypertarget{goals-framework-and-learning-objectives}{%
\section{Goals, Framework, and Learning Objectives}\label{goals-framework-and-learning-objectives}}

\hypertarget{class-announcements-3}{%
\subsection{Class Announcements}\label{class-announcements-3}}

\begin{itemize}
\tightlist
\item
  You're done with probability theory. Congrats!
\item
  You're also done with your first test. Congrats!
\item
  We're going to have a second test in a few weeks; then we're done testing for the semester
\end{itemize}

\hypertarget{learning-objectives-4}{%
\subsection{Learning Objectives}\label{learning-objectives-4}}

At the end of this week, students will be able to

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Understand} what iid sampling is, and evaluate whether the assumption of iid sampling is sufficiently plausible to engage in frequentist modeling.
\item
  \textbf{Appreciate} that with iid sampling, summarizing functions of random variables are, themselves, random variables with probability distributions and values that they obtain.
\item
  \textbf{Recall} the definition of an estimator,
\item
  \textbf{Recall} definition of an estimator, \textbf{state} and \textbf{understand} the desirable properties of estimators, and \textbf{evaluate} whether an estimator possesses those desirable properties.
\item
  \textbf{Distinguish} between the concepts of \{expectation \& sample mean\}, \{variance \& unbiased sample variance estimator, sampling-based variance in the sample mean\}.
\end{enumerate}

\hypertarget{roadmap-1}{%
\subsection{Roadmap}\label{roadmap-1}}

\hypertarget{where-were-going-coming-attractions}{%
\subsubsection{Where We're Going -- Coming Attractions}\label{where-were-going-coming-attractions}}

\begin{itemize}
\tightlist
\item
  We're going to start bringing data into our work
\item
  First, we're going to develop a testing framework that is built on sampling theory and reference distributions -- \texttt{t.tests}, \texttt{wilcox.test} and the like
\item
  Second, we're going to show that OLS regression is the sample estimator of the BLP
\item
  Third, we're going to use the testing distribution to test regression coefficients
\end{itemize}

\hypertarget{where-weve-been-random-variables-and-probability-theory}{%
\subsubsection{Where We've Been -- Random Variables and Probability Theory}\label{where-weve-been-random-variables-and-probability-theory}}

\begin{itemize}
\tightlist
\item
  Statisticians create a model (A.K.A. population) to represent the world.
\item
  That model can be described by parameters like expecation, covariance.
\item
  So far, these parameter values have come from our imaginations
\end{itemize}

\hypertarget{where-we-are}{%
\subsubsection{Where we Are}\label{where-we-are}}

\begin{itemize}
\tightlist
\item
  We want to fit models -- use data to set their parameter values.
\item
  A sample is a set of random variables
\item
  Sample statistics are functions of a sample, and they are random variables
\item
  Under iid and other assumptions, we get useful properties:

  \begin{itemize}
  \tightlist
  \item
    Statistics may be consistent estimators for population parameters
  \item
    The distribution of sample statistics may be asymptotically normal
  \end{itemize}
\end{itemize}

\hypertarget{key-terms-and-assumptions}{%
\section{Key Terms and Assumptions}\label{key-terms-and-assumptions}}

\hypertarget{iid}{%
\subsection{IID}\label{iid}}

For each scenario, is the IID assumption plausible?

\begin{itemize}
\tightlist
\item
  Call a random phone number. If someone answers, interview all persons in the household. Repeat until you have data on 100 people.
\item
  Call a random phone number, interview the person if they are over 30. Repeat until you have data on 100 people.
\item
  Record year-to-date price change for 20 largest car manufacturers.
\item
  Measure net exports per GDP for all 195 countries recognized by the UN.
\end{itemize}

\hypertarget{definitions}{%
\subsection{Definitions}\label{definitions}}

Define each of the following:

\begin{itemize}
\tightlist
\item
  Sample
\item
  Sample Statistic
\item
  Estimator
\item
  Bias
\item
  Efficiency
\item
  Consistency
\item
  Convergence in Probability
\item
  Convergence in Distribution
\end{itemize}

\hypertarget{understanding-sampling-distributions}{%
\section{Understanding Sampling Distributions}\label{understanding-sampling-distributions}}

\begin{itemize}
\tightlist
\item
  Let \(X\) be a Bernoulli random variable representing an unfair coin with \(P(X=1) = 0.7\).
\item
  You have an iid sample of size 2, \((X_1,X_2)\).
\item
  Compute the sampling distribution of \(\overline X = \frac{X_1+X_2}{2}\).
\end{itemize}

\includegraphics{203-live-session_files/figure-latex/make axes-1.pdf}

\textbf{Questions:}

\begin{itemize}
\tightlist
\item
  Explain the difference between a population distribution and the sampling distribution of a statistic.
\item
  As we toss more and more coins, \(\overline X_{(100)} \rightarrow \overline X_{(10000)}\) what will the value of \(\overline X\) get closer to? What law generates this, and why does this law generate this result?
\item
  Why do we want to know things about the sampling distribution of a statistic?
\end{itemize}

\hypertarget{uncertainty}{%
\section{Uncertainty}\label{uncertainty}}

\textbf{Which Result is Better?}

\begin{itemize}
\tightlist
\item
  Suppose that you measure salary data among individuals who try different strategies
\item
  Report out in the following table:
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2941}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2059}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3088}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1912}}@{}}
\toprule
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Early Rising
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Mindfulness Retreat
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MIDS Degree
\end{minipage} \\
\midrule
\endhead
Increase in Salary & \$1020 & \$5130 & \$9200 \\
\(SE\) & (\$350) & (\$4560) & \\
\(N =\) & 1,000 & 77 & 700 \\
\bottomrule
\end{longtable}

\emph{(Standard errors in parentheses when available)}

\hypertarget{standard-errors}{%
\section{Standard Errors}\label{standard-errors}}

Errors in stating standard errors frequently occur. (We wanted to start with what might the most vapid, but also confusing statement possible!)

Standard errors are a statement about the sampling variance of the sample average. But, related to this concept are the ideas of the \emph{Population Variance}, the \emph{Plug-In Estimator for the Sample Variance}, the \emph{Unbiased Sample Variance}, and, finally, the \emph{Sampling Variance of the Sample Average} (i.e the \emph{Standard Error}).

How are each of these concepts related to one another, and how can we keep them all straight? As a group, fill out the following columns?

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2255}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1765}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2157}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3824}}@{}}
\toprule
\begin{minipage}[b]{\linewidth}\raggedright
Population Concept
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Sample Estimator
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Estimator Properties
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Sampling Variance of Sample Estimator
\end{minipage} \\
\midrule
\endhead
Expected Value & & & \\
Population Variance & & & \\
Population Covariance & & & \\
CEF & & & \\
BLP & & & \\
\bottomrule
\end{longtable}

\hypertarget{write-code-to-demo-the-central-limit-theorem-clt}{%
\section{Write Code to Demo the Central Limit Theorem (CLT)}\label{write-code-to-demo-the-central-limit-theorem-clt}}

\hypertarget{motivating-the-central-limit-theorem-clt}{%
\subsection{Motivating the Central Limit Theorem (CLT)}\label{motivating-the-central-limit-theorem-clt}}

\begin{itemize}
\tightlist
\item
  Standard Errors tell us a lot about the uncertainty in our statistics
\item
  But we want to say more:

  \begin{itemize}
  \tightlist
  \item
    How confident are we that this vitamin has a positive effect?
  \item
    How plausible is a mean income \$1000 below our estimate?
  \end{itemize}
\item
  For these questions, we need to know the sampling distribution of our statistic.
\item
  How is this possible when we don't know the population distribution?
\end{itemize}

\hypertarget{sampling-from-the-bernoulli-distribution-in-r}{%
\subsection{Sampling from the Bernoulli Distribution in R}\label{sampling-from-the-bernoulli-distribution-in-r}}

\begin{itemize}
\tightlist
\item
  To demonstrate the CLT, we chose a Bernoulli distribution with parameter \(p\).

  \begin{itemize}
  \tightlist
  \item
    This distribution is very simple
  \item
    This distribution is non-normal, and can be very skewed depending on \(p\).
  \end{itemize}
\item
  First, set \texttt{p=0.5} so your population distribution is symmetric. Use a variable \texttt{n} to represent your sample size. Initially, set \texttt{n=3}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{3}
\NormalTok{p }\OtherTok{\textless{}{-}} \FloatTok{0.5}
\end{Highlighting}
\end{Shaded}

\hypertarget{useful-r-commands}{%
\subsection{Useful R Commands}\label{useful-r-commands}}

\textbf{sample() or rbinom()}

\begin{itemize}
\tightlist
\item
  R doesn't have a \texttt{bernoulli} function.
\item
  To simulate draws from a Bernoulli variable, you can either:

  \begin{itemize}
  \tightlist
  \item
    Use \texttt{sample}
  \item
    Or, use \texttt{rbinom} (the Bernoulli distribution is a special case of a binomial distribution. In this function, \texttt{size} refers to a distribution parameter, not the number of draws.)
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sample}\NormalTok{(}\AttributeTok{x=}\DecValTok{0}\SpecialCharTok{:}\DecValTok{1}\NormalTok{, }\AttributeTok{size=}\NormalTok{n, }\AttributeTok{replace=}\ConstantTok{TRUE}\NormalTok{, }\AttributeTok{prob=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{p, p))}
\FunctionTok{rbinom}\NormalTok{(}\AttributeTok{n=}\NormalTok{n, }\AttributeTok{size=}\DecValTok{1}\NormalTok{, }\AttributeTok{prob=}\NormalTok{p)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0 0 0
## [1] 0 0 0
\end{verbatim}

\textbf{replicate()}

\begin{itemize}
\tightlist
\item
  To repeat an action, you can use \texttt{replicate}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{replicate}\NormalTok{(}\DecValTok{10}\NormalTok{, }\FunctionTok{log}\NormalTok{(}\DecValTok{10}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 2.302585 2.302585 2.302585 2.302585 2.302585 2.302585 2.302585 2.302585 2.302585 2.302585
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{bins =} \StringTok{\textquotesingle{}8\textquotesingle{}}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\includegraphics{203-live-session_files/figure-latex/ggplot histogram-1.pdf}

\hypertarget{exercise}{%
\section{Exercise}\label{exercise}}

\textbf{Part 1}

Throughout this part, we will use fair coins (\texttt{p\ =\ 0.5}).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Fill in the function below so that it simulates taking n draws from a Bernoulli distribution with parameter p.~This is like tossing n coins at the same time. Use the \texttt{mean} function to compute the sample mean -- the average of the number of heads that are showing. Make sure that when you run it, you return values in \(\{0,1/3,2/3,1\}\).
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{experiment }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(n, p)\{}

\NormalTok{  \}}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  The sample mean is a random variable. To understand it, use the visualization trick from a few weeks ago. Use the \texttt{replicate} function to run the above experiment 1000 times, and plot a histogram of the results.
\item
  If you replicate the experiment enough times, will the distribution ever look normal? Why or why not?
\item
  Use \texttt{sd()} to check the standard deviation of the sampling distribution of the mean for \texttt{number\_of\_coins\ =\ 3}. What sample size is needed to decrease the standard deviation by a factor of 10? Check that your answer is correct.
\end{enumerate}

\textbf{Part 2}

For this part, we'll continue to study a fair coin.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Try different values for the sample size n, and examine the shape of the sampling distribution of the mean. At what point does it look normal to you?
\end{enumerate}

\textbf{Part 3}

For this part, we'll study a very unfair coin. \texttt{p\ =\ 0.01}.

This is an example of a highly skewed random variable. That roughly means that one tail is a lot longer than the other.

For this activity, you can simply use your eyes to gauge how skewed a distribution is. If you prefer, you can also use the skewness command in the univar package to measure skewness. You may hear a rule of thumb that a skewness above 1 or below -1 is a highly skewed distribution.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\item
  Start with n=3 as before. What do you notice about the shape of the sampling distribution?
\item
  Try different values for the sample size n, and examine the shape of the sampling distribution of the mean. At what point does it look normal to you?
\end{enumerate}

\hypertarget{discussion-questions}{%
\subsection{Discussion Questions}\label{discussion-questions}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  How does the skewness of the population distribution affect the applicability of the Central Limit Theorem? What lesson can you take for your practice of statistics?
\item
  Name a variable you would be interested in measuring that has a substantially skewed distribution.
\item
  One definition of a heavy tailed distribution is one with infinite variance. For example, you can use the \texttt{rcauchy} command in R to take draws from a Cauchy distribution, which has heavy tails. Do you think a ``heavy tails'' distribution will follow the CLT? What leads you to this intuition?
\end{enumerate}

\hypertarget{hypothesis-testing}{%
\chapter{Hypothesis Testing}\label{hypothesis-testing}}

Frequentist Hypothesis testing is a very well established framework in the applied practice, and scientific literature. Sometimes (often, currently) referred to as Null Hypothesis Significance Testing, this framework essentially makes an absurd assumption and asks the data to overturn that absurd assumption.

Like a petulant child, NHST essentially proclaims,

\begin{quote}
``Oh yeah? Well, if you actually loved me then you would buy me an iPad!''
\end{quote}

Which is to say, it says, ``Oh yeah? If this absurd thing isn't true, prove it to me!''

\begin{figure}
\centering
\includegraphics{./images/frequentists_vs_bayesians.png}
\caption{update!}
\end{figure}

\hypertarget{what-is-frequentist-testing-doing}{%
\subsection*{What is Frequentist testing doing?}\label{what-is-frequentist-testing-doing}}
\addcontentsline{toc}{subsection}{What is Frequentist testing doing?}

This testing framework works on \textbf{samples} of data, and applies \textbf{estimators} to produce \textbf{estimates} of \textbf{population parameters} that are fundamentally unknown and unknowable. Despite this unknown and unknowable population target, with some carefully written down estimators we can rely on the convergence characteristics of some estimators to produce \emph{useful}, \emph{reliable} results.

We begin with the one-sample t-test. The one-sample t-test relies on the sample average as an estimator of a population expectation. In doing so, it relies on the effectiveness of the \textbf{Weak Law of Large Numbers} and the \textbf{Central Limit Theorem} to guarantee that the estimator that \textbf{converges in probability} to the population expectation, while also \textbf{converging in distribution} to a Gaussian distribution.

These two convergences permit a data scientist to make several \textbf{inferences} based on data:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The probability of generating data that \emph{``looks like what is observed''}, if the null-hypothesis were true. This is often referred to as the \textbf{p-value} of a test, and is the petulant statement identified above.
\item
  An interval of values that, with some stated probability (e.g.~95\%), contains the true population parameter.
\end{enumerate}

This framework begins a \textbf{exceedingly important} task that we must understand, and undertake when we are working as data scientists: Producing our best-estimate, communicating how we arrived at that estimate, what (if any) guarantees that estimate provides, and \emph{crucially} all \textbf{limitations} of our estimate.

\hypertarget{learning-objectives-5}{%
\section{Learning Objectives}\label{learning-objectives-5}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Understand} the connection between random variables, sampling, and statistical tests.
\item
  \textbf{Apply} the Frequentist testing framework in a simple test -- the one-sample t-test.
\item
  \textbf{Anticipate} that every additional Frequentist test is a closely related variant of this test.
\end{enumerate}

\hypertarget{class-announcements-4}{%
\section{Class Announcements}\label{class-announcements-4}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  You will be taking your second test this week. The test follows the same format as the first test, which we will discuss in live session. The test will cover \emph{Unit 4: Conditional Expectation and the Best Linear Predictor} and \emph{Unit 5: Learning from Random Samples.}
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Like the first test, our goal is to communicate to you what concepts we think are important, and then to test those concepts directly, and fairly. The purpose of the test is to give you an incentive to review what you have learned through probability theory, and then to demonstrate that you can produce work based on that knowledge.
\item
  There is another practice test on Gradescope, and in the GitHub repository.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  In rosier news, we're moving out of the \emph{only} pencil and paper section of this course, and bringing what we have learned out into the dirty world of data. This means a few things:
\end{enumerate}

\begin{itemize}
\tightlist
\item
  If you haven't yet worked through the \textbf{R Bridge Course} that is available to you, working on this bridge course will be useful for you (after you complete your test). The goal of the course is to get you up and running with reasonably successful code and workflows for the data-based portion of the course.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  We will assign teams, and begin our work on \textbf{Lab 1} in Live Session next week. This is a two-week, group lab that you will work on with three total team-mates. The lab will cover some of the fundamentals of hypothesis tests,
\end{enumerate}

\hypertarget{roadmap-2}{%
\section{Roadmap}\label{roadmap-2}}

\textbf{Looking Backwards}

\begin{itemize}
\tightlist
\item
  Statisticians create a model to represent the world
\item
  We saw examples of estimators, which approximate model parameters we're interested in.
\item
  By itself, an estimate isn't much good; we need to capture the uncertainty in the estimate.
\item
  We've seen two ways to express uncertainty in an estimator: standard errors and confidence intervals.
\end{itemize}

\textbf{Today}

\begin{itemize}
\tightlist
\item
  We introduce hypothesis testing

  \begin{itemize}
  \tightlist
  \item
    A hypothesis test also captures uncertainty, but in relation to a specific hypothesis.
  \end{itemize}
\end{itemize}

\textbf{Looking Ahead}

\begin{itemize}
\tightlist
\item
  We'll build on the one-sample t-test, to introduce several other statistical tests.
\item
  We'll see how to choose a test from different alternatives, with an eye on meeting the required assumptions, and maximizing power.
\end{itemize}

\hypertarget{what-does-a-hypothesis-test-do}{%
\section{What does a hypothesis test do?}\label{what-does-a-hypothesis-test-do}}

\begin{itemize}
\tightlist
\item
  What are the two possible outcomes of a hypothesis test?
\item
  What are the four-possible combinations of (a) hypothesis test rest; and (b) state of the world?
\item
  Does a hypothesis test always have to report a result that is consistent with the state of the world? What does it mean if it \emph{does}, and what does it mean if it \emph{does not}.
\item
  What if you made up your own testing framework, called the \{Your Last Name's\} Groundhog test. Which is literally a groundhog looking to see its shadow. Because you made the test, suppose that you know that it is \emph{totally random} whether a groundhog sees its shadow. \emph{How useful would this test be at separating states of the world?}\\
\item
  What guarantee do you get if you follow the decision rules properly?
\item
  Why do we standardize the mean to create a test statistic?
\end{itemize}

\[ 
  t = \frac{ \overline{X}_n - \mu}{\sqrt{\frac{s^2}{n}}}
\]

\hypertarget{madlib-prompt}{%
\section{Madlib prompt}\label{madlib-prompt}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tone\_of\_voice     }\OtherTok{\textless{}{-}} \StringTok{\textquotesingle{}yell\textquotesingle{}}
\NormalTok{mode\_of\_speech    }\OtherTok{\textless{}{-}} \StringTok{\textquotesingle{}call\textquotesingle{}}
\NormalTok{superlative       }\OtherTok{\textless{}{-}} \StringTok{\textquotesingle{}worst\textquotesingle{}}
\NormalTok{score\_on\_test     }\OtherTok{\textless{}{-}} \StringTok{\textquotesingle{}70\textquotesingle{}}
\NormalTok{name\_of\_classmate }\OtherTok{\textless{}{-}} \StringTok{\textquotesingle{}victor\textquotesingle{}}
\NormalTok{emotion           }\OtherTok{\textless{}{-}} \StringTok{\textquotesingle{}ennui\textquotesingle{}}
\NormalTok{eating\_verb       }\OtherTok{\textless{}{-}} \StringTok{\textquotesingle{}smack\textquotesingle{}} \CommentTok{\#slurp}
\NormalTok{vessel            }\OtherTok{\textless{}{-}} \StringTok{\textquotesingle{}bowl\textquotesingle{}}
\NormalTok{thing\_found\_in\_compost }\OtherTok{\textless{}{-}} \StringTok{\textquotesingle{}bannana peel\textquotesingle{}}
\end{Highlighting}
\end{Shaded}

\hypertarget{madlib-completed}{%
\section{Madlib completed}\label{madlib-completed}}

Suppose that a classmate comes to you, and, in a yell voice call, ``Hey, I've got something that is worst for statistics. All you've got to do to get 70 on Test 2 and make victor ennui is smack this bowl of bannana peel.

You're skeptical, but also curious because that last test was tough. Good.

Acknowledging that we're only 40\% of the way through the course, you decide to hire a hungry, underpaid PhD student the School to conduct the experiment to evaluate this claim. They report back, no details about the test, but they do tell you, ``We're sure there's no effect of bannana peel.''

\begin{itemize}
\tightlist
\item
  Do you believe them?
\item
  What, if any, reasons can you imagine not to believe this conclusion?
\end{itemize}

\hypertarget{all-joking-aside}{%
\section{All joking aside}\label{all-joking-aside}}

Explain this joke:

\begin{figure}
\centering
\includegraphics{./images/null_hypothesis.png}
\caption{har har har}
\end{figure}

\hypertarget{manual-computation-of-a-t-test}{%
\section{Manual Computation of a t-Test}\label{manual-computation-of-a-t-test}}

In a warehouse full of power packs labeled as 12 volts we randomly measured the voltage of 7. Here is the data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{voltage }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{11.77}\NormalTok{, }\FloatTok{11.90}\NormalTok{, }\FloatTok{11.64}\NormalTok{, }\FloatTok{11.84}\NormalTok{, }\FloatTok{12.13}\NormalTok{, }\FloatTok{11.99}\NormalTok{,  }\FloatTok{11.77}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Find the mean and the standard deviation.
\item
  Using \texttt{qt()}, compute the t critical value for a hypothesis test for this sample.
\item
  Define a test statistic, \(t\), for testing whether the population mean is 12.
\item
  Calculate the p-value using the t statistic.
\item
  Should you reject the null? Argue this in two different ways. (Following convention, set \(\alpha = .05\).)
\item
  Suppose you were to use a normal distribution instead of a t-distribution to test your hypothesis. What would your p-value be for the z-test?
\item
  Without actually computing it, say whether a 95\% confidence interval for the mean would include 12 volts.
\item
  Compute a 95\% confidence interval for the mean.
\end{enumerate}

\hypertarget{data-exercise}{%
\section{Data Exercise}\label{data-exercise}}

\textbf{t-Test Micro Cheat Sheet}

\begin{itemize}
\tightlist
\item
  Key t-Test Assumptions

  \begin{itemize}
  \tightlist
  \item
    Metric variable
  \item
    IID
  \item
    No major deviations from normality, considering sample size
  \end{itemize}
\end{itemize}

\textbf{Testing the Home Team Advantage}

The file athlet2.Rdata contains data on college football games. The data is provided by Wooldridge and was collected by Paul Anderson, an MSU economics major, for a term project. Football records and scores are from 1993 football season.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{load}\NormalTok{(}\StringTok{"data/athlet2.RData"}\NormalTok{)}
\NormalTok{data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    dscore dinstt doutstt htpriv vtpriv dapps htwrd vtwrd dwinrec dpriv
## 1      10   -409   -4679      0      0 -1038     1     1       0     0
## 2     -14     NA     -66      0      0 -7051     1     1       0     0
## 3      23   -654    -637      0      0  6209     1     0       1     0
## 4       8   -222     456      0      0  -129     1     1       0     0
## 5     -12    -10     208      0      0   794     1     1       0     0
## 6       7    494      17      0      0   411     0     0       0     0
## 7     -21      2       2      0      0 -4363     1     1       0     0
## 8      -5     96    -333      0      0  1144     1     0       1     0
## 9      -3    223    2526      0      0  3956     0     0       0     0
## 10    -32    -20       0      0      0  -641     0     1      -1     0
## 11      9     66       0      0      0  -278     1     0       1     0
## 12      1     56    -346      0      0 -2223     1     0       1     0
## 13      7    556     717      0      0 -5217     1     0       1     0
## 14    -20    169    -461      0      0  1772     0     1      -1     0
## 15     35   -135     396      0      0    85     1     0       1     0
## 16     35    -40       0      0      0  -988     1     0       1     0
## 17    -25     24       0      0      0 -8140     1     1       0     0
## 18     -9     90       0      0      0  8418     0     1      -1     0
## 19    -33     27     900      0      0 -3273     0     0       0     0
## 20      7    -89     -31      0      0  1906     1     0       1     0
## 21     -3    536    2352      0      0  -151     1     1       0     0
## 22     -6  13261    9111      1      0 -9936     1     1       0     1
## 23    -29  13809   10076      1      0 -6265     0     1      -1     1
## 24     14 -17631  -10589      0      1  1252     1     0       1    -1
## 25    -18  14885    9983      1      0 -4529     1     1       0     1
## 26     48 -15220  -11400      0      1  -318     1     0       1    -1
## 27     -3     99     -29      0      0  -797     0     1      -1     0
## 28     -3    -54     -88      0      0  -372     0     1      -1     0
## 29     -3    -98   -4175      1      0  2460     1     1       0     1
## 30      2   -304    2987      0      1 -3035     1     1       0    -1
\end{verbatim}

We are especially interested in the variable, dscore, which represents the score differential, home team score - visiting team score. We would like to test whether a home team really has an advantage over the visiting team.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The instructor will assign you to one of two teams. Team 1 will argue that the t-test is appropriate to this scenario. Team 2 will argue that the t-test is invalid. Take a few minutes to examine the data, then formulate your best argument.
\item
  Should you perform a one-tailed test or a two-tailed test? What is the strongest argument for your answer?
\item
  Execute the t-test and interpret every component of the output.
\item
  Based on your output, suggest a different hypothesis that would have led to a different test result. Try executing the test to confirm that you are correct.
\end{enumerate}

\hypertarget{assumptions-behind-the-t-test}{%
\section{Assumptions Behind the t-test}\label{assumptions-behind-the-t-test}}

For the following scenarios, what is the strongest argument against the validity of a t-test?

\begin{itemize}
\item
  You have a sample of 50 CEO salaries, and you want to know whether the mean salary is greater than \$1 million.
\item
  A nonprofit organization measures the percentage of students that pass an 8th grade reading test in 40 neighboring California counties. You are interested in whether the percentage of students that pass in California is over 80\%
\item
  You have survey data in which respondents assess their own opinion of corgis, with options ranging from ``1 - extreme disgust'' to ``5 - affection so intense it threatens my career.'' You want to know whether people on the average like corgis more than 3, representing neutrality.
\end{itemize}

\hypertarget{comparing-two-groups}{%
\chapter{Comparing Two Groups}\label{comparing-two-groups}}

\includegraphics{./images/goin_left.jpeg}

\hypertarget{learning-objectives-6}{%
\section{Learning Objectives}\label{learning-objectives-6}}

This week, we introduce the idea of comparing two groups to evaluate whether the data that we have sampled lead us to believe that the population distribution of the random variables are different. Of course, because we don't get access to the function that describes the random variable, we can't \emph{actually} know if the populations are different. It is for this reason that we call it statistical inference -- we are inferring from a sample some belief about the populations.

At the conclusion of this week, students will be able to:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{Recognize} the similarities between all frequentist hypothesis tests.
\item
  \emph{Evaluate} the conditions that surround the data, and choose a test that is best-powered and justifiable.
\item
  \emph{Perform} and \emph{Interpret} the results of the most common statistical tests.
\end{enumerate}

\hypertarget{class-announcements-5}{%
\section{Class Announcements}\label{class-announcements-5}}

\begin{itemize}
\tightlist
\item
  Great work completing your final w203 test!
\item
  There is \textbf{no} unit 7 homework!
\item
  The Hypothesis Testing Lab is released today!

  \begin{itemize}
  \tightlist
  \item
    Lab is due at Unit 09 Live Session (two weeks)
  \item
    Group lab in two parts:

    \begin{itemize}
    \tightlist
    \item
      \emph{Part 1}: Work as a team to engage the fundamentals of hypothesis tests
    \item
      \emph{Part 2}: Apply these fundamentals to analyze 2020 election data and write a single, three-page analysis
    \end{itemize}
  \end{itemize}
\end{itemize}

\hypertarget{roadmap-3}{%
\section{Roadmap}\label{roadmap-3}}

\hypertarget{rearview-mirror-1}{%
\subsection{Rearview Mirror}\label{rearview-mirror-1}}

\begin{itemize}
\tightlist
\item
  Statisticians create a population model to represent the world
\item
  A population model has parameters we are interested in

  \begin{itemize}
  \tightlist
  \item
    Ex: A parameter might represent the effect that a vitamin has on test performance
  \end{itemize}
\item
  A null hypothesis is a specific statement about a parameter

  \begin{itemize}
  \tightlist
  \item
    Ex: The vitamin has zero effect on performance
  \end{itemize}
\item
  A hypothesis test is a procedure for rejecting or not rejecting a null, such the probability of a type 1 error is constrained.
\end{itemize}

\hypertarget{today}{%
\subsection{Today}\label{today}}

\begin{itemize}
\tightlist
\item
  There are often multiple hypothesis tests you can apply to a scenario.
\item
  Our primary concern is choosing a test with assumptions we can defend.
\item
  Secondarily, we want to maximize power.
\end{itemize}

\hypertarget{looking-ahead}{%
\subsection{Looking ahead}\label{looking-ahead}}

\begin{itemize}
\tightlist
\item
  Next week, we start working with models for linear regression
\item
  We will see how hypothesis testing is also used for regression parameters.
\end{itemize}

\hypertarget{teamwork-discussion}{%
\section{Teamwork Discussion}\label{teamwork-discussion}}

\hypertarget{working-on-data-science-teams}{%
\subsection{Working on Data Science Teams}\label{working-on-data-science-teams}}

Data science is a \emph{beautiful} combination of team-work and individual-work. It provides the opportunity to work together on a data pipeline with people from all over the organization, to deal with technical, statistical, and social questions that are always interesting. While we expect that everyone on a team will be a professional, there is so much range within the pursuit of data science that projects are nearly always collaborative exercises.

Together as teams, we

\begin{itemize}
\tightlist
\item
  Define research ambitions and scope
\item
  Imagine/envision the landscape of what is possible
\item
  Support, discuss, review and integrate individual contributions
\end{itemize}

Together as individuals we conduct the heads-down work that moves question answering forward. This might be reading papers to determine the most appropriate method to bring to bear on the question, or researching the data that is available, or understanding the technical requirements that we have to meet for this answer to be useful to our organization in real time.

What is your instructor \emph{uniquely} capable of? Let them tell you! But, at the same time, what would they acknowledge that others are better than them?

See, the thing is, because there is so much that has to be done, there literally are very, very few people who are one-stop data science shops. Instead, teams rely on collaboration and joint expertise in order to get good work done.

\hypertarget{the-problematic-psychology-of-data-science}{%
\subsection{The Problematic Psychology of Data Science}\label{the-problematic-psychology-of-data-science}}

People talk about the \emph{impostor syndrome}: a feeling of inadequacy or interloping that is sometimes also associated with a fear of under-performing relative to the expectation of others on the team. These emotions are common through data science, academics, computer science. But, these types of emotions are also commonplace in journalism, film-making, and public speaking.

Has anybody ever had the dream that they're late to a test? Or, that that they've got to give a speech that they're unprepared for? Does anybody remember playing an instrument as a kid and having to go to recitals? Or, play for a championship on a youth sports team? Or, go into a test two?

What are the feelings associated with those events? What might be generating these feelings?

\includegraphics[width=0.25\textwidth,height=\textheight]{images/among_us.jpeg}

\hypertarget{what-makes-an-effective-team}{%
\subsection{What Makes an Effective Team?}\label{what-makes-an-effective-team}}

\begin{itemize}
\tightlist
\item
  This reading on \href{https://rework.withgoogle.com/print/guides/5721312655835136/}{\emph{effective} teams} summarizes academic research to argue:
\end{itemize}

\begin{quote}
What really matters to creating an effective tema is less about who is on the team, and more about how the team works together.
\end{quote}

In your live session, your section might take 7 minutes to read this brief. If so, please read the following sections:

\begin{itemize}
\tightlist
\item
  The problem statement;
\item
  The proposed solution;
\item
  The framework for team effectiveness, stopping at the section titled \emph{``Tool: Help teams determine their own needs.''}
\end{itemize}

\begin{quote}
``Psychological safety refers to an individual's perception of the consequences of taking an interpersonal risk. It is a belief that a team is safe for risk taking in the face of being seen as ignorant, incompetent, negative, or disruptive.''

``In a team with high psychological safety, teammates feel safe to take risks around their team members. They feel confident that no one on the team will embarrass or punish anyone else for admitting a mistake, asking a question, or offering a new idea.''
\end{quote}

\hypertarget{we-all-belong}{%
\subsection{We All Belong}\label{we-all-belong}}

\begin{itemize}
\tightlist
\item
  From your experience, can you give an example of taking a personal risk as part of a team?

  \begin{itemize}
  \tightlist
  \item
    Can you describe your emotions when contemplating this risk?
  \item
    If you did take the risk, how did the reactions of your teammates affect you?
  \end{itemize}
\item
  Knowing the circumstances that generate feelings of anxiety -- what steps can we take as a section, or a team, to recognize and respond to these circumstances?
\end{itemize}

\begin{quote}
How can you add to the psychological safety of your peers in the section and lab teammates?
\end{quote}

\hypertarget{team-kick-off}{%
\section{Team Kick-Off}\label{team-kick-off}}

\textbf{Lab 1 Teams}

\begin{itemize}
\tightlist
\item
  Here are teams for Lab 1!
\end{itemize}

\textbf{Team Kick-Off Conversation}

\begin{itemize}
\tightlist
\item
  In a 10 minute breakout with your team, please discuss the following questions:
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  How much time will you invest in the lab each week?
\item
  How often will you meet and for how long?
\item
  How will you discuss, review, and integrate individual work into the team deliverable?
\item
  What do you see as the biggest risks when working on a team? How can you contribute to an effective team dynamic?
\end{enumerate}

\hypertarget{a-quick-review}{%
\section{A Quick Review}\label{a-quick-review}}

\textbf{Review of Key Terms}

\begin{itemize}
\tightlist
\item
  Define each of the following:

  \begin{itemize}
  \tightlist
  \item
    Population Parameter
  \item
    Null Hypothesis
  \item
    Test Statistic
  \item
    Null Distribution
  \end{itemize}
\end{itemize}

\textbf{Comparing Groups Review}

Take a moment to recall the tests you learned this week. Here is a quick cheat-sheet to their key assumptions.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1097}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5226}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3677}}@{}}
\toprule
\begin{minipage}[b]{\linewidth}\raggedright
paired/unpaired
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
parametric
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
non-parametric
\end{minipage} \\
\midrule
\endhead
unpaired & \textbf{unpaired t-test} - metric var - i.i.d. - (not too un-)normal & \textbf{Wilcoxon rank-sum} ordinal var i.i.d.  \\
paired & \textbf{paired t-test} metric var i.i.d. (not too un-)normal & \textbf{Wilcoxon signed-rank} metric var i.i.d. difference is symmetric \textbf{sign test} ordinal var i.i.d. \\
\bottomrule
\end{longtable}

\hypertarget{comparing-groups-r-exercise}{%
\section{Comparing Groups R Exercise}\label{comparing-groups-r-exercise}}

The General Social Survey (GSS) is one of the longest running and extensive survey projects in the US. The full dataset includes over 1000 variables spanning demographics, attitudes, and behaviors. The file \texttt{GSS\_w203.RData} contains a small selection of a variables from the 2018 GSS.

To learn about each variable, you can enter it into the search bar at the \href{https://gssdataexplorer.norc.org/variables/vfilter}{GSS data explorer}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{load}\NormalTok{(}\StringTok{\textquotesingle{}data/GSS\_w203.RData\textquotesingle{}}\NormalTok{)}
\FunctionTok{summary}\NormalTok{(GSS)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##            rincome               happy                           sexnow        wwwhr       
##  $25000 or more: 851   very happy   : 701   women                   :758   Min.   :  0.00  
##  $20000 - 24999: 107   pretty happy :1307   man                     :640   1st Qu.:  3.00  
##  $10000 - 14999:  94   not too happy: 336   transgender             :  2   Median :  8.00  
##  $15000 - 19999:  61   DK           :   0   a gender not listed here:  1   Mean   : 13.91  
##  lt $1000      :  33   IAP          :   0   Don't know              :  0   3rd Qu.: 20.00  
##  (Other)       : 169   NA           :   0   (Other)                 :  0   Max.   :140.00  
##  NA's          :1033   NA's         :   4   NA's                    :947   NA's   :986     
##     emailhr                     socrel                socommun      numpets          tvhours      
##  Min.   :  0.000   sev times a week:382   never           :510   Min.   : 0.000   Min.   : 0.000  
##  1st Qu.:  0.000   sev times a mnth:287   once a month    :243   1st Qu.: 0.000   1st Qu.: 1.000  
##  Median :  2.000   once a month    :259   sev times a week:219   Median : 1.000   Median : 2.000  
##  Mean   :  7.152   sev times a year:240   sev times a year:196   Mean   : 1.718   Mean   : 2.938  
##  3rd Qu.: 10.000   almost daily    :217   sev times a mnth:174   3rd Qu.: 2.000   3rd Qu.: 4.000  
##  Max.   :100.000   (Other)         :171   (Other)         :215   Max.   :20.000   Max.   :24.000  
##  NA's   :929       NA's            :792   NA's            :791   NA's   :1201     NA's   :793     
##                      major1         owngun   
##  business administration: 138   yes    :537  
##  education              :  79   no     :993  
##  engineering            :  54   refused: 39  
##  nursing                :  51   DK     :  0  
##  health                 :  42   IAP    :  0  
##  (Other)                : 546   NA     :  0  
##  NA's                   :1438   NA's   :779
\end{verbatim}

You have a set of questions that you would like to answer with a statistical test. \textbf{For each question}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Choose the most appropriate test.
\item
  List and evaluate the assumptions for your test.
\item
  Conduct your test.
\item
  Discuss statistical and practical significance.
\end{enumerate}

\hypertarget{the-questions}{%
\section{The Questions}\label{the-questions}}

\hypertarget{set-1}{%
\subsection{Set 1}\label{set-1}}

\begin{itemize}
\tightlist
\item
  Do economics majors watch more or less TV than computer science majors?
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(magrittr)}
\FunctionTok{library}\NormalTok{(ggthemes)}

\NormalTok{GSS }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(major1 }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}computer science\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}economics\textquotesingle{}}\NormalTok{)) }\SpecialCharTok{\%$\%} 
  \FunctionTok{t.test}\NormalTok{(tvhours }\SpecialCharTok{\textasciitilde{}}\NormalTok{ major1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Welch Two Sample t-test
## 
## data:  tvhours by major1
## t = 1.7123, df = 11.663, p-value = 0.1133
## alternative hypothesis: true difference in means between group computer science and group economics is not equal to 0
## 95 percent confidence interval:
##  -0.3134265  2.5800931
## sample estimates:
## mean in group computer science        mean in group economics 
##                       2.600000                       1.466667
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{GSS }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(major1 }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}computer science\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}economics\textquotesingle{}}\NormalTok{)) }\SpecialCharTok{\%$\%} 
  \FunctionTok{wilcox.test}\NormalTok{(tvhours }\SpecialCharTok{\textasciitilde{}}\NormalTok{ major1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in wilcox.test.default(x = c(1L, 2L, 2L, 1L, 7L, 5L, 1L, 3L, 2L, : cannot compute exact p-value
## with ties
\end{verbatim}

\begin{verbatim}
## 
##  Wilcoxon rank sum test with continuity correction
## 
## data:  tvhours by major1
## W = 101, p-value = 0.1372
## alternative hypothesis: true location shift is not equal to 0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{GSS }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(major1 }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}computer science\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}economics\textquotesingle{}}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{tv\_category =} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{    tvhours }\SpecialCharTok{\textless{}} \DecValTok{2} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{,}
\NormalTok{    tvhours }\SpecialCharTok{\textgreater{}=} \DecValTok{2} \SpecialCharTok{\&}\NormalTok{ tvhours }\SpecialCharTok{\textless{}} \DecValTok{5} \SpecialCharTok{\textasciitilde{}} \DecValTok{2}\NormalTok{,}
\NormalTok{    tvhours }\SpecialCharTok{\textgreater{}=} \DecValTok{5} \SpecialCharTok{\textasciitilde{}} \DecValTok{3}\NormalTok{)) }\SpecialCharTok{\%$\%}
  \CommentTok{\# t.test(tv\_category \textasciitilde{} major1)}
  \FunctionTok{wilcox.test}\NormalTok{(tv\_category }\SpecialCharTok{\textasciitilde{}}\NormalTok{ major1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in wilcox.test.default(x = c(1, 2, 2, 1, 3, 3, 1, 2, 2, 2), y = c(1, : cannot compute exact p-value
## with ties
\end{verbatim}

\begin{verbatim}
## 
##  Wilcoxon rank sum test with continuity correction
## 
## data:  tv_category by major1
## W = 99.5, p-value = 0.1379
## alternative hypothesis: true location shift is not equal to 0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{GSS }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(major1 }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}computer science\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}economics\textquotesingle{}}\NormalTok{)) }\SpecialCharTok{\%$\%} 
  \FunctionTok{wilcox.test}\NormalTok{(tvhours }\SpecialCharTok{\textasciitilde{}}\NormalTok{ major1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in wilcox.test.default(x = c(1L, 2L, 2L, 1L, 7L, 5L, 1L, 3L, 2L, : cannot compute exact p-value
## with ties
\end{verbatim}

\begin{verbatim}
## 
##  Wilcoxon rank sum test with continuity correction
## 
## data:  tvhours by major1
## W = 101, p-value = 0.1372
## alternative hypothesis: true location shift is not equal to 0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{GSS }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(major1 }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}computer science\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}economics\textquotesingle{}}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{tvhours) }\SpecialCharTok{+} 
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{bins=}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: Removed 11 rows containing non-finite values (stat_bin).
\end{verbatim}

\includegraphics{203-live-session_files/figure-latex/unnamed-chunk-32-1.pdf}

\begin{itemize}
\tightlist
\item
  Do Americans with pets watch more or less TV than Americans without pets?
\end{itemize}

\hypertarget{set-2}{%
\subsection{Set 2}\label{set-2}}

\begin{itemize}
\tightlist
\item
  Do Americans spend more time emailing or using the web?
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{GSS }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(wwwhr, emailhr) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{drop\_na}\NormalTok{() }\SpecialCharTok{\%$\%} 
  \FunctionTok{t.test}\NormalTok{(}\AttributeTok{x=}\NormalTok{wwwhr, }\AttributeTok{y=}\NormalTok{emailhr, }\AttributeTok{paired=}\ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Paired t-test
## 
## data:  wwwhr and emailhr
## t = 13.44, df = 1360, p-value < 2.2e-16
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  5.530219 7.420553
## sample estimates:
## mean of the differences 
##                6.475386
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{GSS }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{wwwhr), }\AttributeTok{fill =} \StringTok{\textquotesingle{}blue\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{emailhr, }\AttributeTok{fill =} \StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
\end{verbatim}

\begin{verbatim}
## Warning: Removed 986 rows containing non-finite values (stat_bin).
\end{verbatim}

\begin{verbatim}
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
\end{verbatim}

\begin{verbatim}
## Warning: Removed 929 rows containing non-finite values (stat_bin).
\end{verbatim}

\includegraphics{203-live-session_files/figure-latex/unnamed-chunk-33-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(}
  \AttributeTok{x =}\NormalTok{ GSS}\SpecialCharTok{$}\NormalTok{wwwhr, }
  \AttributeTok{y =}\NormalTok{ GSS}\SpecialCharTok{$}\NormalTok{emailhr, }
  \AttributeTok{paired =} \ConstantTok{FALSE}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Welch Two Sample t-test
## 
## data:  GSS$wwwhr and GSS$emailhr
## t = 12.073, df = 2398.5, p-value < 2.2e-16
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  5.657397 7.851614
## sample estimates:
## mean of x mean of y 
## 13.906021  7.151515
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Do Americans spend more evenings with neighbors or with relatives?
\end{itemize}

What a great question this is\ldots{} so glad for the chance to answer it, and to deal with this data that someone else has coded for me. Thank you universe.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wilcox\_test\_data }\OtherTok{\textless{}{-}}\NormalTok{ GSS }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(socrel, socommun) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{family\_ordered =} \FunctionTok{factor}\NormalTok{(}
      \AttributeTok{x      =}\NormalTok{ socrel, }
      \AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}almost daily\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}sev times a week\textquotesingle{}}\NormalTok{, }
                 \StringTok{\textquotesingle{}sev times a mnth\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}once a month\textquotesingle{}}\NormalTok{,}
                 \StringTok{\textquotesingle{}sev times a year\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}once a year\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}never\textquotesingle{}}\NormalTok{)),}
    \AttributeTok{friends\_ordered =} \FunctionTok{factor}\NormalTok{(}
      \AttributeTok{x      =}\NormalTok{ socommun, }
      \AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}almost daily\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}sev times a week\textquotesingle{}}\NormalTok{, }
                 \StringTok{\textquotesingle{}sev times a mnth\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}once a month\textquotesingle{}}\NormalTok{,}
                 \StringTok{\textquotesingle{}sev times a year\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}once a year\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}never\textquotesingle{}}\NormalTok{))) }
\end{Highlighting}
\end{Shaded}

To begin this investigation, we've got to look at the data and see what is in it. If you look below, you'll note that it sure seems that people are spending more time with their family\ldots{} erp, actually no. They're ``hanging out'' with their friends rather than taking their mother out to dinner.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wilcox\_test\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(friends\_ordered, family\_ordered) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{rename}\NormalTok{(}
    \AttributeTok{Friends =}\NormalTok{ friends\_ordered, }
    \AttributeTok{Family  =}\NormalTok{ family\_ordered}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{drop\_na}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_longer}\NormalTok{(}\AttributeTok{cols =} \FunctionTok{c}\NormalTok{(Friends, Family)) }\SpecialCharTok{\%\textgreater{}\%}   
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
    \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{value, }\AttributeTok{fill=}\NormalTok{name) }\SpecialCharTok{+} 
    \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{stat=}\StringTok{\textquotesingle{}count\textquotesingle{}}\NormalTok{, }\AttributeTok{position=}\StringTok{\textquotesingle{}dodge\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{scale\_fill\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}\#003262\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}\#FDB515\textquotesingle{}}\NormalTok{)) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{title =} \StringTok{\textquotesingle{}Do Americans Spend Times With Friends or Family?\textquotesingle{}}\NormalTok{,}
    \AttributeTok{subtitle =} \StringTok{\textquotesingle{}A cutting analysis.\textquotesingle{}}\NormalTok{, }
    \AttributeTok{fill =} \StringTok{\textquotesingle{}Friends or Family\textquotesingle{}}\NormalTok{, }
    \AttributeTok{x =} \StringTok{\textquotesingle{}Amount of Time Spent\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{scale\_x\_discrete}\NormalTok{(}\AttributeTok{guide =} \FunctionTok{guide\_axis}\NormalTok{(}\AttributeTok{n.dodge =} \DecValTok{2}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{theme\_fivethirtyeight}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: Ignoring unknown parameters: binwidth, bins, pad
\end{verbatim}

\includegraphics{203-live-session_files/figure-latex/produce descriptive plot-1.pdf}

With this plot created, we can ask if what we observe in the plot is the produce of what could just be sampling error, or if this is something that was unlikely to arise due if the null hypothesis were true. What is the null hypothesis? Well, lets suppose that if we didn't know anything about the data that we would expect there to be no difference between the amount of time spent with friends or families.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# risky choice {-}{-} casting the factor to a numeric without checking what happens.}
\NormalTok{wilcox\_test\_data }\SpecialCharTok{\%$\%} 
  \FunctionTok{wilcox.test}\NormalTok{(}
    \AttributeTok{x=}\FunctionTok{as.numeric}\NormalTok{(family\_ordered), }
    \AttributeTok{y=}\FunctionTok{as.numeric}\NormalTok{(friends\_ordered),}
    \AttributeTok{paired=}\ConstantTok{FALSE}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Wilcoxon rank sum test with continuity correction
## 
## data:  as.numeric(family_ordered) and as.numeric(friends_ordered)
## W = 716676, p-value < 2.2e-16
## alternative hypothesis: true location shift is not equal to 0
\end{verbatim}

\hypertarget{set-3}{%
\subsection{Set 3}\label{set-3}}

\begin{itemize}
\tightlist
\item
  Are Americans that own guns or Americans that don't own guns more likely to have pets?
\item
  Are Americans with pets happier than Americans without pets?
\end{itemize}

\hypertarget{ols-regression-estimates}{%
\chapter{OLS Regression Estimates}\label{ols-regression-estimates}}

\includegraphics{./images/linear_regression.png}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(broom)}
\FunctionTok{library}\NormalTok{(testthat)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{theme\_set}\NormalTok{(}\FunctionTok{theme\_minimal}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\hypertarget{learning-objectives-7}{%
\section{Learning Objectives}\label{learning-objectives-7}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
\item
\item
\end{enumerate}

\hypertarget{class-announcements-6}{%
\section{Class Announcements}\label{class-announcements-6}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Lab 1 is due next week.
\item
  There is no HW 8. We will have HW 9 as usual.
\item
  You're doing great - keep it up!
\end{enumerate}

\hypertarget{roadmap-4}{%
\section{Roadmap}\label{roadmap-4}}

\textbf{Rear-View Mirror}

\begin{itemize}
\tightlist
\item
  Statisticians create a population model to represent the world.
\item
  Sometimes, the model includes an ``outcome'' random variable \(Y\) and ``input'' random variables \(X_1, X_2,...,X_k\).
\item
  The joint distribution of \(Y\) and \(X_1, X_2,...,X_k\) is complicated.
\item
  The best linear predictor (BLP) is the canonical way to summarize the relationship.
\end{itemize}

\textbf{Today}

\begin{itemize}
\tightlist
\item
  OLS regression is an estimator for the BLP
\item
  We'll discuss the \emph{mechanics} of OLS
\end{itemize}

\textbf{Looking Ahead}

\begin{itemize}
\tightlist
\item
  To make regression estimates useful, we need measures of uncertainty (standard errors, tests\ldots).
\item
  The process of building a regression model looks different, depending on whether the goal is prediction, description, or explanation.
\end{itemize}

\hypertarget{regression-discussion}{%
\section{Regression Discussion}\label{regression-discussion}}

\hypertarget{working-on-math}{%
\section{Working on Math}\label{working-on-math}}

Meet us over at this \href{https://miro.com/welcomeonboard/OWZ3SHlpeVBMYjdnc2xXUFhzQVNWaFpnTkg2dmF1Z3I1UUJBWVZobWFOeVduSHJNTTRSQUt3akVyQUt0MnAyM3wzNDU4NzY0NTE4MDA4MjA1NTc3?invite_link_id=319921528710}{link} to work on some math together.

\hypertarget{visualizing-data}{%
\section{Visualizing Data}\label{visualizing-data}}

Consider that you have two variables, \(X_{1}\) and \(X_{2}\) that are predicting \(Y\).

\hypertarget{discussion-questions-1}{%
\subsection{Discussion Questions}\label{discussion-questions-1}}

Suppose we have random variables \(X\) and \(Y\).

\begin{itemize}
\tightlist
\item
  Why do we care about the BLP?
\item
  What assumptions are needed for OLS to consistently estimate the BLP?
\item
  What assumptions are needed in terms of causality (\(X\) causes \(Y\), \(Y\) causes \(X\), etc.) in order to compute the regression of \(Y\) on \(X\)?
\end{itemize}

\hypertarget{the-regression-anatomy-formula}{%
\section{The Regression Anatomy Formula}\label{the-regression-anatomy-formula}}

We make the claim in live session that we can re-represent a coefficient that we're interested in as a function of all the other variable in a regression. That is, suppose that we were interested, initially, in estimating the model:

\[ 
Y = \hat\beta_{0} + \hat\beta_{1} X_{1} + \hat\beta_{2} X_{2} + \hat\beta_{3}X_{3} + e
\]
that we can produce an estimate for \(\hat\beta_{1}\) by fitting this auxiliary regression,

\[
X_{1} = \hat\delta_{0} + \hat\delta_2X_2 + \hat\delta_3X_3 + r_{1}
\]

And then using the residuals, noted as \(r_1\) above, in a second auxiliary regression,

\[ 
Y = \gamma_0 + \gamma_1 r_1
\]

The claim that we make in the live session is that there is a guarantee that \(\beta_1 = \gamma_1\). Here, we are first going to show that this is true, and then we're going to reason about what this means, and why this feature is interesting (or at least useful) when we are estimating a regression.

Suppose that the population model is the following:

\[
Y = -3 + (1\cdot X_1) + (2\cdot X_2) + (3\cdot X_3)
\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{x1 =} \FunctionTok{runif}\NormalTok{(}\AttributeTok{n=}\DecValTok{100}\NormalTok{, }\AttributeTok{min=}\DecValTok{0}\NormalTok{, }\AttributeTok{max=}\DecValTok{10}\NormalTok{), }
  \AttributeTok{x2 =} \FunctionTok{runif}\NormalTok{(}\AttributeTok{n=}\DecValTok{100}\NormalTok{, }\AttributeTok{min=}\DecValTok{0}\NormalTok{, }\AttributeTok{max=}\DecValTok{10}\NormalTok{), }
  \AttributeTok{x3 =} \FunctionTok{runif}\NormalTok{(}\AttributeTok{n=}\DecValTok{100}\NormalTok{, }\AttributeTok{min=}\DecValTok{0}\NormalTok{, }\AttributeTok{max=}\DecValTok{10}\NormalTok{)}
\NormalTok{)}

\DocumentationTok{\#\# because we know the population model, we can produce a single sample from it }
\DocumentationTok{\#\# using the following code: }

\NormalTok{d }\OtherTok{\textless{}{-}}\NormalTok{ d }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{y =} \SpecialCharTok{{-}}\DecValTok{3} \SpecialCharTok{+} \DecValTok{1}\SpecialCharTok{*}\NormalTok{x1 }\SpecialCharTok{+} \DecValTok{2}\SpecialCharTok{*}\NormalTok{x2 }\SpecialCharTok{+} \DecValTok{3}\SpecialCharTok{*}\NormalTok{x3 }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n=}\FunctionTok{n}\NormalTok{(), }\AttributeTok{mean=}\DecValTok{0}\NormalTok{, }\AttributeTok{sd=}\DecValTok{1}\NormalTok{))}

\FunctionTok{head}\NormalTok{(d)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          x1        x2       x3         y
## 1 0.1759703 9.0569401 1.139907 19.323966
## 2 7.7666990 0.1850593 5.278850 19.770301
## 3 7.5730149 1.9214382 9.415096 36.320725
## 4 0.1054815 3.8430702 1.489464  8.088767
## 5 8.1791408 3.0743642 5.891215 28.172100
## 6 8.5634588 5.2829125 8.782764 42.706789
\end{verbatim}

Notice that when we made this data, we included a set of random noise at the end. The idea here is that there are other ``things'' in this universe that also affect \(Y\), but that we don't have access to them. By assumption, what we \emph{have} measured in this world, \(X_1, X_2, X_3\) are uncorrelated with these other features.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model\_main }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x1 }\SpecialCharTok{+}\NormalTok{ x2 }\SpecialCharTok{+}\NormalTok{ x3, }\AttributeTok{data =}\NormalTok{ d)}
\FunctionTok{coef}\NormalTok{(model\_main)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## (Intercept)          x1          x2          x3 
##   -3.486344    1.000128    2.055536    3.037375
\end{verbatim}

The claim is that we can produce an estimate of \(\beta_1\) using an auxiliary set of regression estimates, and then using the regression from that auxiliary regression.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model\_aux }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(x1 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x2 }\SpecialCharTok{+}\NormalTok{ x3, }\AttributeTok{data =}\NormalTok{ d)}
\end{Highlighting}
\end{Shaded}

If we look into the structure of \texttt{model\_aux} we can see that there are \emph{a ton} of pieces in here.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{coef}\NormalTok{(model\_aux)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## (Intercept)          x2          x3 
##  5.21986247 -0.06256717  0.08997822
\end{verbatim}

To evaluate our claim, we need to find the residuals from this regression. As a knowledge check, what is it that we mean when we say, ``residual'' in this sense?

To make talking about these easier, here is a plot that might be useful.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{x1, }\AttributeTok{y=}\NormalTok{y) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_segment}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\DecValTok{0}\NormalTok{, }\AttributeTok{xend=}\DecValTok{10}\NormalTok{, }\AttributeTok{y=}\DecValTok{0}\NormalTok{, }\AttributeTok{yend=}\DecValTok{50}\NormalTok{), }\AttributeTok{color =} \StringTok{\textquotesingle{}steelblue\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{203-live-session_files/figure-latex/plot x1 and y and line-1.pdf}

In order to access these residuals, we can ``augment'' the dataframe that we used in the model, using the \texttt{broom::augment} function call.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d\_augmented }\OtherTok{\textless{}{-}} \FunctionTok{augment}\NormalTok{(model\_aux)}
\NormalTok{d\_augmented}\SpecialCharTok{$}\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ d}\SpecialCharTok{$}\NormalTok{y}
\NormalTok{d\_augmented}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 100 x 10
##       x1    x2    x3 .fitted .resid   .hat .sigma  .cooksd .std.resid     y
##    <dbl> <dbl> <dbl>   <dbl>  <dbl>  <dbl>  <dbl>    <dbl>      <dbl> <dbl>
##  1 0.176 9.06  1.14     4.76 -4.58  0.0399   2.81 0.0375       -1.65  19.3 
##  2 7.77  0.185 5.28     5.68  2.08  0.0454   2.85 0.00894       0.751 19.8 
##  3 7.57  1.92  9.42     5.95  1.63  0.0590   2.85 0.00729       0.590 36.3 
##  4 0.105 3.84  1.49     5.11 -5.01  0.0237   2.81 0.0257       -1.78   8.09
##  5 8.18  3.07  5.89     5.56  2.62  0.0202   2.84 0.00596       0.932 28.2 
##  6 8.56  5.28  8.78     5.68  2.88  0.0350   2.84 0.0129        1.03  42.7 
##  7 4.22  7.28  1.78     4.92 -0.701 0.0230   2.85 0.000489     -0.250 20.3 
##  8 6.34  9.54  4.71     5.05  1.30  0.0302   2.85 0.00223       0.463 35.5 
##  9 4.18  4.96  1.41     5.04 -0.856 0.0215   2.85 0.000681     -0.305 14.0 
## 10 3.25  1.32  0.994    5.23 -1.98  0.0449   2.85 0.00797      -0.713  4.43
## # ... with 90 more rows
\end{verbatim}

And finally, with this augmented data that has information from the model, we can estimate the model that includes only the residuals as predictors of \(Y\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model\_two }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ .resid, }\AttributeTok{data =}\NormalTok{ d\_augmented)}
\FunctionTok{coef}\NormalTok{(model\_two)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## (Intercept)      .resid 
##   26.339388    1.000128
\end{verbatim}

Our claim was that the coefficients from \texttt{model\_main} and \texttt{model\_two} should be the same.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{test\_that}\NormalTok{(}
  \StringTok{\textquotesingle{}the model coefficients are equal\textquotesingle{}}\NormalTok{, }
  \FunctionTok{expect\_equal}\NormalTok{(}
    \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{coef}\NormalTok{(model\_main)[}\StringTok{\textquotesingle{}x1\textquotesingle{}}\NormalTok{]), }
    \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{coef}\NormalTok{(model\_two)[}\StringTok{\textquotesingle{}.resid\textquotesingle{}}\NormalTok{]))}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Test passed
\end{verbatim}

But, why is this an interesting, or at least useful, feature to appreciate?

\hypertarget{coding-activityr-cheat-sheet}{%
\section{Coding Activity:R Cheat Sheet}\label{coding-activityr-cheat-sheet}}

Suppose \texttt{x} and \texttt{y} are variables in dataframe \texttt{d}.

To fit an ols regression of Y on X:

\begin{verbatim}
mod <- lm(y ~ x, data = d)
\end{verbatim}

To access \textbf{coefficients} from the model object:

\begin{verbatim}
mod$coefficients
or coef(mod)
\end{verbatim}

To access \textbf{fitted values} from the model object:

\begin{verbatim}
mod$fitted
or fitted(mod)
or predict(mod)
\end{verbatim}

To access \textbf{residuals} from the model object:

\begin{verbatim}
mod$residuals
or resid(mod)
\end{verbatim}

To create a scatterplot that includes the regression line:

\begin{verbatim}
plot(d['x'], d['y'])
abline(mod)
or 
d %>% 
  ggplot() + 
  aes(x = x, y = y) + 
  geom_point() + 
  geom_smooth(method = lm)
\end{verbatim}

\hypertarget{r-exercise}{%
\section{R Exercise}\label{r-exercise}}

\textbf{Real Estate in Boston}

The file \texttt{hprice1.Rdata} contains 88 observations of homes in the Boston area, taken from the real estate pages of the Boston Globe during 1990. This data was provided by Wooldridge.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{load}\NormalTok{(}\StringTok{\textquotesingle{}data/hprice1.RData\textquotesingle{}}\NormalTok{) }\CommentTok{\# provides 3 objects }
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     price assess bdrms lotsize sqrft colonial   lprice  lassess llotsize   lsqrft
## 1 300.000  349.1     4    6126  2438        1 5.703783 5.855359 8.720297 7.798934
## 2 370.000  351.5     3    9903  2076        1 5.913503 5.862210 9.200593 7.638198
## 3 191.000  217.7     3    5200  1374        0 5.252274 5.383118 8.556414 7.225482
## 4 195.000  231.8     3    4600  1448        1 5.273000 5.445875 8.433811 7.277938
## 5 373.000  319.1     4    6095  2514        1 5.921578 5.765504 8.715224 7.829630
## 6 466.275  414.5     5    8566  2754        1 6.144775 6.027073 9.055556 7.920810
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Are there variables that would \emph{not} be valid outcomes for an OLS regression? If so, why?
\item
  Are there variables that would \emph{not} be valid inputs for an OLS regression? If so, why?
\end{itemize}

\hypertarget{assess-the-relationship-between-price-and-square-footage}{%
\subsection{Assess the Relationship between Price and Square Footage}\label{assess-the-relationship-between-price-and-square-footage}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{sqrft, }\AttributeTok{y=}\NormalTok{price) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{203-live-session_files/figure-latex/unnamed-chunk-41-1.pdf}

Suppose that you're interested in knowing the relationship between price and square footage.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{-1}
\item
  Assess the assumptions of the Large-Sample Linear Model.
\item
  Create a scatterplot of \texttt{price} and \texttt{sqrft}. Like every plot you make, ensure that the plot \emph{minimally} has a title and meaningful axes.
\item
  Find the correlation between the two variables.
\item
  Recall the equation for the slope of the OLS regression line -- here you can either use Variance and Covariance, or if you're bold, the linear algebra. Compute the slope manually (without using \texttt{lm()})
\item
  Regress \texttt{price} on \texttt{sqrft} using the \texttt{lm} function. This will produce an estimate for the following model:
\end{enumerate}

\[ 
price = \beta_{0} + \beta_{1} sqrft + e
\]

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# old estimate was 0.14}
\NormalTok{model\_price }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ sqrft }\SpecialCharTok{+}\NormalTok{ lotsize, }\AttributeTok{data =}\NormalTok{ data)}
\FunctionTok{coef}\NormalTok{(model\_price)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## (Intercept)       sqrft     lotsize 
## 5.932414240 0.133362017 0.002113495
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{sqrft, }\AttributeTok{y=}\NormalTok{lotsize) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{203-live-session_files/figure-latex/unnamed-chunk-45-1.pdf}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\item
  Create a scatterplot that includes the fitted regression.
\item
  Interpret what the coefficient means.
\end{enumerate}

\begin{itemize}
\tightlist
\item
  State what features you are allowing to change and what features you're requiring do not change.
\item
  For each additional square foot, how much more (or less) is the house worth?
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{6}
\tightlist
\item
  Estimate a new model (and save it into another object) that includes the size of the lot and whether the house is a colonial. This will estimate the model:
\end{enumerate}

\[ 
price = \beta_{0} + \beta_{1} sqrft + \beta_{2} lotsize + \beta_{3} colonial? + e
\]

\begin{itemize}
\tightlist
\item
  \emph{BUT BEFORE YOU DO}, make a prediction: What do you think is going to happen to the coefficient that relates square footage and price?

  \begin{itemize}
  \tightlist
  \item
    Will the coefficient increase, decrease, or stay the same?
  \end{itemize}
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{6}
\tightlist
\item
  Compute the sample correlation between \(X\) and \(e_i\). What guarantees do we have from the book about this correlation? Does the data seem to bear this out?
\end{enumerate}

\hypertarget{regression-plots-and-discussion}{%
\section{Regression Plots and Discussion}\label{regression-plots-and-discussion}}

In this next set of notes, we're going to give some data, displayed in plots, and we will try to apply what we have learned in the async and reading for this week to answer questions about each of the scatter plots.

\hypertarget{plot-1}{%
\subsection{Plot 1}\label{plot-1}}

Consider data that is generated according to the following function:

\[
  Y = 1 + 2x_1 + 3x_2 + e, 
\]

where \(x_1 \sim N(0,2)\), \(x_2 \sim N(0,2)\) and \(e\) is a constant equal to zero.

From this population, you might consider taking a sample of 100 observations, and representing this data in the following 3d scatter plot. In this plot, there are three dimensions, an \(x_1, x_2\), and \(y\) dimensions.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{include\_app}\NormalTok{(}\AttributeTok{url =}\StringTok{"http://www.statistics.wtf/minibeta\_l01/"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Rotate the cube and explore the data, looking at each face of the cube, including from the top down.
\item
  One of the lessons that we learned during the random variables section of the course is that every random variable that has been measured can also be marginalized off. You might think of this as ``casting down'' data from three dimensions, to only two.
\item
  Sketch the following 2d scatter plots, taking care the label your axes. You need not represent all 100 points, but rather create the \emph{gestalt} of what you see.\\
  1. \(Y = f(x_1)\) (but not \(x_2\))
  2. \(Y = f(x_2)\) (but not \(x_1\))
  3. \(x2 = f(x_1)\)
\item
  Once you have sketched the scatter plots, what line would you fit that minimizes the sum of squared residuals in the vertical direction. Define a residual, \(\epsilon\), to be the vertical distance between the line you draw, and the corresponding point on the input data.
\item
  What is the \emph{average} of the residuals for each of the lines that you have fitted? How does this correspond to the \emph{moment conditions} discussed in the async? What would happen if you translated this line vertically?
\item
  Rotate the cube so that the points ``fall into line''. When you see this line, how does it help you describe the function that governs this data?
\end{enumerate}

\hypertarget{ols-regression-inference}{%
\chapter{OLS Regression Inference}\label{ols-regression-inference}}

\begin{figure}
\centering
\includegraphics{./images/bridge_sunset.jpeg}
\caption{sunset on golden gate}
\end{figure}

\hypertarget{learning-objectives-8}{%
\section{Learning Objectives}\label{learning-objectives-8}}

After this week's learning, student will be able to

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{Describe} how sampling based uncertainty is reflected in OLS regression parameter estimates.
\item
  \emph{Report} standard errors, and \emph{conduct} tests for NHST of regression coefficients against zero.
\item
  \emph{Conduct} a regression based analysis, on real data, in ways that begin to explore regression as a modeling tool.
\end{enumerate}

\hypertarget{class-announcements-7}{%
\section{Class Announcements}\label{class-announcements-7}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Congratulations on finishing your first lab!
\item
  The next (and the last) lab is coming up in two weeks.
\item
  Homework 09 has been assigned today, and it's due in a week.
\end{enumerate}

\hypertarget{roadmap-5}{%
\section{Roadmap}\label{roadmap-5}}

\textbf{Rear-View Mirror}

\begin{itemize}
\tightlist
\item
  Statisticians create a population model to represent the world.
\item
  Sometimes, the model includes an ``outcome'' random variable \(Y\) and ``input'' random variables \(X_1, X_2,...,X_k\).
\item
  The joint distribution of \(Y\) and \(X_1, X_2,...,X_k\) is complicated.
\item
  The best linear predictor (BLP) is the canonical way to summarize the relationship.
\item
  OLS provides a point estimate of the BLP
\end{itemize}

\textbf{Today}

\begin{itemize}
\tightlist
\item
  Robust Standard Error: quantify the uncertainty of OLS coefficients
\item
  Hypothesis testing with OLS coefficients
\item
  Bootstrapping
\end{itemize}

\textbf{Looking Ahead}

\begin{itemize}
\tightlist
\item
  Regression is a foundational tool that can be applied to different contexts
\item
  The process of building a regression model looks different, depending on whether the goal is prediction, description, or explanation.
\end{itemize}

\hypertarget{uncertainty-in-ols}{%
\section{Uncertainty in OLS}\label{uncertainty-in-ols}}

\hypertarget{discussion-questions-2}{%
\subsection{Discussion Questions}\label{discussion-questions-2}}

\begin{itemize}
\tightlist
\item
  List as many differences between the BLP and the OLS line as you can.
\item
  In the following regression table, explain in your own words what the standard error in parentheses means.
\end{itemize}

\begin{longtable}[]{@{}ll@{}}
\toprule
& outcome: sleep hours \\
\midrule
\endhead
mg. melatonin & 0.52 \\
& (0.31) \\
\bottomrule
\end{longtable}

\hypertarget{understanding-uncertainty}{%
\section{Understanding Uncertainty}\label{understanding-uncertainty}}

Imagine three different regression models, each of the following form:

\[
  Y = 0 + 0 \cdot X + \epsilon
\]

The only difference is in the error term. The conditional distribution is given by:

\begin{longtable}[]{@{}ll@{}}
\toprule
Model & Distribution of \(\epsilon\) cond. on \(X\) \\
\midrule
\endhead
A & Uniform on \([-.5, +.5]\) \\
B & Uniform on \([ - |X|, |X| ]\) \\
C & Uniform on \([ -1 + |X|, 1- |X| ]\) \\
\bottomrule
\end{longtable}

A is what we call a homoskedastic distribution. B and C are what we call heteroskedastic. Below, we define R functions that simulate draws from these three distributions.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rA }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(n, }\AttributeTok{slope=}\DecValTok{0}\NormalTok{)\{}
\NormalTok{  x       }\OtherTok{=} \FunctionTok{runif}\NormalTok{(n, }\AttributeTok{min=}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\AttributeTok{max =} \DecValTok{1}\NormalTok{)}
\NormalTok{  epsilon }\OtherTok{=} \FunctionTok{runif}\NormalTok{(n, }\AttributeTok{min=}\SpecialCharTok{{-}}\NormalTok{.}\DecValTok{5}\NormalTok{, }\AttributeTok{max=}\NormalTok{.}\DecValTok{5}\NormalTok{)}
\NormalTok{  y       }\OtherTok{=} \DecValTok{0} \SpecialCharTok{+}\NormalTok{ slope}\SpecialCharTok{*}\NormalTok{x }\SpecialCharTok{+}\NormalTok{ epsilon}
  \FunctionTok{return}\NormalTok{( }\FunctionTok{data.frame}\NormalTok{(}\AttributeTok{x=}\NormalTok{x,}\AttributeTok{y=}\NormalTok{y) )}
\NormalTok{\}}

\NormalTok{rB }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(n, }\AttributeTok{slope=}\DecValTok{0}\NormalTok{)\{}
\NormalTok{  x       }\OtherTok{=} \FunctionTok{runif}\NormalTok{(n, }\AttributeTok{min=}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\AttributeTok{max =} \DecValTok{1}\NormalTok{)}
\NormalTok{  epsilon }\OtherTok{=} \FunctionTok{runif}\NormalTok{(n, }\AttributeTok{min=}\SpecialCharTok{{-}} \FunctionTok{abs}\NormalTok{(x), }\AttributeTok{max=}\FunctionTok{abs}\NormalTok{(x))}
\NormalTok{  y       }\OtherTok{=} \DecValTok{0} \SpecialCharTok{+}\NormalTok{ slope}\SpecialCharTok{*}\NormalTok{x }\SpecialCharTok{+}\NormalTok{ epsilon}
  \FunctionTok{return}\NormalTok{( }\FunctionTok{data.frame}\NormalTok{(}\AttributeTok{x=}\NormalTok{x,}\AttributeTok{y=}\NormalTok{y) )}
\NormalTok{\}}

\NormalTok{rC }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(n, }\AttributeTok{slope=}\DecValTok{0}\NormalTok{)\{}
\NormalTok{  x       }\OtherTok{=} \FunctionTok{runif}\NormalTok{(n, }\AttributeTok{min=}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\AttributeTok{max =} \DecValTok{1}\NormalTok{)}
\NormalTok{  epsilon }\OtherTok{=} \FunctionTok{runif}\NormalTok{(n, }\AttributeTok{min=} \SpecialCharTok{{-}}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{abs}\NormalTok{(x), }\AttributeTok{max=}\DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{abs}\NormalTok{(x))}
\NormalTok{  y       }\OtherTok{=} \DecValTok{0} \SpecialCharTok{+}\NormalTok{ slope}\SpecialCharTok{*}\NormalTok{x }\SpecialCharTok{+}\NormalTok{ epsilon}
  \FunctionTok{return}\NormalTok{( }\FunctionTok{data.frame}\NormalTok{(}\AttributeTok{x=}\NormalTok{x,}\AttributeTok{y=}\NormalTok{y) )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{rbind}\NormalTok{( }
  \FunctionTok{data.frame}\NormalTok{( }\FunctionTok{rA}\NormalTok{(}\DecValTok{200}\NormalTok{), }\AttributeTok{label =} \StringTok{\textquotesingle{}A\textquotesingle{}}\NormalTok{),}
  \FunctionTok{data.frame}\NormalTok{( }\FunctionTok{rB}\NormalTok{(}\DecValTok{200}\NormalTok{), }\AttributeTok{label =} \StringTok{\textquotesingle{}B\textquotesingle{}}\NormalTok{),}
  \FunctionTok{data.frame}\NormalTok{( }\FunctionTok{rC}\NormalTok{(}\DecValTok{200}\NormalTok{), }\AttributeTok{label =} \StringTok{\textquotesingle{}C\textquotesingle{}}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{x, }\AttributeTok{y=}\NormalTok{y)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{lims}\NormalTok{(}
    \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{), }
    \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{\textquotesingle{}Samples Drawn from Three Distributions\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{facet\_grid}\NormalTok{(}\AttributeTok{rows=}\FunctionTok{vars}\NormalTok{(label))}
\end{Highlighting}
\end{Shaded}

\includegraphics{203-live-session_files/figure-latex/unnamed-chunk-51-1.pdf}

\hypertarget{question-1}{%
\subsection{Question 1}\label{question-1}}

The following code draws a sample from distribution A, fits a regression line, and plots it. Run it a few times to see what happens. Now explain how you would visually estimate the standard error of the slope coefficient. Why is this standard error important?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OtherTok{\textless{}{-}}  \FunctionTok{rA}\NormalTok{(}\DecValTok{10}\NormalTok{, }\AttributeTok{slope=}\DecValTok{0}\NormalTok{)}

\NormalTok{data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{x, }\AttributeTok{y=}\NormalTok{y) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method=}\StringTok{\textquotesingle{}lm\textquotesingle{}}\NormalTok{, }\AttributeTok{formula =} \StringTok{\textquotesingle{}y \textasciitilde{} x\textquotesingle{}}\NormalTok{, }\AttributeTok{se=}\ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{lims}\NormalTok{(}
    \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{), }
    \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{\textquotesingle{}Regression Fit to Distribution A\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{203-live-session_files/figure-latex/unnamed-chunk-52-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{base\_plot\_a }\OtherTok{\textless{}{-}} \FunctionTok{rA}\NormalTok{(}\DecValTok{10}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}  
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{x, }\AttributeTok{y=}\NormalTok{y) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{stat\_smooth}\NormalTok{(}\AttributeTok{method=}\StringTok{\textquotesingle{}lm\textquotesingle{}}\NormalTok{, }\AttributeTok{se=}\ConstantTok{FALSE}\NormalTok{, }\AttributeTok{formula =} \StringTok{\textquotesingle{}y \textasciitilde{}x\textquotesingle{}}\NormalTok{)}

\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{100}\NormalTok{) \{ }
\NormalTok{    base\_plot\_a }\OtherTok{\textless{}{-}}\NormalTok{ base\_plot\_a }\SpecialCharTok{+} \FunctionTok{rA}\NormalTok{(}\DecValTok{100}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
      \FunctionTok{stat\_smooth}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{x, }\AttributeTok{y=}\NormalTok{y), }\AttributeTok{method=}\StringTok{\textquotesingle{}lm\textquotesingle{}}\NormalTok{, }\AttributeTok{se=}\ConstantTok{FALSE}\NormalTok{, }\AttributeTok{formula=}\StringTok{\textquotesingle{}y\textasciitilde{}x\textquotesingle{}}\NormalTok{, }\AttributeTok{color =} \StringTok{\textquotesingle{}grey\textquotesingle{}}\NormalTok{, }\AttributeTok{size =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.5}\NormalTok{)}
\NormalTok{\}}

\NormalTok{base\_plot\_b }\OtherTok{\textless{}{-}} \FunctionTok{rB}\NormalTok{(}\DecValTok{10}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}  
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{x, }\AttributeTok{y=}\NormalTok{y) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{stat\_smooth}\NormalTok{(}\AttributeTok{method=}\StringTok{\textquotesingle{}lm\textquotesingle{}}\NormalTok{, }\AttributeTok{se=}\ConstantTok{FALSE}\NormalTok{, }\AttributeTok{formula =} \StringTok{\textquotesingle{}y \textasciitilde{}x\textquotesingle{}}\NormalTok{)}

\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{100}\NormalTok{) \{ }
\NormalTok{    base\_plot\_b }\OtherTok{\textless{}{-}}\NormalTok{ base\_plot\_b }\SpecialCharTok{+} \FunctionTok{rB}\NormalTok{(}\DecValTok{100}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
      \FunctionTok{stat\_smooth}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{x, }\AttributeTok{y=}\NormalTok{y), }\AttributeTok{method=}\StringTok{\textquotesingle{}lm\textquotesingle{}}\NormalTok{, }\AttributeTok{se=}\ConstantTok{FALSE}\NormalTok{, }\AttributeTok{formula=}\StringTok{\textquotesingle{}y\textasciitilde{}x\textquotesingle{}}\NormalTok{, }\AttributeTok{color =} \StringTok{\textquotesingle{}grey\textquotesingle{}}\NormalTok{, }\AttributeTok{size =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.5}\NormalTok{)}
\NormalTok{  \}}

\NormalTok{base\_plot\_c }\OtherTok{\textless{}{-}} \FunctionTok{rC}\NormalTok{(}\DecValTok{10}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}  
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{x, }\AttributeTok{y=}\NormalTok{y) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{stat\_smooth}\NormalTok{(}\AttributeTok{method=}\StringTok{\textquotesingle{}lm\textquotesingle{}}\NormalTok{, }\AttributeTok{se=}\ConstantTok{FALSE}\NormalTok{, }\AttributeTok{formula =} \StringTok{\textquotesingle{}y \textasciitilde{}x\textquotesingle{}}\NormalTok{)}

\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{100}\NormalTok{) \{ }
\NormalTok{    base\_plot\_c }\OtherTok{\textless{}{-}}\NormalTok{ base\_plot\_c }\SpecialCharTok{+} \FunctionTok{rC}\NormalTok{(}\DecValTok{100}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
      \FunctionTok{stat\_smooth}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{x, }\AttributeTok{y=}\NormalTok{y), }\AttributeTok{method=}\StringTok{\textquotesingle{}lm\textquotesingle{}}\NormalTok{, }\AttributeTok{se=}\ConstantTok{FALSE}\NormalTok{, }\AttributeTok{formula=}\StringTok{\textquotesingle{}y\textasciitilde{}x\textquotesingle{}}\NormalTok{, }\AttributeTok{color =} \StringTok{\textquotesingle{}grey\textquotesingle{}}\NormalTok{, }\AttributeTok{size =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.5}\NormalTok{)}
\NormalTok{\}}



\NormalTok{base\_plot\_a }\SpecialCharTok{|}\NormalTok{ base\_plot\_b  }\SpecialCharTok{|}\NormalTok{ base\_plot\_c}
\end{Highlighting}
\end{Shaded}

\includegraphics{203-live-session_files/figure-latex/unnamed-chunk-53-1.pdf}

\hypertarget{question-2}{%
\subsection{Question 2}\label{question-2}}

You have a sample from each distribution, A, B, and C and you fit a regression of Y on X. Which will have the highest standard error for the slope coefficient? Which will have the lowest standard error? Why? (You may want to try experimenting with the function defined above)

\hypertarget{question-3}{%
\subsection{Question 3}\label{question-3}}

For distribution A, perform a simulated experiment. Draw a large number of samples, and for each sample fit a linear regression. Store the slope coefficient from each regression in a vector. Finally, compute the standard deviation for the slope coefficients.

Repeat this process for distributions B and C. Do the results match your intuition?

\hypertarget{understanding-uncertainty-1}{%
\section{Understanding Uncertainty}\label{understanding-uncertainty-1}}

Under the relatively stricter assumptions of constant error variance, the variance of a slope coefficient is given by

\[
  V(\hat{\beta_j}) = \frac{\sigma^2}{SST_j (1-R_j^2)}
\]

A similar formulation is given in \emph{FOAS} as definition 4.2.3,

\[
  \hat{V}_{C}[\hat{\beta}] = \hat{\sigma}^2 \left( X^{T} X \right)^{-1} \rightsquigarrow \frac{\hat{\sigma}^{2}}{\left( X^{T}X\right)}
\]

Explain why each term makes the variance higher or lower:

\begin{itemize}
\tightlist
\item
  \(\sigma^2\) is the variance of the error \(\epsilon\)
\item
  \(SST_j\) is (unscaled) variance of \(X_j\)
\item
  \(R_j^2\) is \(R^2\) for a regression of \(X_j\) on the other \(X\)'s
\end{itemize}

\hypertarget{r-exercise-1}{%
\section{R Exercise}\label{r-exercise-1}}

\textbf{Real Estate in Boston}

The file \texttt{hprice1.RData} contains 88 observations of homes in the Boston area, taken from the real estate pages of the Boston Globe during 1990. This data was provided by Wooldridge.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{load}\NormalTok{(}\StringTok{\textquotesingle{}data/hprice1.RData\textquotesingle{}}\NormalTok{) }\CommentTok{\# provides 3 objects }
\end{Highlighting}
\end{Shaded}

Last week, we fit a regression of price on square feet.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model\_one }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ sqrft, }\AttributeTok{data =}\NormalTok{ data)}
\NormalTok{model\_one}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = price ~ sqrft, data = data)
## 
## Coefficients:
## (Intercept)        sqrft  
##     11.2041       0.1402
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# any frequentist test has what components: numerator and denominator? }

\NormalTok{numerator   }\OtherTok{\textless{}{-}} \FloatTok{0.14}
\NormalTok{denominator }\OtherTok{\textless{}{-}} \FunctionTok{diag}\NormalTok{(}\FunctionTok{sqrt}\NormalTok{(}\FunctionTok{vcovHC}\NormalTok{(model\_one)))[}\StringTok{\textquotesingle{}sqrft\textquotesingle{}}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in sqrt(vcovHC(model_one)): NaNs produced
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test\_stat }\OtherTok{\textless{}{-}}\NormalTok{ numerator }\SpecialCharTok{/}\NormalTok{ denominator}

\NormalTok{test\_stat}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    sqrft 
## 6.631694
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pt}\NormalTok{(test\_stat, }\AttributeTok{df =} \DecValTok{87}\NormalTok{, }\AttributeTok{lower.tail =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{*} \DecValTok{2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      sqrft 
## 2.6884e-09
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# we could do it by hand... or... let someone else do it for us}

\FunctionTok{coeftest}\NormalTok{(model\_one, }\AttributeTok{vcov. =} \FunctionTok{vcovHC}\NormalTok{(model\_one))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## t test of coefficients:
## 
##              Estimate Std. Error t value  Pr(>|t|)    
## (Intercept) 11.204145  39.450563  0.2840    0.7771    
## sqrft        0.140211   0.021111  6.6417 2.673e-09 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{var}\NormalTok{(data[}\StringTok{\textquotesingle{}sqrft\textquotesingle{}}\NormalTok{]) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          sqrft
## sqrft 333150.1
\end{verbatim}

\textbf{Questions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Estimate a new model (and save it into another object) that includes the size of the lot and whether the house is a colonial. This will estimate the model:
\end{enumerate}

\[
  price = \beta_{0} + \beta_{1} sqrft + \beta_{2} lotsize + \beta_{3} colonial? + e
\]

\begin{itemize}
\tightlist
\item
  \emph{BUT BEFORE YOU DO}, make a prediction: What do you think is going to happen to the coefficient that relates square footage and price?

  \begin{itemize}
  \tightlist
  \item
    Will the coefficient increase, decrease, or stay the same?
  \item
    Will the \emph{uncertainty} about the coefficient increase, decrease, or stay the same?
  \item
    Conduct an F-test that evaluates whether the model \emph{as a whole} does better when the coefficients on \texttt{colonial} and \texttt{lotsize} are allowed to estimate freely, or instead are restricted to be zero (i.e.~\(\beta_{2} = \beta_{3} = 0\).
  \end{itemize}
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Use the function \texttt{vcovHC} from the \texttt{sandwich} package to estimate (a) the the heteroskedastic consistent (i.e.~``robust'') variance covariance matrix; and (b) the robust standard errors for the intercept and slope of this regression. Recall, what is the relationship between the VCOV and SE in a regression?
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data}\SpecialCharTok{$}\NormalTok{sqrft\_two }\OtherTok{\textless{}{-}}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{sqrft }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n=}\DecValTok{88}\NormalTok{, }\AttributeTok{mean=}\DecValTok{0}\NormalTok{, }\AttributeTok{sd=}\FloatTok{0.1}\NormalTok{)}

\NormalTok{data[ , }\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}sqrft\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}sqrft\_two\textquotesingle{}}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    sqrft sqrft_two
## 1   2438  2437.873
## 2   2076  2075.980
## 3   1374  1374.181
## 4   1448  1448.064
## 5   2514  2513.984
## 6   2754  2754.082
## 7   2067  2067.199
## 8   1731  1731.076
## 9   1767  1767.097
## 10  1890  1889.953
## 11  2336  2335.950
## 12  2634  2634.155
## 13  3375  3374.900
## 14  1899  1899.174
## 15  2312  2311.739
## 16  1760  1759.907
## 17  2000  1999.972
## 18  1774  1774.031
## 19  1376  1375.955
## 20  1835  1835.009
## 21  2048  2048.034
## 22  2124  2123.909
## 23  1768  1767.959
## 24  1732  1732.137
## 25  1440  1440.041
## 26  1932  1932.017
## 27  1932  1932.039
## 28  2106  2106.028
## 29  3529  3529.113
## 30  2051  2050.931
## 31  1573  1572.936
## 32  2829  2828.934
## 33  1630  1629.987
## 34  1840  1840.023
## 35  2066  2066.174
## 36  1702  1702.204
## 37  2750  2749.893
## 38  3880  3880.001
## 39  1854  1853.928
## 40  1421  1421.053
## 41  1662  1662.135
## 42  3331  3331.100
## 43  1656  1656.077
## 44  1171  1170.999
## 45  2293  2293.112
## 46  1764  1763.897
## 47  2768  2767.944
## 48  3733  3732.881
## 49  1536  1535.893
## 50  1638  1637.994
## 51  1972  1971.803
## 52  1478  1478.000
## 53  1408  1407.997
## 54  1812  1812.092
## 55  1722  1721.983
## 56  1780  1780.039
## 57  1674  1673.853
## 58  1850  1849.872
## 59  1925  1925.001
## 60  2343  2342.912
## 61  1567  1567.250
## 62  1664  1663.950
## 63  1386  1386.091
## 64  2617  2617.013
## 65  2321  2321.212
## 66  2638  2638.057
## 67  1915  1915.152
## 68  2589  2589.083
## 69  2709  2708.944
## 70  1587  1587.092
## 71  1694  1693.960
## 72  1536  1536.059
## 73  3662  3662.146
## 74  1736  1736.030
## 75  2205  2205.106
## 76  1502  1501.912
## 77  1696  1696.024
## 78  2186  2186.050
## 79  1928  1928.063
## 80  1294  1294.067
## 81  1535  1534.989
## 82  1980  1980.043
## 83  2090  2089.849
## 84  1837  1836.923
## 85  1715  1715.065
## 86  1574  1573.905
## 87  1185  1184.876
## 88  1774  1773.989
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model\_two }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ sqrft }\SpecialCharTok{+}\NormalTok{ lotsize, }\AttributeTok{data =}\NormalTok{ data)}
\NormalTok{model\_two}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = price ~ sqrft + lotsize, data = data)
## 
## Coefficients:
## (Intercept)        sqrft      lotsize  
##    5.932414     0.133362     0.002113
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model\_three }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(sqrft }\SpecialCharTok{\textasciitilde{}}\NormalTok{ lotsize, }\AttributeTok{data =}\NormalTok{ data)}
\NormalTok{model\_three}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = sqrft ~ lotsize, data = data)
## 
## Coefficients:
## (Intercept)      lotsize  
##   1.920e+03    1.043e-02
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model\_four }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ sqrft }\SpecialCharTok{+}\NormalTok{ sqrft\_two, }\AttributeTok{data =}\NormalTok{ data)}
\FunctionTok{coeftest}\NormalTok{(model\_four, vcovHC)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## t test of coefficients:
## 
##             Estimate Std. Error t value Pr(>|t|)
## (Intercept)    8.621     38.811  0.2221   0.8247
## sqrft       -116.927     74.427 -1.5710   0.1199
## sqrft_two    117.067     74.436  1.5727   0.1195
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{coeftest}\NormalTok{(model\_three, vcovHC)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## t test of coefficients:
## 
##               Estimate Std. Error t value  Pr(>|t|)    
## (Intercept) 1.9196e+03 4.1208e+02  4.6584 1.152e-05 ***
## lotsize     1.0430e-02 5.2537e-02  0.1985    0.8431    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{sqrft, }\AttributeTok{y=}\NormalTok{lotsize) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{203-live-session_files/figure-latex/unnamed-chunk-58-1.pdf}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\item
  Perform a hypothesis test to check whether the population relationship between \texttt{sqrft} and \texttt{price} is zero. Use \texttt{coeftest()} with the robust standard errors computed above.
\item
  Use the robust standard error and \texttt{qt} to compute a 95\% confidence interval for the coefficient \texttt{sqrft} in the second model that you estimated. \(price = \beta_{0} + \beta_{1} sqrft + \beta_{2} lotsize + \beta_{3} colonial\).
\item
  \textbf{Bootstrap.} The book \emph{very} quickly talks about bootstrapping which is the process of sampling \emph{with replacement} and fitting a model. The idea behind the bootstrap is that since the data is generated via an iid sample from the population, that you can simulate re-running your analysis by drawing repeated samples from the data that you have.
\end{enumerate}

Below is code that will conduct a boostrapping estimator of the uncertainty of the \texttt{sqrft} variable when \texttt{lotsize} and \texttt{colonial} are included in the model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model\_one }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ sqrft, }\AttributeTok{data =}\NormalTok{ data)}
\FunctionTok{coeftest}\NormalTok{(model\_one, vcovHC)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## t test of coefficients:
## 
##              Estimate Std. Error t value  Pr(>|t|)    
## (Intercept) 11.204145  39.450563  0.2840    0.7771    
## sqrft        0.140211   0.021111  6.6417 2.673e-09 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{coefci}\NormalTok{(model\_one, }\AttributeTok{level =} \FloatTok{0.95}\NormalTok{, }\AttributeTok{vcov. =}\NormalTok{ vcovHC)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                   2.5 %     97.5 %
## (Intercept) -67.2209785 89.6292686
## sqrft         0.0982442  0.1821778
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bootstrap\_sqft }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(}\AttributeTok{d =}\NormalTok{ data, }\AttributeTok{number\_of\_bootstraps =} \DecValTok{1000}\NormalTok{) \{ }
\NormalTok{  number\_of\_rows }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(d)}

\NormalTok{    coef\_sqft }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\ConstantTok{NA}\NormalTok{, number\_of\_bootstraps)}

    \ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{number\_of\_bootstraps) \{ }
\NormalTok{      bootstrap\_data }\OtherTok{\textless{}{-}}\NormalTok{ d[}\FunctionTok{sample}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\SpecialCharTok{:}\NormalTok{number\_of\_rows, }\AttributeTok{size=}\NormalTok{number\_of\_rows, }\AttributeTok{replace=}\ConstantTok{TRUE}\NormalTok{), ]  }
\NormalTok{      estimated\_model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ sqrft, }\AttributeTok{data =}\NormalTok{ bootstrap\_data)}
\NormalTok{      coef\_sqft[i]    }\OtherTok{\textless{}{-}} \FunctionTok{coef}\NormalTok{(estimated\_model)[}\StringTok{\textquotesingle{}sqrft\textquotesingle{}}\NormalTok{]}
\NormalTok{    \}}
  \FunctionTok{return}\NormalTok{(coef\_sqft)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bootstrap\_result }\OtherTok{\textless{}{-}} \FunctionTok{bootstrap\_sqft}\NormalTok{(}\AttributeTok{d =}\NormalTok{ data, }\AttributeTok{number\_of\_bootstraps =} \DecValTok{10000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

With this, it is possible to plot the distribution of these regression coefficients:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ bootstrap\_result) }\SpecialCharTok{+} 
  \FunctionTok{geom\_histogram}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{x =} \StringTok{\textquotesingle{}Estimated Coefficient\textquotesingle{}}\NormalTok{, }
    \AttributeTok{y =} \StringTok{\textquotesingle{}Count\textquotesingle{}}\NormalTok{, }
    \AttributeTok{title =} \StringTok{\textquotesingle{}Bootstrap coefficients for square footage\textquotesingle{}}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
\end{verbatim}

\includegraphics{203-live-session_files/figure-latex/unnamed-chunk-62-1.pdf}

Compute the standard deviation of the bootstrapped regression coefficients. How does this compare to the robust standard errors you computed above?

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(bootstrap\_result }\SpecialCharTok{\textless{}} \FloatTok{0.10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.0171
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      price assess bdrms lotsize sqrft colonial   lprice  lassess  llotsize   lsqrft sqrft_two
## 1  300.000  349.1     4    6126  2438        1 5.703783 5.855359  8.720297 7.798934  2437.873
## 2  370.000  351.5     3    9903  2076        1 5.913503 5.862210  9.200593 7.638198  2075.980
## 3  191.000  217.7     3    5200  1374        0 5.252274 5.383118  8.556414 7.225482  1374.181
## 4  195.000  231.8     3    4600  1448        1 5.273000 5.445875  8.433811 7.277938  1448.064
## 5  373.000  319.1     4    6095  2514        1 5.921578 5.765504  8.715224 7.829630  2513.984
## 6  466.275  414.5     5    8566  2754        1 6.144775 6.027073  9.055556 7.920810  2754.082
## 7  332.500  367.8     3    9000  2067        1 5.806640 5.907539  9.104980 7.633853  2067.199
## 8  315.000  300.2     3    6210  1731        1 5.752573 5.704449  8.733916 7.456455  1731.076
## 9  206.000  236.1     3    6000  1767        0 5.327876 5.464255  8.699514 7.477038  1767.097
## 10 240.000  256.3     3    2892  1890        0 5.480639 5.546349  7.969704 7.544332  1889.953
## 11 285.000  314.0     4    6000  2336        1 5.652489 5.749393  8.699514 7.756196  2335.950
## 12 300.000  416.5     5    7047  2634        1 5.703783 6.031887  8.860357 7.876259  2634.155
## 13 405.000  434.0     3   12237  3375        1 6.003887 6.073044  9.412219 8.124150  3374.900
## 14 212.000  279.3     3    6460  1899        0 5.356586 5.632287  8.773385 7.549083  1899.174
## 15 265.000  287.5     3    6519  2312        1 5.579730 5.661223  8.782476 7.745868  2311.739
## 16 227.400  232.9     4    3597  1760        1 5.426711 5.450609  8.187856 7.473069  1759.907
## 17 240.000  303.8     4    5922  2000        0 5.480639 5.716370  8.686430 7.600903  1999.972
## 18 285.000  305.6     3    7123  1774        1 5.652489 5.722277  8.871084 7.480992  1774.031
## 19 268.000  266.7     3    5642  1376        1 5.590987 5.586124  8.637994 7.226936  1375.955
## 20 310.000  326.0     4    8602  1835        1 5.736572 5.786897  9.059750 7.514800  1835.009
## 21 266.000  294.3     3    5494  2048        1 5.583496 5.684599  8.611412 7.624619  2048.034
## 22 270.000  318.8     3    7800  2124        1 5.598422 5.764564  8.961879 7.661057  2123.909
## 23 225.000  294.2     3    6003  1768        0 5.416101 5.684260  8.700015 7.477604  1767.959
## 24 150.000  208.0     4    5218  1732        0 5.010635 5.337538  8.559870 7.457032  1732.137
## 25 247.000  239.7     3    9425  1440        1 5.509388 5.479388  9.151121 7.272398  1440.041
## 26 275.000  294.1     3    6114  1932        0 5.616771 5.683920  8.718336 7.566311  1932.017
## 27 230.000  267.4     3    6710  1932        0 5.438079 5.588746  8.811355 7.566311  1932.039
## 28 343.000  359.9     3    8577  2106        1 5.837730 5.885826  9.056840 7.652546  2106.028
## 29 477.500  478.1     7    8400  3529        1 6.168564 6.169820  9.035987 8.168770  3529.113
## 30 350.000  355.3     4    9773  2051        1 5.857933 5.872962  9.187379 7.626083  2050.931
## 31 230.000  217.8     4    4806  1573        1 5.438079 5.383577  8.477620 7.360740  1572.936
## 32 335.000  385.0     4   15086  2829        0 5.814130 5.953243  9.621523 7.947679  2828.934
## 33 251.000  224.3     3    5763  1630        1 5.525453 5.412984  8.659213 7.396335  1629.987
## 34 235.000  251.9     4    6383  1840        1 5.459586 5.529032  8.761394 7.517521  1840.023
## 35 361.000  354.9     4    9000  2066        1 5.888878 5.871836  9.104980 7.633369  2066.174
## 36 190.000  212.5     4    3500  1702        0 5.247024 5.358942  8.160519 7.439559  1702.204
## 37 360.000  452.4     4   10892  2750        1 5.886104 6.114567  9.295784 7.919356  2749.893
## 38 575.000  518.1     5   15634  3880        1 6.354370 6.250168  9.657204 8.263591  3880.001
## 39 209.001  289.4     4    6400  1854        1 5.342339 5.667810  8.764053 7.525101  1853.928
## 40 225.000  268.1     2    8880  1421        0 5.416101 5.591360  9.091557 7.259116  1421.053
## 41 246.000  278.5     3    6314  1662        1 5.505332 5.629418  8.750525 7.415777  1662.135
## 42 713.500  655.4     5   28231  3331        1 6.570182 6.485246 10.248176 8.111028  3331.100
## 43 248.000  273.3     4    7050  1656        1 5.513429 5.610570  8.860783 7.412160  1656.077
## 44 230.000  212.1     3    5305  1171        0 5.438079 5.357058  8.576406 7.065613  1170.999
## 45 375.000  354.0     5    6637  2293        1 5.926926 5.869297  8.800415 7.737616  2293.112
## 46 265.000  252.1     3    7834  1764        1 5.579730 5.529826  8.966228 7.475339  1763.897
## 47 313.000  324.0     3    1000  2768        0 5.746203 5.780744  6.907755 7.925880  2767.944
## 48 417.500  475.5     4    8112  3733        0 6.034285 6.164367  9.001100 8.224967  3732.881
## 49 253.000  256.8     3    5850  1536        1 5.533390 5.548297  8.674197 7.336937  1535.893
## 50 315.000  279.2     4    6660  1638        1 5.752573 5.631928  8.803875 7.401231  1637.994
## 51 264.000  313.9     3    6637  1972        1 5.575949 5.749074  8.800415 7.586803  1971.803
## 52 255.000  279.8     2   15267  1478        0 5.541264 5.634075  9.633449 7.298445  1478.000
## 53 210.000  198.7     3    5146  1408        1 5.347107 5.291796  8.545975 7.249926  1407.997
## 54 180.000  221.5     3    6017  1812        1 5.192957 5.400423  8.702344 7.502186  1812.092
## 55 250.000  268.4     3    8410  1722        1 5.521461 5.592478  9.037177 7.451241  1721.983
## 56 250.000  282.3     4    5625  1780        1 5.521461 5.642970  8.634976 7.484369  1780.039
## 57 209.000  230.7     4    5600  1674        1 5.342334 5.441118  8.630522 7.422971  1673.853
## 58 258.000  287.0     4    6525  1850        1 5.552959 5.659482  8.783396 7.522941  1849.872
## 59 289.000  298.7     3    6060  1925        1 5.666427 5.699440  8.709465 7.562681  1925.001
## 60 316.000  314.6     4    5539  2343        0 5.755742 5.751302  8.619569 7.759187  2342.912
## 61 225.000  291.0     3    7566  1567        0 5.416101 5.673323  8.931419 7.356918  1567.250
## 62 266.000  286.4     4    5484  1664        1 5.583496 5.657390  8.609590 7.416980  1663.950
## 63 310.000  253.6     6    5348  1386        1 5.736572 5.535758  8.584478 7.234177  1386.091
## 64 471.250  482.0     5   15834  2617        1 6.155389 6.177944  9.669915 7.869784  2617.013
## 65 335.000  384.3     4    8022  2321        1 5.814130 5.951424  8.989944 7.749753  2321.212
## 66 495.000  543.6     4   11966  2638        1 6.204558 6.298213  9.389825 7.877776  2638.057
## 67 279.500  336.5     4    8460  1915        1 5.633002 5.818598  9.043104 7.557473  1915.152
## 68 380.000  515.1     4   15105  2589        1 5.940171 6.244361  9.622781 7.859027  2589.083
## 69 325.000  437.0     4   10859  2709        0 5.783825 6.079933  9.292749 7.904335  2708.944
## 70 220.000  263.4     3    6300  1587        1 5.393628 5.573674  8.748305 7.369601  1587.092
## 71 215.000  300.4     3   11554  1694        0 5.370638 5.705115  9.354787 7.434848  1693.960
## 72 240.000  250.7     3    6000  1536        1 5.480639 5.524257  8.699514 7.336937  1536.059
## 73 725.000  708.6     5   31000  3662        0 6.586172 6.563291 10.341743 8.205765  3662.146
## 74 230.000  276.3     3    4054  1736        1 5.438079 5.621487  8.307459 7.459339  1736.030
## 75 306.000  388.6     2   20700  2205        0 5.723585 5.962551  9.937889 7.698483  2205.106
## 76 425.000  252.5     3    5525  1502        0 6.052089 5.531411  8.617039 7.314553  1501.912
## 77 318.000  295.2     4   92681  1696        1 5.762052 5.687653 11.436919 7.436028  1696.024
## 78 330.000  359.5     3    8178  2186        1 5.799093 5.884714  9.009203 7.689829  2186.050
## 79 246.000  276.2     4    5944  1928        1 5.505332 5.621125  8.690138 7.564239  1928.063
## 80 225.000  249.8     3   18838  1294        0 5.416101 5.520660  9.843632 7.165493  1294.067
## 81 111.000  202.4     4    4315  1535        1 4.709530 5.310246  8.369853 7.336286  1534.989
## 82 268.125  254.0     3    5167  1980        1 5.591453 5.537334  8.550048 7.590852  1980.043
## 83 244.000  306.8     4    7893  2090        1 5.497168 5.726196  8.973732 7.644919  2089.849
## 84 295.000  318.3     3    6056  1837        1 5.686975 5.762994  8.708805 7.515889  1836.923
## 85 236.000  259.4     3    5828  1715        0 5.463832 5.558371  8.670429 7.447168  1715.065
## 86 202.500  258.1     3    6341  1574        0 5.310740 5.553347  8.754792 7.361375  1573.905
## 87 219.000  232.0     2    6362  1185        0 5.389072 5.446737  8.758098 7.077498  1184.876
## 88 242.000  252.0     4    4950  1774        1 5.488938 5.529429  8.507143 7.480992  1773.989
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{wilcox.test}\NormalTok{(bdrms }\SpecialCharTok{\textasciitilde{}}\NormalTok{ colonial, }\AttributeTok{data =}\NormalTok{ data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Wilcoxon rank sum test with continuity correction
## 
## data:  bdrms by colonial
## W = 538.5, p-value = 0.004926
## alternative hypothesis: true location shift is not equal to 0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(bdrms }\SpecialCharTok{\textasciitilde{}}\NormalTok{ colonial, }\AttributeTok{data =}\NormalTok{ data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Welch Two Sample t-test
## 
## data:  bdrms by colonial
## t = -3.1146, df = 56.164, p-value = 0.002899
## alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0
## 95 percent confidence interval:
##  -0.9078686 -0.1971709
## sample estimates:
## mean in group 0 mean in group 1 
##        3.185185        3.737705
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{bdrms, }\AttributeTok{fill =} \FunctionTok{as.factor}\NormalTok{(colonial)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{position =} \StringTok{\textquotesingle{}dodge\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
\end{verbatim}

\includegraphics{203-live-session_files/figure-latex/unnamed-chunk-64-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data\_adam }\OtherTok{\textless{}{-}}\NormalTok{ data[}\FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{88}\NormalTok{, }\AttributeTok{size=}\DecValTok{10}\NormalTok{), ]}

\FunctionTok{lm}\NormalTok{(price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ sqrft, }\AttributeTok{data =}\NormalTok{ data\_adam)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = price ~ sqrft, data = data_adam)
## 
## Coefficients:
## (Intercept)        sqrft  
##   160.55451      0.05291
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bs2 }\OtherTok{\textless{}{-}} \FunctionTok{bootstrap\_sqft}\NormalTok{(}\AttributeTok{d =}\NormalTok{ data\_adam, }\DecValTok{1000}\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{bs2) }\SpecialCharTok{+} 
  \FunctionTok{geom\_histogram}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
\end{verbatim}

\includegraphics{203-live-session_files/figure-latex/unnamed-chunk-64-2.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data\_adam}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    price assess bdrms lotsize sqrft colonial   lprice  lassess llotsize   lsqrft sqrft_two
## 27 230.0  267.4     3    6710  1932        0 5.438079 5.588746 8.811355 7.566311  1932.039
## 87 219.0  232.0     2    6362  1185        0 5.389072 5.446737 8.758098 7.077498  1184.876
## 26 275.0  294.1     3    6114  1932        0 5.616771 5.683920 8.718336 7.566311  1932.017
## 8  315.0  300.2     3    6210  1731        1 5.752573 5.704449 8.733916 7.456455  1731.076
## 12 300.0  416.5     5    7047  2634        1 5.703783 6.031887 8.860357 7.876259  2634.155
## 55 250.0  268.4     3    8410  1722        1 5.521461 5.592478 9.037177 7.451241  1721.983
## 46 265.0  252.1     3    7834  1764        1 5.579730 5.529826 8.966228 7.475339  1763.897
## 61 225.0  291.0     3    7566  1567        0 5.416101 5.673323 8.931419 7.356918  1567.250
## 21 266.0  294.3     3    5494  2048        1 5.583496 5.684599 8.611412 7.624619  2048.034
## 16 227.4  232.9     4    3597  1760        1 5.426711 5.450609 8.187856 7.473069  1759.907
\end{verbatim}

\hypertarget{descriptive-model-building}{%
\chapter{Descriptive Model Building}\label{descriptive-model-building}}

\includegraphics[width=0.8\textwidth,height=\textheight]{./images/curve_fitting.png}

\hypertarget{learning-objectives-9}{%
\section{Learning Objectives}\label{learning-objectives-9}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
\item
\item
\end{enumerate}

\hypertarget{class-announcements-8}{%
\section{Class Announcements}\label{class-announcements-8}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The Regression Lab begins next week.

  \begin{itemize}
  \tightlist
  \item
    Your instructor will divide you into teams.
  \item
    As part of the lab, you will perform a statistical analysis using linear regression models.
  \end{itemize}
\end{enumerate}

\hypertarget{roadmap-6}{%
\section{Roadmap}\label{roadmap-6}}

\textbf{Rearview Mirror}

\begin{itemize}
\item
  Statisticians create a population model to represent the world.
\item
  The BLP is a useful way to summarize the relationship between one outcome random variable \(Y\) and input random varibles \(X_1,...,X_k\)
\item
  OLS regression is an estimator for the Best Linear Predictor (BLP)
\item
  We can capture the sampling uncertainty in an OLS regression with standard errors, and tests for model parameters.
\end{itemize}

\textbf{Today}

\begin{itemize}
\item
  The research goal determines the strategy for building a linear model.
\item
  Description means summarizing or representing data in a compact, human-understandable way.
\item
  We will capture complex relationships by transforming data, including using indicator variables and interaction terms.
\end{itemize}

\textbf{Looking Ahead}

\begin{itemize}
\item
  We will see how model building for explanation is different from building for description.
\item
  The famous Classical Linear Model (CLM) allows us to apply regression to smaller samples.
\end{itemize}

\hypertarget{discussion}{%
\section{Discussion}\label{discussion}}

\hypertarget{three-modes-of-model-building}{%
\subsection{Three modes of model building}\label{three-modes-of-model-building}}

\begin{itemize}
\tightlist
\item
  Recall the three major modes of model building: Prediction, Description, Explanation.
\item
  What is the appropriate mode for each of the following questions?
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What is going on?
\item
  Why is something going on?
\item
  What is going to happen?
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Think of a research question you are interested in. Which mode is it aligned with?
\end{itemize}

\hypertarget{the-statistical-modeling-process-in-different-modes}{%
\subsection{The statistical modeling process in different modes}\label{the-statistical-modeling-process-in-different-modes}}

\begin{itemize}
\item
  How does the modeling goal influence each of the following steps in the statistical modeling process?

  \begin{itemize}
  \item
    Choice of variables and transformation
  \item
    Choice of model (ols regression, neural nets, random forest, etc.)
  \item
    Model evaluation
  \end{itemize}
\end{itemize}

\hypertarget{r-activity-measuring-the-return-to-education}{%
\section{R Activity: Measuring the return to education}\label{r-activity-measuring-the-return-to-education}}

\begin{itemize}
\tightlist
\item
  In labor economics, a key concept is \emph{returns to education}.\\
\item
  Our goal is description: what is the relationship between education and wages? We will proceed in two steps:

  \begin{itemize}
  \tightlist
  \item
    First, we will discuss what the appropriate specifications are.
  \item
    Then we will estimate the different models to answer this question.
  \end{itemize}
\item
  We will use wage1 dataset in the wooldridge package in the following sections.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#?wage1}
\CommentTok{\#names(wage1)}
\end{Highlighting}
\end{Shaded}

\hypertarget{transformations}{%
\subsection{Transformations}\label{transformations}}

\hypertarget{applying-and-interpreting-logarithms}{%
\subsubsection{Applying and Interpreting Logarithms}\label{applying-and-interpreting-logarithms}}

\begin{itemize}
\item
  Which of the following specifications best capture the relationship between education and hourly wage? (Hint: Do a quick a EDA)

  \begin{itemize}
  \tightlist
  \item
    level-level: \(wage = \beta_0 + \beta_1 educ + u\)
  \item
    Level-log: \(wage = \beta_0 + \beta_1 \ln(educ) + u\)
  \item
    log-level: \(\ln(wage) = \beta_0 + \beta_1 educ + u\)
  \item
    log-log: \(\ln(wage) = \beta_0 + \beta_1 \ln(educ) + u\)
  \end{itemize}
\item
  What is the interpretation of \(\beta_0\) and \(\beta_1\) in your selected specification?
\item
  Can we use \(R^2\) or Adjusted \(R^2\) to choose between level-level or log-level specifications?
\end{itemize}

\textbf{Remember}

\begin{itemize}
\tightlist
\item
  \textbf{Doing a log transformation for any reason essentially implies a fundamentally different relationship between outcome (Y) and predictor (X) that we need to capture}
\end{itemize}

\hypertarget{applying-and-interpreting-polynomials}{%
\subsubsection{Applying and Interpreting Polynomials}\label{applying-and-interpreting-polynomials}}

\begin{itemize}
\item
  The following specifications include two control variables: years of experience (exper) and years at current company (tenure).
\item
  Do a quick EDA and select the specification that better suits our description goal.

  \begin{itemize}
  \item
    \(wage = \beta_0 + \beta_1 educ + \beta_2 exper + \beta_3 tenure + u\)
  \item
    \(\begin{aligned} wage &= \beta_0 + \beta_1 educ + \beta_2 exper + \beta_3 exper^2 + \\ & \beta_4 tenure + \beta_5 tenure^2 + u \end{aligned}\)
  \end{itemize}
\item
  How do you interpret the \(\beta\) coefficients?
\end{itemize}

\hypertarget{applying-and-interpreting-indicator-variables-and-interaction-terms}{%
\subsubsection{Applying and Interpreting Indicator variables and interaction terms}\label{applying-and-interpreting-indicator-variables-and-interaction-terms}}

\begin{itemize}
\item
  In the following models, first, explain why the indicator variables or interaction terms have been included. Then identify the reference group (if any) and interpret all coefficients.

  \begin{itemize}
  \item
    \(wage = \beta_0 + \beta_1 educ + \beta_2 I(educ \geq 12) + u\)
  \item
    \(wage = \beta_0 + \beta_1 educ + \beta_2 female + u\)
  \item
    \(wage = \beta_0 + \beta_1 educ + \beta_2 female + \beta_3 educ*female + u\)
  \item
    \(\begin{aligned} wage &= \beta_0 + \beta_1 female + \beta_2 I(educ = 2) + \beta_3 I(educ = 3)\\ &...+ \beta_{20} I(educ = 20) + u\\ \end{aligned}\)
  \end{itemize}
\end{itemize}

\hypertarget{estimation}{%
\subsection{Estimation}\label{estimation}}

\textbf{Estimating Returns to Education}

\begin{itemize}
\tightlist
\item
  Answer the following questions using an appropriate hypothesis test.

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    Is a year of education associated with changes to hourly wage? (Include experience and tenure without polynomial terms).
  \item
    Is the association between wage and experience / wage and tenure non-linear?
  \item
    Is there evidence for gender wage discrimination in the U.S.?
  \item
    Is there any evidence for a graduation effect on wage?
  \end{enumerate}
\item
  Display all estimated models in a regression table, and discuss the robustness of your results.
\end{itemize}

\hypertarget{explanatory-model-building}{%
\chapter{Explanatory Model Building}\label{explanatory-model-building}}

\includegraphics[width=0.8\textwidth,height=\textheight]{./images/correlation.png}

\hypertarget{learning-objectives-10}{%
\section{Learning Objectives}\label{learning-objectives-10}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
\item
\item
\end{enumerate}

\hypertarget{class-announcements-9}{%
\section{Class Announcements}\label{class-announcements-9}}

\textbf{Lab 2-Regression}

\textbf{Overview}

\begin{itemize}
\tightlist
\item
  \textbf{Setting}: You are data scientists for a maker of products.
\item
  \textbf{Task}: You select your own research question

  \begin{itemize}
  \tightlist
  \item
    Your X should be an aspect of product design
  \item
    Your Y should be a metric of product success
  \end{itemize}
\item
  \textbf{Deliverable}: A statistical analysis that includes

  \begin{itemize}
  \tightlist
  \item
    An introduction that motivates your research question
  \item
    A description of your model-building process
  \item
    A discussion of statistical assumptions that may be problematic
  \item
    A well-formatted regression table with a minimum of 3 specifications
  \item
    A conclusion that extracts key lessons from your statistical results
  \end{itemize}
\end{itemize}

\textbf{The Report}\\
- Writing for a collaborating data scientist, what research question have you asked, what answers have you found, and how did you find them?

\begin{longtable}[]{@{}lll@{}}
\toprule
Deliverable Name & Week Due & Grade Weight \\
\midrule
\endhead
Research Proposal & Week 12 & 10\% \\
Within-Team Review & Week 12 & 5\% \\
Final Presentation & Week 14 & 10\% \\
Final Report & Week 14 & 75\% \\
\bottomrule
\end{longtable}

\textbf{Team Work Evaluation}

\begin{itemize}
\tightlist
\item
  Most data science work happens on teams.
\item
  Our educational goals include helping you improve in your role as a teammate.
\item
  We'll ask you to fill out a confidential evaluation regarding your team dynamics.
\end{itemize}

\textbf{Final Presentation}

\begin{itemize}
\tightlist
\item
  Team will present their work in live session 14.

  \begin{itemize}
  \tightlist
  \item
    Teams have between 10-15 min dedicated to discussing their work (depending on section size)
  \item
    Two-thirds of the time can be the team presenting
  \item
    \textbf{BUT} at least one-third should be asking and answering questions with your peers
  \item
    For example, if teams have 15 minutes total, then plan to present for no more than 10 minutes and structure 5 minutes of questions.
  \end{itemize}
\end{itemize}

\hypertarget{roadmap-7}{%
\section{Roadmap}\label{roadmap-7}}

\textbf{Rearview Mirror}

\begin{itemize}
\tightlist
\item
  Statisticians create a population model to represent the world.
\item
  The BLP is a useful way to summarize relationships in a model, and OLS regression is a way to estimate the BLP.
\item
  OLS regression is a foundational tool that can be applied to questions of description
\end{itemize}

\textbf{Today}

\begin{itemize}
\tightlist
\item
  Questions of explanation require a substantially different modeling process.
\item
  To answer causal questions, we must work within a causal theory
\item
  OLS regression is sometimes appropriate for measuring a causal effect,
\item
  But, only when the model estimated matches the causal theory.
\item
  So, we must watch out for omitted variable bias, reverse causality, and outcome variables on the right hand side.
\end{itemize}

\textbf{Looking Ahead}

\begin{itemize}
\tightlist
\item
  The famous Classical Linear Model (CLM) allows us to apply regression to smaller samples.
\item
  We will address the pervasive issue of false discovery, and ways to be a responsible member of the scientific community.
\end{itemize}

\hypertarget{discussion-1}{%
\section{Discussion}\label{discussion-1}}

\hypertarget{path-diagrams}{%
\subsection{Path Diagrams}\label{path-diagrams}}

\[
\begin{matrix}
\\
\text{Sleep} \rightarrow \text{Feelings of Stress} \\
\\
\end{matrix}
\]

\begin{itemize}
\tightlist
\item
  How would the following fit into this causal path diagram?

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    All the other factors in the world that also cause stress but don't have a causal relationship with sleep.
  \item
    A factor: Coffee Intake

    \begin{itemize}
    \tightlist
    \item
      What happens if you omit it in your regression?
    \end{itemize}
  \item
    Reverse causality
  \item
    An outcome variable on the RHS: Job Performance

    \begin{itemize}
    \tightlist
    \item
      What happens if you include it in your regression?
    \end{itemize}
  \end{enumerate}
\end{itemize}

\hypertarget{omitted-variable-bias}{%
\subsection{Omitted Variable Bias}\label{omitted-variable-bias}}

\begin{itemize}
\tightlist
\item
  Recall the equation for omitted variable bias
\end{itemize}

\includegraphics[width=0.8\textwidth,height=\textheight]{images/ovb.png}

\begin{itemize}
\tightlist
\item
  What specific regressions do \(\beta_2\) and \(\gamma_1\) come from?
\end{itemize}

\hypertarget{r-exercise-2}{%
\section{R Exercise}\label{r-exercise-2}}

\textbf{Omitted Variable Bias in R}

The file \texttt{htv.RData} contains data from the 1991 National Longitudinal Survey of Youth, provided by Wooldridge. All people in the sample are males age 26 to 34. The data is interesting here, because it includes education, stored in the variable \texttt{educ}, and also a score on an ability test, stored in the variable \texttt{abil}.

Assume that the true model is,

\includegraphics[width=0.8\textwidth,height=\textheight]{images/wage_system.png}

\textbf{Questions:}

1- Are we able to \emph{directly} measure ability? If so, how would you propose to measure it?

2- If not, what \emph{do} we measure and how is this measurement related to ability? And there is a lot of evidence to suggest that standardized tests are not a very good proxy. But for now, let's pretend that we really are measuring ability.

3- Using R, estimate (a) the true model, and (b) the regression of ability on education.

\begin{itemize}
\item
  Write down the expression for what omitted variable bias would be if you couldn't measure ability.
\item
  Add this omitted variable bias to the coefficient for education to see what it would be.
\end{itemize}

4- Now evaluate your previous result by fitting the model, \[wage = \alpha_0 + \alpha_1 educ + w\]

\begin{itemize}
\item
  Does the coefficient for the relationship between education and wages match what you estimated earlier?
\item
  Why or why not?
\end{itemize}

5- Reflect on your results:

\begin{itemize}
\item
  What does the direction of omitted variable bias suggest about OLS estimates of returns to education?
\item
  What does this suggest about the reported statistical significance of education?
\end{itemize}

\hypertarget{discussion-2}{%
\section{Discussion}\label{discussion-2}}

\textbf{The Direction of Omitted Variable Bias}

\begin{itemize}
\tightlist
\item
  For each regression, estimate whether omitted variable bias is towards zero or away from zero.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5976}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.4024}}@{}}
\toprule
\begin{minipage}[b]{\linewidth}\raggedright
Regression Output
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Omitted Variable
\end{minipage} \\
\midrule
\endhead
\(\widehat{grade} = 72.1 + 0.4\ attendance\) & \(time\_studying\) \\
\(\widehat{lifespan} = 87.4 - 1.2\ cigarettes\) & \(exercise\) \\
\(\widehat{lifespan} = 87.4 - 1.2\ cigarettes\) & \(time\_socializing\) \\
\(\widehat{wage} = 14.0 + 2.1\ grad\_education\) & \(experience\) \\
\(\widehat{wage} = 14.0 + 2.1\ grad\_education\) & desire to effect \(social\_good\) \\
\(\widehat{literacy} = 54 + 12\ network\_access\) & \(wealth\) \\
\bottomrule
\end{longtable}

\hypertarget{the-classical-linear-model}{%
\chapter{The Classical Linear Model}\label{the-classical-linear-model}}

\hypertarget{learning-objectives-11}{%
\section{Learning Objectives}\label{learning-objectives-11}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
\item
\item
\end{enumerate}

\hypertarget{class-announcements-10}{%
\section{Class Announcements}\label{class-announcements-10}}

\begin{itemize}
\tightlist
\item
  Lab 2 Deliverable and Dates

  \begin{itemize}
  \tightlist
  \item
    Research Proposal (Today)
  \item
    Within-Team Review (Today)
  \item
    Final Report (Week 14)
  \item
    Final Presentation (Week 14)
  \end{itemize}
\end{itemize}

\hypertarget{roadmap-8}{%
\section{Roadmap}\label{roadmap-8}}

\textbf{Rearview Mirror}

\begin{itemize}
\tightlist
\item
  Statisticians create a population model to represent the world.
\item
  The BLP is a useful summary for a relationship among random variables.
\item
  OLS regression is an estimator for the Best Linear Predictor (BLP).
\item
  For a large sample, we only need two mild assumptions to work with OLS

  \begin{itemize}
  \tightlist
  \item
    To know coefficients are consistent
  \item
    To have valid standard errors, hypothesis tests
  \end{itemize}
\end{itemize}

\textbf{Today}

\begin{itemize}
\tightlist
\item
  The Classical Linear Model (CLM) allows us to apply regression to smaller samples.
\item
  The CLM requires more to be true of the data generating process, to make coefficients, standard errors, and tests \emph{meaningful} in small samples.
\item
  Understanding if the data meets these requirements (often called assumptions) requires considerable care.
\end{itemize}

\textbf{Looking Ahead}

\begin{itemize}
\tightlist
\item
  The CLM -- and the methods that we use to evaluate the CLM -- are the basis of advanced models (\emph{inter alia} time-series)
\item
  (Week 13) In a regression studies (and other studies), false discovery is a widespread problem. Understanding its causes can make you a better member of the scientific community.
\end{itemize}

\hypertarget{the-classical-linear-model-1}{%
\section{The Classical Linear Model}\label{the-classical-linear-model-1}}

\hypertarget{comparing-the-large-sample-model-and-the-clm}{%
\subsection{Comparing the Large Sample Model and the CLM}\label{comparing-the-large-sample-model-and-the-clm}}

\textbf{Part I}

\begin{itemize}
\tightlist
\item
  We say that in small samples, more needs be true of our data for OLS regression to ``work.''

  \begin{itemize}
  \tightlist
  \item
    What do we mean when we say ``work''?

    \begin{itemize}
    \tightlist
    \item
      If our goals are descriptive, how is a ``working'' estimator useful?
    \item
      If our goals are explanatory, how is a ``working'' estimator useful?
    \item
      If our goals are predictive, are the requirements the same?
    \end{itemize}
  \end{itemize}
\end{itemize}

\textbf{Part II}

\begin{itemize}
\tightlist
\item
  Suppose that you're interested in understanding how subsidized school meals benefit under-resourced students.

  \begin{itemize}
  \tightlist
  \item
    Using the tools from 201, refine this question to a data science question.
  \item
    Suppose that to answer the question you have identified, there are two data sources:

    \begin{itemize}
    \tightlist
    \item
      Individual-level data about income, nutrition and test scores, self-reported by individual families who have opted in to the study.\\
    \item
      Government data about school district characteristics, including district-level college achievement; county-level home prices, and state-level tax receipts.
    \end{itemize}
  \item
    What are the tradeoffs to these different sources?
  \end{itemize}
\end{itemize}

\textbf{Part III}

\begin{itemize}
\tightlist
\item
  Suppose you use individual-level data (you have a large sample).

  \begin{itemize}
  \tightlist
  \item
    Which of the large-sample assumptions do you expect are valid, and which are problematic?
  \end{itemize}
\item
  Say you use school-district-level data (you have a small sample).

  \begin{itemize}
  \tightlist
  \item
    Which of the CLM assumptions do you expect are valid, and which do you expect are most problematic?
  \end{itemize}
\item
  Which dataset do you think will give you more precise estimates?
\end{itemize}

\hypertarget{problems-with-the-clm-requirements}{%
\section{Problems with the CLM Requirements}\label{problems-with-the-clm-requirements}}

\begin{itemize}
\item
  There are five requirements for the CLM

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    IID Sampling
  \item
    Linear Conditional Expectation
  \item
    No Perfect Collinearity
  \item
    Homoskedastic Errors
  \item
    Normally Distributed Errors
  \end{enumerate}
\item
  For each of these requirements:

  \begin{itemize}
  \tightlist
  \item
    Identify one \textbf{concrete} way that the data might not satisfy the requirement.
  \item
    Identify what the consequence of failing to satisfy the requirement would be.
  \item
    Identify a path forward to satisfy the requirement.
  \end{itemize}
\end{itemize}

\hypertarget{r-exercise-3}{%
\section{R Exercise}\label{r-exercise-3}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(wooldridge)}
\FunctionTok{library}\NormalTok{(car)}
\FunctionTok{library}\NormalTok{(lmtest)}
\FunctionTok{library}\NormalTok{(sandwich)}
\FunctionTok{library}\NormalTok{(stargazer)}
\end{Highlighting}
\end{Shaded}

If you haven't used the \texttt{mtcars} dataset, you haven't been through an intro applied stats class!

In this analysis, we will use the mtcars dataset which is a dataset that was extracted from the 1974 Motor Trend US magazine, and comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973-74 models). The dataset is automatically available when you start R. For more information about the dataset, use the R command: help(mtcars)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(mtcars)}

\FunctionTok{glimpse}\NormalTok{(mtcars)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 32
## Columns: 11
## $ mpg  <dbl> 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8, 16.4, 17.3, 15.2, 10.4, 10~
## $ cyl  <dbl> 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8, 8, 8, 8, 4, 4, 4, 8, 6, 8,~
## $ disp <dbl> 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 167.6, 167.6, 275.8, 275.8, 2~
## $ hp   <dbl> 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180, 205, 215, 230, 66, 52, 65~
## $ drat <dbl> 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92, 3.07, 3.07, 3.07, 2.93, 3.~
## $ wt   <dbl> 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.440, 3.440, 4.070, 3.730, 3~
## $ qsec <dbl> 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18.30, 18.90, 17.40, 17.60, 1~
## $ vs   <dbl> 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,~
## $ am   <dbl> 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,~
## $ gear <dbl> 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3, 3, 3, 3, 4, 5, 5, 5, 5, 5,~
## $ carb <dbl> 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2, 2, 4, 2, 1, 2, 2, 4, 6, 8,~
\end{verbatim}

\textbf{Questions:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Using the mtcars data, run a multiple linear regression to find the relationship between displacement (\texttt{disp}), gross horsepower (\texttt{hp}), weight (\texttt{wt}), and rear axle ratio (\texttt{drat}) on the miles per gallon (\texttt{mpg}).
\item
  For \textbf{each} of the following CLM assumptions, assess whether the assumption holds. Where possible, demonstrate multiple ways of assessing an assumption. When an assumption appears violated, state what steps you would take in response.

  \begin{itemize}
  \tightlist
  \item
    I.I.D. data
  \item
    Linear conditional expectation
  \item
    No perfect collinearity
  \item
    Homoskedastic errors
  \item
    Normally distributed errors
  \end{itemize}
\item
  In addition to the above, assess to what extent (imperfect) collinearity is affecting your inference.
\item
  Interpret the coefficient on horsepower.
\item
  Perform a hypothesis test to assess whether rear axle ratio has an effect on mpg. What assumptions need to be true for this hypothesis test to be informative? Are they?
\item
  Choose variable transformations (if any) for each variable, and try to better meet the assumptions of the CLM (which also maintaining the readability of your model).
\item
  (As time allows) report the results of both models in a nicely formatted regression table.
\end{enumerate}

\hypertarget{reproducible-research}{%
\chapter{Reproducible Research}\label{reproducible-research}}

\hypertarget{learning-objectives-12}{%
\section{Learning Objectives}\label{learning-objectives-12}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
\item
\item
\end{enumerate}

\hypertarget{class-announcements-11}{%
\section{Class Announcements}\label{class-announcements-11}}

\hypertarget{roadmap-9}{%
\section{Roadmap}\label{roadmap-9}}

\textbf{Rearview Mirror}

\textbf{Today}

\textbf{Looking Ahead}

\hypertarget{what-data-science-hopes-to-accomplish}{%
\section{What data science hopes to accomplish}\label{what-data-science-hopes-to-accomplish}}

\begin{itemize}
\tightlist
\item
  As a data scientist, our goal is to learn about the world:

  \begin{itemize}
  \tightlist
  \item
    \emph{Theorists} and \emph{theologians} build systems of explanations that are consistent with themselves
  \item
    \emph{Analysts} build systems of explanations that are consistent with the past
  \item
    \emph{Scientists} build systems of explanations that usefully predict events, \textbf{or data}, that hasn't yet been seen
  \end{itemize}
\end{itemize}

\hypertarget{learning-from-data}{%
\section{Learning from Data}\label{learning-from-data}}

\begin{itemize}
\tightlist
\item
  As a data scientist, the way we learn about the world is through the streams of data that \textbf{real world} events produce

  \begin{itemize}
  \tightlist
  \item
    Machine processes
  \item
    Political outcomes
  \item
    Customer actions
  \end{itemize}
\item
  The watershed moment in our field has been the profusion of data available, from many places, that is richer than at any other point in our past.

  \begin{itemize}
  \tightlist
  \item
    In 251, and 266 we place structure on data series like audio, video and text that are \emph{transcendently} rich
  \item
    In 261 we bring together flows of data that are generated at massive scales
  \item
    In 209 we ask, ``How can we take data, and produce a \emph{new} form of it that is most effectively understood by the human visual and interactive mind?
  \end{itemize}
\end{itemize}

\hypertarget{data-science-and-statistics}{%
\section{Data Science and Statistics}\label{data-science-and-statistics}}

\begin{itemize}
\tightlist
\item
  So why statistics?
\item
  And why the way we've chosen to approach statistics in 203?
\end{itemize}

\hypertarget{why-statistics-a-closing-argument-for-statistics}{%
\section{Why Statistics?: A Closing Argument for Statistics}\label{why-statistics-a-closing-argument-for-statistics}}

\begin{itemize}
\item
  Business, policy, education and medical decisions are made \emph{by humans} based on data
\item
  A central task when we observe some pattern in data is to \textbf{infer} whether the pattern will occur in some novel context
\item
  Statistics, as we practice it in 203, allows us to characterize:

  \begin{itemize}
  \tightlist
  \item
    What we have seen
  \item
    What we \emph{could have seen}
  \item
    Whether any guarantees exist about what we have seen
  \item
    What we can infer about the population
  \end{itemize}
\item
  So that we can either describe, explain or predict behavior.
\end{itemize}

\hypertarget{course-goals}{%
\section{Course Goals}\label{course-goals}}

\hypertarget{course-section-iii-purpose-driven-models}{%
\subsection{Course Section III: Purpose-Driven Models}\label{course-section-iii-purpose-driven-models}}

\begin{itemize}
\item
  Statistical models are unknowing transformations of data

  \begin{itemize}
  \tightlist
  \item
    Because they're built on the foundation of probability, we have certain guarantees what a model ``says''
  \item
    Because they're unknowing, the models themselves know-not what they say.
  \end{itemize}
\item
  As the data scientist, bring them alive to achieve our modeling goals
\item
  In Lab 2 we have expanded our ability to parse the world using regression, built a model that accomplishes our goals, and done so in a way that brings the ability to test under a \emph{``null''} scenario

  \begin{itemize}
  \tightlist
  \item
    \textbf{Key insight}: regression is little more than conditional averages
  \end{itemize}
\end{itemize}

\hypertarget{course-section-ii-sampling-theory-and-testing}{%
\subsection{Course Section II: Sampling Theory and Testing}\label{course-section-ii-sampling-theory-and-testing}}

\begin{itemize}
\tightlist
\item
  Under \textbf{very} general assumptions, sample averages follow a predictable, known, distribution -- the \emph{Gaussian distribution}
\item
  This is true, even when the underlying probability distribution is \emph{very} complex, or unknown!
\item
  Due to this common distribution, we can produce reliable, general tests!
\item
  In Lab 1 we computed simple statistics, and used guarantees from sampling theory to \textbf{test} whether these differences were likely to arise under a \emph{``null''} scenario
\end{itemize}

\hypertarget{course-section-i-probability-theory}{%
\subsection{Course Section I: Probability Theory}\label{course-section-i-probability-theory}}

\begin{itemize}
\tightlist
\item
  Probability theory

  \begin{itemize}
  \tightlist
  \item
    Underlies modeling and regression (Part III);
  \item
    Underlies sampling, inference, and testing (Part II)
  \item
    \textbf{Every} model built in \textbf{every} corner of data science
  \end{itemize}
\end{itemize}

We can:

\begin{itemize}
\tightlist
\item
  Model the complex world that we live in using probability theory;
\item
  Move from a probability density function that is defined in terms of a single variable, into a function that is defined in terms of many variables
\item
  Compute useful summaries -- i.e.~the BLP, expected value, and covariance -- even with \emph{highly} complex probability density functions.
\end{itemize}

\hypertarget{statistics-as-a-foundation-for-mids}{%
\subsection{Statistics as a Foundation for MIDS}\label{statistics-as-a-foundation-for-mids}}

\begin{itemize}
\tightlist
\item
  In w203, we hope to have laid a foundation in probability that can be used not only in statistical applications, but also in every other machine learning application that are likely to ever encounter
\end{itemize}

\hypertarget{reproducibility-discussion}{%
\section{Reproducibility Discussion}\label{reproducibility-discussion}}

\textbf{Green Jelly Beans}

\textbf{What went wrong here?}

\hypertarget{discussion-3}{%
\subsection{Discussion}\label{discussion-3}}

\textbf{Status Update}
You have a dataset of the number of Facebook status updates by day of the week. You run 7 different t-tests, one for posts on Monday (versus all other days), or for Tuesday (versus all other days), etc. Only the test for Sunday is significant, with a p-value of .045, so you throw out the other tests.

Should you conclude that Sunday has a significant effect on number of posts? (How can you address this situation responsibly when you publish your results?)

\textbf{Such Update}
As before, you have a dataset of the number of Facebook status updates by day of the week. You do a little EDA and notice that Sunday seems to have more ``status updates'' than all other days, so you recode your ``day of the week'' variable into a binary one: Sunday = 1, All other days = 0. You run a t-test and get a p-value of .045. Should you conclude that Sunday has a significant effect on number of posts?

\textbf{Sunday Funday}
Suppose researcher A tests if Monday has an effect (versus all other days), Researcher B tests Tuesday (versus all other days), and so forth. Only Researcher G, who tests Sunday finds a significant effect with a p-value of .045. Only Researcher G gets to publish her work. If you read the paper, should you conclude that Sunday has a significant effect on number of posts?

\textbf{Sunday Repentence}
What if researcher G above is a sociologist that chooses to measure the effect of Sunday based on years of observing the way people behave on weekends? Researcher G is not interested in the other tests, because Sunday is the interesting day from her perspective, and she wouldn't expect any of the other tests to be significant.

\textbf{Decreasing Effect Sizes}
Many observers have noted that as studies yielding statistically significant results are repeated, estimated effect sizes go down and often become insignificant. Why is this the case?

\hypertarget{maximum-likelihood-estimation}{%
\chapter{Maximum Likelihood Estimation}\label{maximum-likelihood-estimation}}

\begin{figure}
\centering
\includegraphics{./images/salvation_mountain.jpg}
\caption{salvation mountain}
\end{figure}

\hypertarget{learning-objectives-13}{%
\section{Learning Objectives}\label{learning-objectives-13}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
\item
\item
\end{enumerate}

\hypertarget{class-announcements-12}{%
\section{Class Announcements}\label{class-announcements-12}}

\hypertarget{roadmap-10}{%
\section{Roadmap}\label{roadmap-10}}

\textbf{Rearview Mirror: What We've Seen}

\begin{itemize}
\tightlist
\item
  \textbf{WLLN}: \(\displaystyle\lim_{n \to \infty} \overline{X}_n \overset{p}{=} E[X]\)
\item
  \textbf{CLT} \(\displaystyle\lim_{n \to \infty} \bar{X}_n \overset{d}{=} N(E[X], \text{Var}[X])\)
\end{itemize}

\textbf{Today}

\begin{itemize}
\tightlist
\item
  Use maximum likelihood to generate a good guess for model parameters;
\item
  Use a confidence interval to indicate a range of plausible parameter values
\end{itemize}

\hypertarget{what-is-a-model}{%
\section{What is a model?}\label{what-is-a-model}}

\begin{itemize}
\tightlist
\item
  A data science model is:

  \begin{itemize}
  \tightlist
  \item
    A representation of the world built from random variables
  \item
    FOIS: ``agnostic'' models place minimal restrictions on joint distribution
  \item
    Parametric models (i.e.~MLE) are models based on a family of distributions.
  \item
    \(f_{Y|X}(y|\mathbf{x}) \sim g(y, \mathbf{x}; \mathbf{\theta})\)
  \end{itemize}
\end{itemize}

\hypertarget{estimation-1}{%
\section{Estimation}\label{estimation-1}}

\begin{itemize}
\item
  We have the tools to use data to infer information about the (joint) distribution
\item
  Because the joint distribution is complicated, we'll usually estimate simpler summaries of the joint distribution -- e.g.~\(E[X]\), \(V[X]\), \(E[Y|X]\), \(Cov[X,Y]\)
\item
  There are a number of techniques that you can use to develop an estimator for a parameter. These techniques vary in terms of the principle used to arrive at the estimator and the strength of the assumptions needed to support it.
\item
  However, all of these estimators are statistics meaning they are functions of the data \(\{X_i\}_{i=1}^n\)
\end{itemize}

\hypertarget{discussion-of-maximum-likelihood-estimation}{%
\section{Discussion of Maximum Likelihood Estimation}\label{discussion-of-maximum-likelihood-estimation}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What is the goal of estimating a parameter? Why is this something that we are interested in as data scientists?
\item
  In your own words, describe how the method of maximum likelihood is used to estimate the unknown parameters.
\item
  Why does a likelihood function have a \(\Pi\) (product operator) within it?
\item
  Is it possible to estimate using maximum likelihood without writing down a model for the data?
\item
  What happens if your model for the data is wrong? Are your estimates for the parameters ``incorrect''? Or, are they ``correct'' within the context of the model that you've written down?
\end{enumerate}

\hypertarget{optimization-in-r}{%
\section{Optimization in R}\label{optimization-in-r}}

\begin{itemize}
\tightlist
\item
  The method of maximum likelihood requires an optimization routine.
\item
  For a few very simple probability models, a closed-form solution exists and the MLE can be derived by hand. (This is also \emph{potentially} the case for OLS regression.)
\item
  But, instead lets use some machine learning to find the estimates that maximize the likelihood function.
\item
  There are many optimizers (e.g.~\texttt{optimize}, and \texttt{optim}). \texttt{optimize} is the simplest to use, but only works in one dimension.
\end{itemize}

\hypertarget{optimization-example-optimum-price}{%
\subsection{Optimization Example: Optimum Price}\label{optimization-example-optimum-price}}

\begin{itemize}
\tightlist
\item
  Suppose that a firm's profit from selling a product is related to price, \(p\), and cost, \(c\), as follows:
\end{itemize}

\[ 
\text{profit} = (p - p^2) - c + 100
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Explain how you would use calculus to find the maximizing price. Assume that cost is fixed.
\item
  What is the firms revenue as \texttt{p=0,\ cost\ =\ 2}? What is it at \texttt{p=10,\ cost\ =\ 2}?
\item
  Create a plot with the following characteristics:

  \begin{itemize}
  \tightlist
  \item
    On the x-axis is a sequence (\texttt{seq()}) of prices from {[}0, 10{]}.
  \item
    On the y-axis is the revenue as a function of those prices. Hold cost constant at \texttt{c=2}.\\
  \item
    What does the best price seem to be?
  \end{itemize}
\item
  Solve this numerically in \emph{R}, using the \emph{optimize()} function.

  \begin{itemize}
  \tightlist
  \item
    Take note: using the default arguments, will \texttt{optimize} try to find a maximum or a minimum?
  \item
    Check into the help documentation.
  \end{itemize}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{profit }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(p, c) \{ }
\NormalTok{  r }\OtherTok{=}\NormalTok{ (p }\SpecialCharTok{{-}}\NormalTok{ p}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{{-}}\NormalTok{ c }\SpecialCharTok{+} \DecValTok{100}
  \FunctionTok{return}\NormalTok{(r) }
\NormalTok{  \} }
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{profit}\NormalTok{(}\AttributeTok{p=}\DecValTok{2}\NormalTok{, }\AttributeTok{c=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 96
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{best\_price }\OtherTok{\textless{}{-}} \FunctionTok{optimize}\NormalTok{(}
\NormalTok{  profit,                    }\CommentTok{\# profit is the function}
  \AttributeTok{lower =} \DecValTok{0}\NormalTok{, }\AttributeTok{upper =} \DecValTok{1000}\NormalTok{,   }\CommentTok{\# this is the low and high we consider}
  \AttributeTok{c =} \DecValTok{2}\NormalTok{,                     }\CommentTok{\# here, we\textquotesingle{}re passing cost into profit}
  \AttributeTok{maximum =} \ConstantTok{TRUE}\NormalTok{)            }\CommentTok{\# we\textquotesingle{}d like to maximize, not minimize}
\NormalTok{best\_price}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $maximum
## [1] 0.5
## 
## $objective
## [1] 98.25
\end{verbatim}

\hypertarget{mle-for-poisson-random-variables}{%
\section{MLE for Poisson Random Variables}\label{mle-for-poisson-random-variables}}

\begin{itemize}
\tightlist
\item
  Suppose we use a camera to record an intersection for a particular length of time, and we write down the number of cars accidents in that interval.\\
\item
  This process can be modeled by a \emph{Poisson} random variable (now we are non-agnostic), that has a well-known probability mass function given by,
\end{itemize}

\[
f(x;\lambda) = \frac{\lambda^x e^{-\lambda}}{x!}
\]

Here is an example of a string of outcomes generated by a Poisson RV, with parameter \(\lambda = 2\).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rpois}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{, }\AttributeTok{lambda =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 0 2 0 1 1 3 2 3 0 1
\end{verbatim}

\hypertarget{mle-for-poisson-random-variables-data}{%
\subsection{MLE for Poisson Random Variables: Data}\label{mle-for-poisson-random-variables-data}}

\begin{itemize}
\tightlist
\item
  Suppose that we conduct an iid sample, and gather the following number of accidents. (It is a busy street!)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}
  \DecValTok{2}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{24}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{1}\NormalTok{,  }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{5}\NormalTok{, }
  \DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{,  }\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{,  }\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{3}\NormalTok{, }
  \DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{,  }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{,  }\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }
  \DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{,  }\DecValTok{5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{,  }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }
  \DecValTok{4}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{2}\NormalTok{,  }\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{,  }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }
  \DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{,  }\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{,  }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }
  \DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{2}\NormalTok{,  }\DecValTok{7}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{,  }\DecValTok{5}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }
  \DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{,  }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{,  }\DecValTok{4}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{, }
  \DecValTok{6}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{,  }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }
  \DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{2}\NormalTok{,  }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{1}\NormalTok{,  }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}
\NormalTok{  )}

\FunctionTok{table}\NormalTok{(data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## data
##  1  2  3  4  5  6  7  9 10 24 
## 54 69 38 14 14  6  1  2  1  1
\end{verbatim}

\hypertarget{mle-estimation}{%
\subsection{MLE Estimation}\label{mle-estimation}}

\begin{itemize}
\item
  Use the data that is stored in \texttt{data}, together with a Poisson model to estimate the \(\lambda\) values that produce the ``good'' model from the Poisson family.
\item
  That is, use MLE to estimate \(\lambda\).
\item
  Here is your work flow:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    Define your random variables.
  \item
    Write down the likelihood function for a sample of data that is generated by a \emph{Poisson} process.
  \item
    To make the math easier, take the log of this likelihood function.
  \item
    Optimize this log-likelihood using calculus -- what is the value of \(\lambda\) that results? Compute this value, given the data that you have.
  \item
    Maximize this log-likelihood numerically, and report the value for \(\lambda\) that produces the highest likelihood of seeing this data.
  \item
    Comment on your answers from parts 4 and 5. Are you surprised or not by what you see?
  \end{enumerate}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{poisson\_ll }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data, lambda) \{ }
  \DocumentationTok{\#\# fill this in:  }
\NormalTok{  lambda }\CommentTok{\# this is a placeholder, change this}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{search\_space }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{100}\NormalTok{, }\AttributeTok{by =} \FloatTok{0.1}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(}
  \AttributeTok{x =}\NormalTok{ search\_space, }\AttributeTok{xlab =} \StringTok{\textquotesingle{}Search Space\textquotesingle{}}\NormalTok{,}
  \AttributeTok{y =} \FunctionTok{poisson\_ll}\NormalTok{(}\AttributeTok{data=}\NormalTok{data, }\AttributeTok{lambda=}\NormalTok{search\_space), }\AttributeTok{ylab =} \StringTok{\textquotesingle{}Log Likelihood\textquotesingle{}}\NormalTok{,}
  \AttributeTok{type =} \StringTok{\textquotesingle{}l\textquotesingle{}}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{203-live-session_files/figure-latex/plot the likelihood-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# optimize(poisson\_ll, lower = 0, upper = 100, data = data, maximum = TRUE)}
\end{Highlighting}
\end{Shaded}

\hypertarget{confidence-intervals}{%
\section{Confidence Intervals}\label{confidence-intervals}}

This exercise is meant to demonstrate what the confidence level in a confidence interval represents. We will assume a standard normal population distribution and simulate what happens when we draw a sample and compute a confidence interval.

Your task is to complete the following function so that it,

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  simulates and stores\texttt{}n draws from a standard normal distribution
\item
  based on those draws, computes a valid confidence interval with confidence level \(\alpha\), a parameter that you pass to the function.
\end{enumerate}

Your function should return a vector of length 2, containing the lower bound and upper bound of the confidence interval.

\[
CI_{\alpha} = \overline{X} \pm t_{\alpha/2} \cdot \frac{s}{\sqrt{n}}
\]

where:

\begin{itemize}
\tightlist
\item
  \(CI_\alpha\) is the confidence interval that you're seeking to produce
\item
  \(\overline{X}\) is the sample average,\\
\item
  \(t_{\alpha/2}\) is your critical value (accessible through \texttt{qt}),
\item
  and \(s\) is your sample standard deviation. Notice that you'll need each of these pieces in the code that you're about to write.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim\_conf\_int }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(n, alpha) \{}
  \CommentTok{\# Fill in your code to: }
  \CommentTok{\# 1. simulate n draws from a standard normal dist.}
  \CommentTok{\# 2. compute a confidence interval with confidence level alpha}
  
\NormalTok{  sample\_draws }\OtherTok{\textless{}{-}} \StringTok{\textquotesingle{}fill this in\textquotesingle{}}
\NormalTok{  sample\_mean  }\OtherTok{\textless{}{-}} \StringTok{\textquotesingle{}fill this in\textquotesingle{}}
\NormalTok{  sample\_sd    }\OtherTok{\textless{}{-}} \StringTok{\textquotesingle{}fill this in\textquotesingle{}}
  
\NormalTok{  critical\_t }\OtherTok{\textless{}{-}} \StringTok{\textquotesingle{}fill this in\textquotesingle{}}
  
\NormalTok{  ci\_95 }\OtherTok{\textless{}{-}} \StringTok{\textquotesingle{}fill this in\textquotesingle{}}
  
  \FunctionTok{return}\NormalTok{(ci\_95)  }
\NormalTok{\}}

\FunctionTok{sim\_conf\_int}\NormalTok{(}\AttributeTok{n =} \DecValTok{100}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.25}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "fill this in"
\end{verbatim}

When your function is complete, you can use the following code to run your function 100 times and plot the results.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{many\_confidence\_intervals }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(num\_simulations, n, alpha) \{}
  \DocumentationTok{\#\# args: }
  \DocumentationTok{\#\#  {-} num\_simulations: the number of simulated confidence intervals }
  \DocumentationTok{\#\#  {-} n: the number of observations in each simulation that will pass }
  \DocumentationTok{\#\#           into your \textasciigrave{}sim\_conf\_int\textasciigrave{} function}
  \DocumentationTok{\#\#  {-} alpha: the confidence interval that you will pass into }
  \DocumentationTok{\#\#           your \textasciigrave{}sim\_conf\_int\textasciigrave{} function}
  
\NormalTok{  results }\OtherTok{\textless{}{-}} \ConstantTok{NULL}
  \ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{num\_simulations) \{}
\NormalTok{    interval }\OtherTok{=} \FunctionTok{sim\_conf\_int}\NormalTok{(n, alpha)}
\NormalTok{    results }\OtherTok{=} \FunctionTok{rbind}\NormalTok{(results, }\FunctionTok{c}\NormalTok{(interval[}\DecValTok{1}\NormalTok{], interval[}\DecValTok{2}\NormalTok{], interval[}\DecValTok{1}\NormalTok{]}\SpecialCharTok{\textless{}}\DecValTok{0} \SpecialCharTok{\&}\NormalTok{ interval[}\DecValTok{2}\NormalTok{]}\SpecialCharTok{\textgreater{}}\DecValTok{0}\NormalTok{))}
\NormalTok{  \}}
\NormalTok{  resultsdf }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(results)}
  \FunctionTok{names}\NormalTok{(resultsdf) }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"low"}\NormalTok{, }\StringTok{"high"}\NormalTok{, }\StringTok{"captured"}\NormalTok{)}
  \FunctionTok{return}\NormalTok{(resultsdf)}
\NormalTok{\}}

\NormalTok{n }\OtherTok{=} \DecValTok{20}
\NormalTok{confidence\_intervals }\OtherTok{=} \FunctionTok{many\_confidence\_intervals}\NormalTok{(}\DecValTok{100}\NormalTok{, n, .}\DecValTok{05}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plot\_many\_confidence\_intervals }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(c) \{ }
  \FunctionTok{plot}\NormalTok{(}\ConstantTok{NULL}\NormalTok{, }\AttributeTok{type =} \StringTok{"n"}\NormalTok{,}
       \AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{100}\NormalTok{), }\AttributeTok{xlab =} \StringTok{\textquotesingle{}Trial\textquotesingle{}}\NormalTok{,}
       \AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\FunctionTok{min}\NormalTok{(c}\SpecialCharTok{$}\NormalTok{low), }\FunctionTok{max}\NormalTok{(c}\SpecialCharTok{$}\NormalTok{high)), }\AttributeTok{ylab=}\FunctionTok{expression}\NormalTok{(mu),}\AttributeTok{pch=}\DecValTok{19}\NormalTok{)}

  \FunctionTok{abline}\NormalTok{(}\AttributeTok{h =} \DecValTok{0}\NormalTok{, }\AttributeTok{col =} \StringTok{\textquotesingle{}gray\textquotesingle{}}\NormalTok{)}
  \FunctionTok{abline}\NormalTok{(}\AttributeTok{h =} \FunctionTok{qt}\NormalTok{(}\FloatTok{0.975}\NormalTok{, n}\DecValTok{{-}1}\NormalTok{)}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(n), }\AttributeTok{lty =} \DecValTok{2}\NormalTok{, }\AttributeTok{col =} \StringTok{\textquotesingle{}gray\textquotesingle{}}\NormalTok{)}
  \FunctionTok{abline}\NormalTok{(}\AttributeTok{h =} \FunctionTok{qt}\NormalTok{(}\FloatTok{0.025}\NormalTok{, n}\DecValTok{{-}1}\NormalTok{)}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(n), }\AttributeTok{lty =} \DecValTok{2}\NormalTok{, }\AttributeTok{col =} \StringTok{\textquotesingle{}gray\textquotesingle{}}\NormalTok{)}
  
  \FunctionTok{points}\NormalTok{(c}\SpecialCharTok{$}\NormalTok{high, }\AttributeTok{col =} \DecValTok{2}\SpecialCharTok{+}\NormalTok{c}\SpecialCharTok{$}\NormalTok{captured, }\AttributeTok{pch =} \DecValTok{20}\NormalTok{)}
  \FunctionTok{points}\NormalTok{(c}\SpecialCharTok{$}\NormalTok{low,  }\AttributeTok{col =} \DecValTok{2}\SpecialCharTok{+}\NormalTok{c}\SpecialCharTok{$}\NormalTok{captured, }\AttributeTok{pch =} \DecValTok{20}\NormalTok{)}
  \ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{nrow}\NormalTok{(c)) \{}
    \FunctionTok{lines}\NormalTok{(}\FunctionTok{c}\NormalTok{(i,i), }\FunctionTok{c}\NormalTok{(c}\SpecialCharTok{$}\NormalTok{low[i],c}\SpecialCharTok{$}\NormalTok{high[i]), }\AttributeTok{col =} \DecValTok{2}\SpecialCharTok{+}\NormalTok{c}\SpecialCharTok{$}\NormalTok{captured[i], }\AttributeTok{pch =} \DecValTok{19}\NormalTok{)}
\NormalTok{  \}}
  
  \FunctionTok{title}\NormalTok{(}\FunctionTok{expression}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\StringTok{"Simulation of t{-}Confidence Intervals for "}\NormalTok{, mu,}
                          \StringTok{" with Sample Size 20"}\NormalTok{)))}

  \FunctionTok{legend}\NormalTok{(}\DecValTok{0}\NormalTok{,}\SpecialCharTok{{-}}\NormalTok{.}\DecValTok{65}\NormalTok{, }\AttributeTok{legend =} \FunctionTok{c}\NormalTok{(}\FunctionTok{expression}\NormalTok{(}\FunctionTok{paste}\NormalTok{(mu,}\StringTok{" Captured"}\NormalTok{)),}
                             \FunctionTok{expression}\NormalTok{(}\FunctionTok{paste}\NormalTok{(mu,}\StringTok{" Not Captured"}\NormalTok{))), }\AttributeTok{fill =} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\NormalTok{  \}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# plot\_many\_confidence\_intervals(confidence\_intervals)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  How many of the simulated confidence intervals contain the true mean, zero?\\
\item
  Suppose you run a single study. Based on what you've just written above, why is it incorrect to say that, ``There is a 95\% probability that the true mean is inside this (single) confidence interval''?
\end{enumerate}

\hypertarget{maximum-likelihood-example-printers}{%
\section{Maximum Likelihood Example: Printers}\label{maximum-likelihood-example-printers}}

\textbf{Part I}

Suppose that you've got a particular sequence of values: \({1, 0, 0, 1, 0, 1, 1, 1, 1, 1}\) that indicate whether a printer any particular time you try to print.

You have data from the last 10 times you tried.

\textbf{Question}:

\begin{itemize}
\tightlist
\item
  What is the probability (\(p\)) that the printer jams on the next print job?
\end{itemize}

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth,height=\textheight]{./images/office_space.jpg}
\caption{bbc, office space}
\end{figure}

\textbf{Part II}

The data resembles draws from a Bernoulli distribution.

However, even if we want to model this as a Bernoulli distribution, we do not know the value of the parameter, \(p\).

1- Define your random variable.

2- Write down the likelihood function

3- If it will make the math easier, log the likelihood function.

4- \emph{Path 1:} Maximize the likelihood using calculus

5- \emph{Path 2:} Maximize using numeric methods.

\hypertarget{appendix}{%
\chapter*{Appendix}\label{appendix}}
\addcontentsline{toc}{chapter}{Appendix}

\hypertarget{blooms-taxonomy}{%
\section*{Bloom's Taxonomy}\label{blooms-taxonomy}}
\addcontentsline{toc}{section}{Bloom's Taxonomy}

An effective rubric for student understanding is attributed to Bloom (1956). Referred to as \emph{Bloom's Taxonomy}, this proposes that there is a hierarchy of student understanding; that a student may have one \emph{level} of reasoning skill with a concept, but not another. The taxonomy proposes to be ordered: some levels of reasoning build upon other levels of reasoning.

In the learning objective that we present in for each live session, we will also identify the level of reasoning that we hope students will achieve at the conclusion of the live session.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Remember} A student can remember that the concept exists. This might require the student to define, duplicate, or memorize a set of concepts or facts.
\item
  \textbf{Understand} A student can understand the concept, and can produce a working technical and non-technical statement of the concept. The student can explain why the concept \emph{is}, or why the concept works in the way that it does.
\item
  \textbf{Apply} A student can use the concept as it is intended to be used against a novel problem.
\item
  \textbf{Analyze} A student can assess whether the concept has worked as it should have. This requires both an understanding of the intended goal, an application against a novel problem, and then the ability to introspect or reflect on whether the result is as it should be.
\item
  \textbf{Evaluate} A student can analyze multiple approaches, and from this analysis evaluate whether one or another approach has better succeeded at achieving its goals.
\item
  \textbf{Create} A student can create a new or novel method from axioms or experience, and can evaluate the performance of this new method against existing approaches or methods.
\end{enumerate}

\end{document}
