[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics for Data Science",
    "section": "",
    "text": "Live Session\n\nThis is the live session work space for the course. Our goal with this repository, is that we‚Äôre able to communicate ahead of time our aims for each week, and that you can prepare accordingly.",
    "crumbs": [
      "Live Session"
    ]
  },
  {
    "objectID": "probability-theory.html",
    "href": "probability-theory.html",
    "title": "Probabilty Theory",
    "section": "",
    "text": "Probability theory is the basis for all modeling in data science. In the first part of the course, we will cover the basics.",
    "crumbs": [
      "Probabilty Theory"
    ]
  },
  {
    "objectID": "01-probability-spaces.html",
    "href": "01-probability-spaces.html",
    "title": "1¬† Probability Spaces",
    "section": "",
    "text": "1.1 Learning Objectives\nAt the end of this week‚Äôs learning, student will be able to:",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Probability Spaces</span>"
    ]
  },
  {
    "objectID": "01-probability-spaces.html#learning-objectives",
    "href": "01-probability-spaces.html#learning-objectives",
    "title": "1¬† Probability Spaces",
    "section": "",
    "text": "Find and access all of the course materials;\nDevelop a course of study that is builds toward success;\nApply the axioms of probability to make a valid statement;\nSolve word problems through the application of probability and math rules.",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Probability Spaces</span>"
    ]
  },
  {
    "objectID": "01-probability-spaces.html#course-learning-objectives",
    "href": "01-probability-spaces.html#course-learning-objectives",
    "title": "1¬† Probability Spaces",
    "section": "1.2 Course Learning Objectives",
    "text": "1.2 Course Learning Objectives\nAt this point in the course, there is so much that is before us! As we settle in to study for the semester, it is useful to have a point of view of where we‚Äôre trying to go, and what we are going to see along the way.\nAllow a justification by analogy:\n\nSuppose that you decide that you would like to be a chef ‚Äì all of the time watching cooking shows has revealed to you that this is your life‚Äôs true calling ‚Äì and so you enroll in a culinary program.\nOne does not begin such a program by baking croissants and souffle. They begin the program with knife skills, breaking down ingredients and the basic techniques that build up to produce someone who is not a cook, but a chef ‚Äì someone who can combine ingredients and techniques to produce novel ideas.\nAt the same time, however, one has not gone to school just to become a cucumber slicer. The knife skills are instrumental to the eventual goal ‚Äì of being a chef ‚Äì but not the goal itself.\n\nAt the beginning of the program, we‚Äôre teaching these core, fundamental skills. How to read and reason with mathematical objects, how to use conditional probability with the goal of producing a model, and eventually, eventually to create novel work as a data scientist.\nAt the end of this course, students will be able to:\n\n1.2.1 Understand the building blocks of probability theory that prepare learners for the study of statistical models\n\nUnderstand the mathematical objects of probability theory and be able to apply their properties.\nUnderstand how high-level concepts from calculus and linear algebra are related to common procedures in data science.\nTranslate between problems that are defined in business or research terms into problems that can be solved with math.\n\n\n\n1.2.2 Understand and apply statistical models in common situations\n\nUnderstand the theory of statistics to prepare students for inferrential statements.\nUnderstand model parameters and high level strategies to estimate them: means, least squares, and maximum likelihood.\nChoose an appropriate statistic, and conduct a hypothesis test in the Neyman-Pearson framework.\nInterpret the results of a statistical test, including statistical significance and practical significance.\nRecognize limitations of the Neyman-Pearson hypothesis testing framework and be a conscientious participant in the scientific process\n\n\n\n1.2.3 Analyze a research question using a linear regression framework\n\nExplore and wrangle data with the intention of understanding the information and relationships that are (and are not) present\nIdentify the goals of your analysis\nBuild a model that achieves the goals of an analysis\n\n\n\n1.2.4 Interpret the results of a model and communicate them in manner appropriate to the audience\n\nIdentify their audience and report process and findings in a manner appropriate to that audience.\nConstruct regression oriented reports that provide insight for stakeholders.\nConstruct technical documents of process and code for collaboration and reproducability with peer data scientists.\nRead, understand, and assess the claims that are made in technical, regression oriented reports\n\n\n\n1.2.5 Contribute proficient, basic work, using industry standard tools and coding practices to a modern data science team.\nDemonstrate programming proficiency by translating statistical problems into code. 1. Understand and incorporate best practices for coding style and data carpentry 2. Utilize industry standard tooling for collaboration",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Probability Spaces</span>"
    ]
  },
  {
    "objectID": "01-probability-spaces.html#introductions",
    "href": "01-probability-spaces.html#introductions",
    "title": "1¬† Probability Spaces",
    "section": "1.3 Introductions",
    "text": "1.3 Introductions\n\n1.3.1 Instructor Introductions\nThe instructors for the course come to the program, and to statistics from different backgrounds. Instructors hold PhDs in statistics, astrophysics, biology, political science, computer science, and information.\n\n\n1.3.2 What does a statistician look like? You!\nIdentity shapes how people approach and understand their world.\nWe would like to acknowledge that we have limited diversity of identity among the instructors for this course. We each have been fortunate to be able to study, but we want to acknowledge that the education system in the US has systematically benefited the hegemonic groups and marginalized others voices.\nEvery one of the instructors shares a core identity as an empathetic educator that wants to understand your strengths, areas for growth, and unique point of view that is shaped by who you are. We want to see a field of data scientists who embrace each others voices, and respects people for the identies that they hold.\n\nIt doesn‚Äôt matter if you‚Äôve never taken a stats class before, or if you‚Äôre reviewing using this class. There will be challenges for everyone to overcome.\nIt doesn‚Äôt matter how old or young you are. We will all be learning frequentist statistics which is timeless.\nThe color of your skin doesn‚Äôt matter; nor does whether you identify as a woman or a man or trans or non-binary; neither does your sexual orientation. There are legacies of exclusion and discrimination against people due to these identities. We will not continue to propagate those legacies and instead will work to controvert those discriminations to build a diverse community of learning in line with the University‚Äôs Principles of Community.",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Probability Spaces</span>"
    ]
  },
  {
    "objectID": "01-probability-spaces.html#student-introductions-breakout-one",
    "href": "01-probability-spaces.html#student-introductions-breakout-one",
    "title": "1¬† Probability Spaces",
    "section": "1.4 Student Introductions [Breakout One]",
    "text": "1.4 Student Introductions [Breakout One]\nIn a breakout room of between three and four students introduce yourself!\n\nBreakout One. A name story is the unique, and individual story that describes how you came to have the name that you do. While there may be many people are called the same thing, each of their name stories is unique.\nPlease share: What is your name story?",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Probability Spaces</span>"
    ]
  },
  {
    "objectID": "01-probability-spaces.html#student-introductions-breakout-two",
    "href": "01-probability-spaces.html#student-introductions-breakout-two",
    "title": "1¬† Probability Spaces",
    "section": "1.5 Student Introductions [Breakout Two]",
    "text": "1.5 Student Introductions [Breakout Two]\nIn the same breakout room:\n\nBreakout Two. Like our names, the reasons that we joined this program, our goals and our histories are different.\nPlease share: What is your data science story? How did you wind up here, in this room today?",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Probability Spaces</span>"
    ]
  },
  {
    "objectID": "01-probability-spaces.html#probability-theory",
    "href": "01-probability-spaces.html#probability-theory",
    "title": "1¬† Probability Spaces",
    "section": "1.6 Probability Theory",
    "text": "1.6 Probability Theory\nProbability\nProbability is a system of reasoning that we use to model the world under incomplete information. This model underlies virtually every other model you‚Äôll ever use as a data scientist.\n\n\n\ntold you this would be spacey\n\n\nIn this course, probability theory builds out to random variables; when combined with sampling theory we are able to develop p-values (which are also random variables) and an inferential paradigm to communicate what we know and how certain a statement we can make about it.\nIn introduction to machine learning, literally the first model that you will train is a naive bayes classifier, which is an application of Bayes‚Äô Theorem, trained using an iterative fitting algorithm. Later in machine learning, you‚Äôll be fitting non-linear models, but at every point the input data that you are supplying to your models are generated from samples from random variables. That the world can be represented by random variables (which we will cover in the coming weeks) means that you can transform ‚Äì squeeze and smush, or stretch and pull ‚Äì variables to heighten different aspects of the variables to produce the most useful information from your data.\nAs you move into NLP, you might think of generative text as a conditional probability problem: given some particular set of words as an input, what is the most likely next word or words that someone might type?\nBeyond the direct instrumental value that we see working with probability, there are two additional aims that we have in starting the course in the manner.\nFirst, because we are starting with the axioms of probability as they apply to data science statistics, students in this course develop a much fuller understanding of classical statistics than students in most other programs. Unfortunately, it is very common for students and then professionals to see statistics as a series of rules that have to be followed absolutely and without deviation. In this view of statistics, there are distributions to memorize; there are repeated problems to solve that require the rote application of some algebraic rule (i.e.¬†compute the sample average and standard deviation of some vector); and, there are myriad, byzantine statistical tests to memorize and apply. In this view of statistics, if the real-world problem that comes to you as a data scientist doesn‚Äôt clearly fit into a box, there‚Äôs no way to move forward.\n\nStatistics like this is not fun.\n\nIn the way that we are approaching this course, we hope that you‚Äôre able to learn why certain distributions (like the normal distribution) arise repeatedly, and why we can use them. We also hope that because you know how sampling theory and random variables combine, that you can be more creative and inventive to solve problems that you haven‚Äôt seen before.\nThe second additional aim that we have for this course is that it can serve as either an introduction or a re-introduction to reading and making arguments using the language of math. For some, this will be a new language; for others, it may have been some years since they have worked with the language; for some, this will feel quite familiar. New algorithms and data science model advancements nearly always developed in the math first, and then applied into algorithms second. In our view, being a literate reader of graduate- and professional-level math is a necessary skill for any data scientist that is going to keep astride of the field as it continues to develop and these first weeks of the course are designed to bring everyone back into reading and reasoning in the language.",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Probability Spaces</span>"
    ]
  },
  {
    "objectID": "01-probability-spaces.html#axiomatic-probability",
    "href": "01-probability-spaces.html#axiomatic-probability",
    "title": "1¬† Probability Spaces",
    "section": "1.7 Axiomatic Probability",
    "text": "1.7 Axiomatic Probability\nThe book makes a point of defining our axioms of probability, calling them them\n\nKolmogorov Axioms\nLet \\(\\Omega\\) be a sample space, \\(S\\) be an event space, and \\(P\\) be a probability measure. Then, \\((\\Omega, S, P)\\) is a probability space if it satisfies the following:\n\nNon-negativity: \\(\\forall A \\in S, P(A) \\geq 0\\), where \\(P(A)\\) is finite and real.\nUnitarity: \\(P(\\Omega)=1\\).\nCountable additivity: if \\(A_1, A_2, A_3, \\dots \\in S\\) are pairwise disjoint, then\n\n\\[\nP(A_1 \\cup A_2 \\cup A_3 \\cup \\dots) = P(A_1) + P(A_2) + P(A_3) = \\sum_{i}P(A_{i})\n\\]\n\nThere is a lot going on in this definition!\nFirst things first, these are the axioms of probability (read aloud in the booming voice of a god).\nThis means that these are things that we begin from, sort of the foundational principles of the entire system of reasoning that we are going to use. In the style of argument that we‚Äôre going to make, these are things that are sort of off-limits to question. Instead, these serve as the grounding assumptions, and we see what happens as we flow forward from these statements.\nSecond, and importantly, from these axioms there are a very large set of things that we can build. The first set of things that we will build are probability statements about atomic outcomes (Theorem 1.1.4 in the book), and collections of events. But, these statements, are not the only thing that we‚Äôre limited to. We can also build Frequentist Statistics, and Bayesian Statistics and Language Models.\nIn many ways, these axioms are the fundamental particles that hold our system of probabilistic reasoning together. These are to probability what the fermions and and bosons are to physics.",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Probability Spaces</span>"
    ]
  },
  {
    "objectID": "01-probability-spaces.html#definition-vs.-theorem",
    "href": "01-probability-spaces.html#definition-vs.-theorem",
    "title": "1¬† Probability Spaces",
    "section": "1.8 Definition vs.¬†Theorem",
    "text": "1.8 Definition vs.¬†Theorem\nWhat is the difference between a definition and a theorem? On pages 10 and 11 of the textbook, there is a rapid fire collection of pink boxes. We reproduce them here (notice that they may have different index numbers than the book ‚Äì this live session book autoindexes and we‚Äôre not including every theorem and definition in this live session discussion guide).\n\nConditional Probability For \\(A, B \\in S\\) with \\(P(B) &gt; 0\\), the conditional probablity of \\(A\\) given \\(B\\) is \\[P(A|B) = \\frac{P(A\\cap B)}{P(B)}.\\]\n\n\nMultiplicative Law of Probability For \\(A, B \\in S\\) with \\(P(B) &gt; 0\\), \\[P(A|B)P(B) = P(A \\cap B)\\]\n\n\nBaye‚Äôs Rule For \\(A, B \\in S\\) with \\(P(A) &gt; 0\\) and \\(P(B) &gt; 0\\), \\[P(A|B) = \\frac{P(B|A)P(A)}{P(B)}.\\]\n\n\n\nWhat would happen to the statement of the Multiplicative Law of Probability if we did not have the definition of Conditional Probability?\nHow does one get from the definition, to the law?\nCan one get to Baye‚Äôs Rule wihtout using the Multiplicative Law of Probability?",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Probability Spaces</span>"
    ]
  },
  {
    "objectID": "01-probability-spaces.html#working-with-a-sample-space",
    "href": "01-probability-spaces.html#working-with-a-sample-space",
    "title": "1¬† Probability Spaces",
    "section": "1.9 Working with a Sample Space",
    "text": "1.9 Working with a Sample Space\nAs a way to begin lets define terms that we will use for the next activities.\n\nGroup Discussion Question\n\nWhat is the definition of a sample space?\nWhat is the definition of an event?\nHow are sample spaces, and event spaces related?\n\n\n\n1.9.1 Working with a Sample Space, Part I\n\nYou roll two six-sided dice:\n\nHow would you define an appropriate sample space, \\(\\Omega\\)?\nHow many elements exist in \\(\\Omega\\)?\nWhat is an appropriate event space, and how many elements does it have?\nGive an example of an event.\n\n\n¬†\n¬†\n¬†\n¬†\n¬†\n\n\n1.9.2 Working with a Sample Space, Part II\n\nFor a random sample of 1,000 Berkeley students:\n\nHow would you define an appropriate sample space, \\(\\Omega\\)?\nHow big is \\(\\Omega\\)? How many elements does it contain?\nWhat is an example of an event for this scenario?\nCan a single person be represented in the space twice? Why or why not?",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Probability Spaces</span>"
    ]
  },
  {
    "objectID": "01-probability-spaces.html#independence",
    "href": "01-probability-spaces.html#independence",
    "title": "1¬† Probability Spaces",
    "section": "1.10 Independence",
    "text": "1.10 Independence\nThe book provides a (characteristically) terse statement of what it means for two events to be independent of one another.\n\nIndependence of Events Events \\(A, B \\in S\\) are independent if \\[P(A \\cap B) = P(A)P(B)\\].\n\n\nIn your own words:\n\nWhat does it mean for two events to be independent of one another?\nHow do you know if two events are independent of one another?\nHow do you test if two events are independent of one another?\n\nTry using this idea of independent in two places:\n\nSuppose that you are creating a model to predict an outcome. Further, suppose that two events \\(A\\) and \\(B\\) are independent of one another. Can you use \\(B\\) to predict \\(A\\)?\nIf two events, \\(A\\) and \\(B\\) are independent, then what happens if you work through a statement of conditional probability, \\(P(A|B)\\)?",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Probability Spaces</span>"
    ]
  },
  {
    "objectID": "01-probability-spaces.html#a-practice-problem",
    "href": "01-probability-spaces.html#a-practice-problem",
    "title": "1¬† Probability Spaces",
    "section": "1.11 A practice problem",
    "text": "1.11 A practice problem\nThe last task for us to complete today is working through a practice problem on the course practice problem website. Please, click the link below, and follow us over to the the course‚Äôs practice problem website.\nlink here",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Probability Spaces</span>"
    ]
  },
  {
    "objectID": "01-probability-spaces.html#student-tasks-to-complete",
    "href": "01-probability-spaces.html#student-tasks-to-complete",
    "title": "1¬† Probability Spaces",
    "section": "1.12 Student Tasks to Complete",
    "text": "1.12 Student Tasks to Complete\nBefore next live session, please complete the homework that builds on this unit. There are two parts, an applied and a proof part. You can submit these homework as many times as you like before the due date (you will not receive feedback), and you can access this homework through bCourses.\nThe applied homework will be marked either Correct or Incorrect without partial credit applied. These are meant to be problems that you solve, and that have a single straightforward solution concept. The proof homework will be marked for partial credit (out of three points) that evaluates your argument for your solution concept.",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Probability Spaces</span>"
    ]
  },
  {
    "objectID": "02-random-variables.html",
    "href": "02-random-variables.html",
    "title": "2¬† Defining Random Variables",
    "section": "",
    "text": "2.1 Learning Objectives\nAt the end of this week‚Äôs course of study (which includes the async, sync, and homework) students should be able to\nThis week‚Äôs materials are theoretical tooling to build toward one of the first notable results of the course, conditional probability. This is the idea that, if we know that one event has occurred, we can make a conditional statement about the probability distribution for another, dependent distribution.",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Defining Random Variables</span>"
    ]
  },
  {
    "objectID": "02-random-variables.html#learning-objectives",
    "href": "02-random-variables.html#learning-objectives",
    "title": "2¬† Defining Random Variables",
    "section": "",
    "text": "Remember that random variable are neither random, or variables, but instead that they are objects thare are a foundation that we can use to reason about a world.\nUnderstand that the intuition developed by the use of set-theory probability maps into the more expressive space of random variables\nApply the appropriate mathematical transformations to move between joint, marginal, and conditional distributions.",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Defining Random Variables</span>"
    ]
  },
  {
    "objectID": "02-random-variables.html#introduction-to-the-materirals",
    "href": "02-random-variables.html#introduction-to-the-materirals",
    "title": "2¬† Defining Random Variables",
    "section": "2.2 Introduction to the Materirals",
    "text": "2.2 Introduction to the Materirals\nFrom the axioms of probability, it is possible to build a whole, expressive modeling system (that need not be grounded at all in the minutia of the world). With this probability model in place, we can describe how frequently events in the random variable will occur. When variable are dependent upon each other, we can utilize information that is encoded in this dependence in order to make predictions that are closer to the truth than predictions made without this information.\nThere is both a beauty and a tragedy when reasoning about random variables: we describe random variables using their joint density function.\n\nThe beauty is that by reasoning with such general objects ‚Äì the definitions that we create, and the theorems that we derive in this section of the course ‚Äì produce guarantees that hold in every case, no matter the function that stands in for the joint density function. We will compute several examples of specific functions to provide a chance to reason about these objects and how they ‚Äúwork‚Äù.\nThe tragedy is that in the ‚Äúreal world‚Äù, the world where we are going to eventually going to train and deploy our models, we are never provided with this joint density function. Because we don‚Äôt have access to the joint density function, in later weeks we will try to produce estimates using data. The simpler the estimate, the less data we need; the fuller the representation of the joint density function we desire, the more data we need.\n\nPerhaps this is the creation myth for probability theory: in a perfect world, we can produce a perfect result. But, in the ‚Äúfallen‚Äù world of data, we will only be able to produce approximations.",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Defining Random Variables</span>"
    ]
  },
  {
    "objectID": "02-random-variables.html#class-announcements",
    "href": "02-random-variables.html#class-announcements",
    "title": "2¬† Defining Random Variables",
    "section": "2.3 Class Announcements",
    "text": "2.3 Class Announcements\n\nHomework\n\nYou should have turned in your first homework. The solution set for this homework is scheduled to be released to you in Thursday at 2:00p. The solution set contains a full explanation of how we solved the questions posed to you. You can expect that feedback for this homework will be released back to you within seven days.\nYou can start working on your second homework when we are out of this class.\n\n\n\nStudy Groups\nIt is a very good idea for you to create a recurring time to work with a set of your classmates. Working together will help you solve questions more effectively, quickly, and will also help you to learn how to communicate what you do and do not understand about a problem to a group of collaborating data scientists. And, working together with a group will help you to find people who share data science interests with you.\n\n\nCourse Resources\nThere are several resources to support your learning. A learning object last week was that you would be introduced to each of these systems. Please continue to make sure that you have access to the:\n\nLibrary VPN to read all of the scholarly content in the known universe, including the course textbook.\nCourse LMS Page",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Defining Random Variables</span>"
    ]
  },
  {
    "objectID": "02-random-variables.html#using-definitions-of-random-variables",
    "href": "02-random-variables.html#using-definitions-of-random-variables",
    "title": "2¬† Defining Random Variables",
    "section": "2.4 Using Definitions of Random Variables",
    "text": "2.4 Using Definitions of Random Variables\n\n2.4.1 Random Variable\nWhat is a random variable? Does this definition help you?\n\nA random variable is a function \\(X : \\Omega \\rightarrow \\mathbb{R},\\) such that \\(\\forall r \\in \\mathbb{R}, \\{\\omega \\in \\Omega: X(\\omega) \\leq r\\} \\in S\\).\n\nSomeone, please, read that without using a single ‚Äúomega‚Äù, \\(\\mathbb{R}\\), or other jargon terminology. Instead, someone read this aloud and tell us what each of the concepts mean.\n\n\n\n\n\n\nWhy so serious?\n\n\n\nYou might notice that the \\(X(\\omega) \\leq r\\) feels kind of weird; why isn‚Äôt it just \\(X(\\omega) = r\\)? After all, this is a mapping from an outcome, \\(\\omega \\in \\Omega\\) to a real number, right? So, why not just be direct about it? The answer is a real deep-dive, and one that is better suited to a formal measure theory course.\nThe short (but deeply technical) answer, is that the we use this \\(\\leq r\\) range because we‚Äôre using a Boreal \\(\\sigma\\) algebra to constrain the sets that have probability measures. Why this constraint? If we don‚Äôt weird stuff can happen. Like, real weird things: you can split a sphere into pieces and create two new spheres of equal volume from the pieces (Banch-Tarski Paradox). This is an example of a formal language allowing scholars to examine really far out implications of the theory; but, it isn‚Äôt something that we‚Äôre going to engage in this course.\n\n\nThe goal of writing with math symbols like this is to be absolutely clear what concepts the author does and does not mean to invoke when they write a definition or a theorem. In a very real sense, this is a language that has specific meaning attached to specific symbols; there is a correspondence between the mathematical language and each of our home languages, but exactly what the relationship is needs to be defined into each student‚Äôs home language.\n\n\n\n\n\n\nTip\n\n\n\nWhat are the key things that random variables allow you to accomplish?\n\nSuppose that you were going to try to make a model that predicts the probability of winning ‚Äúbig money‚Äù on a slot machine. Big money might be that you get üçíüçíüçí. Can you do math with üçí?\nSuppose that you wanted to build a chatbort that uses a language model so that you don‚Äôt have to do your homework anymore. How would you go about it? Can you do math on words or concepts?\nSuppose you want to direct class support to students in 203, but their grades are scored [A, A-, ..., C+] and features include prior statistics classes grades, also scored A, A-, ..., C+].",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Defining Random Variables</span>"
    ]
  },
  {
    "objectID": "02-random-variables.html#pieces-of-a-random-variable",
    "href": "02-random-variables.html#pieces-of-a-random-variable",
    "title": "2¬† Defining Random Variables",
    "section": "2.5 Pieces of a Random Variable",
    "text": "2.5 Pieces of a Random Variable\nThere are two key pieces that must exist for every random variable. What are these pieces?\n\n\n\n\n\n\nDefinition of a Random Variable\n\n\n\nA random variable is a function \\(X : \\Omega \\rightarrow \\mathbb{R},\\) such that \\(\\forall r \\in \\mathbb{R}, \\{\\omega \\in \\Omega\\}: X(\\omega) \\leq r\\} \\in S\\).\n\n\nThe new piece is provided to us in Definition 1.2.1 Random Variable (on page 16). The older piece (from last week) that is now useful is a part of the Kolmogorov Axiom, the probabilty triple: \\((\\Omega, S, P)\\) where \\(\\Omega\\) is a sample space, \\(S\\) is an event space, and \\(P\\) is a probability-measure.\n\n\n\n\n\nSuppose that a random variable is simple and discrete. For concreteness, you could think of this random variable as the answer to the question, ‚ÄúIs the grass wet outside?‚Äù.\n\nWhat is the sample space?\nWhat is a sensible function that you might use to map from the sample space to real values?\nWhat is a sensible function that you might use to map from the sample space to real values? (A student well-seasoned in Maths might use (and define for the rest of the class) the concept of a bijective function).\nIf you simply had the values that the random variable function maps to are you guaranteed to be able to describe the entire sample space? Why or why not?\nHow would you go about determining the probability mass function for this random variable?\n\n\n\n2.5.1 Functions of Functions\n\nWhy do we say that random variables are functions? Is there some useful property of these being functions rather than any other quantity? What else could they be if not a function?\n\nWhat about a function of a random variable, which is a function of a function.\n\nLet \\(g : U \\rightarrow \\mathbb{R}\\) be some function, where \\(X(\\Omega) \\subseteq U \\subseteq \\mathbb{R}\\). Then, if \\(g \\circ X : \\Omega \\rightarrow \\mathbb{R}\\) is a random variable, we say that \\(g\\) is a function of X and write \\(g(X)\\) to denote the random variable \\(g \\circ X\\).\n\nIf a random variable is a function from the real world, or the sample space, or the outcome space to a real number, then what does it mean to define a function of a random variable?\n\nAt what point does this function work? Does this function change the sample space that is possible to observe? Or, does this function change the real-number that each outcome points to?\n\n\nSuppose that you are doing some image processing work. To keep things simple, that you are doing image classification in the style of the MNIST dataset.\n\nCan someone describe what this task is trying to accomplish?\nHas anyone done work like this?\n\nHowever, suppose that rather than having good clean indicators for whether a pixel is on or off, instead you have weak indicators ‚Äì there‚Äôs a lot of grey. A lot of the cells are marked in the range \\(0.2 - 0.3\\).\n\nHow might creating a function that re-maps this grey into more extreme values help your model?\nIs it possible to ‚Äúblur‚Äù events that are in the outcome space? Does this ‚Äúblurring‚Äù meet the requirements of a function of a random variable, as provided above?\n\n\n\n\n2.5.2 Probability Density Functions and Cumulative Distribution Functions\n\nWhat is a probability mass function?\nWhat do the Kolmogorov Axioms mean must be true about any probability mass function (pmf)?\n\n\nYou should try driving in Berkeley some time. It is a trip! Without being deliberately ageist, the city is full of ageing hippies driving beater Subaru Outbacks and making what seem to be stochastic right-or-left turns to buy incense, pottery, or just sourdough bread.\nSuppose that you are walking to campus, and you have to cross 10 crosswalks, each of which are spaced a block apart. Further, suppose that as you get closer to campus, there are fewer aging hippies, and therefore, there is decreasing risk that you‚Äôre hit by a Subaru as you cross the street. Specifically, and fortunately for our math, the risk of being hit decreases linearly with each block that you cross.\nFinally, campus provides you with the safety reports from last year, and reports that there were 55 student-Subaru incidents last year, out of 10,000 student-crosswalk crossings.\n\nWhat is the pmf for the probability that you are involved in a student-Subaru incident as you walk across these 10 blocks? What sample space, \\(\\Omega\\) is appropriate to represent this scenario?\nSuppose that you don‚Äôt leave your house ‚Äì this is a remote program after all! What is your cumulative probability of being involved in a student-subaru incident?\nWhat is the cumulative probability cmf for the probability that you are involved in a student-Subaru incident?\nSuppose that you live three blocks from campus, but your classmate lives five blocks from campus. What is the difference in the cumulative probability?\nHow would you describe the cumulative probability of being hit as you walk closer to campus? That is, suppose that you start 10 blocks away from campus, and are walking to get closer. Is your cumulative probability of being hit on your way to campus increasing or decreasing as you get closer to campus?\nHow would you describe the cumulative probability of being hit as you walk further from campus? That is, suppose that you start on campus, and you‚Äôre walking to a bar after classes. Is your cumulative probability of being hit on your way away from campus increasing or decreasing as you get further from campus?",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Defining Random Variables</span>"
    ]
  },
  {
    "objectID": "02-random-variables.html#discrete-continuous-random-variables",
    "href": "02-random-variables.html#discrete-continuous-random-variables",
    "title": "2¬† Defining Random Variables",
    "section": "2.6 Discrete & Continuous Random Variables",
    "text": "2.6 Discrete & Continuous Random Variables\nWhat, if anything is fundamentally different between discrete and continuous random variables? As a way of starting the conversation, consider the following cases:\n\nSuppose \\(X\\) is a random variable that describes the time a student spends on w203 homework 1.\n\nIf you have only granular measurement ‚Äì i.e.¬†the number of nights spent working on the homework ‚Äì is this discrete or continuous?\nIf you have the number of hours, is it discrete or continuous?\nIf you have the number of seconds? Or milliseconds?\n\nIs it possible that \\(P(X = a) = 0\\) for every point \\(a\\)? For example, that \\(P(X = 3600) = 0\\).\nDoes one of these measures have more information in it than another?\n\nHow are measurement choices that we make as designers of information capture systems ‚Äì i.e.¬†the machine processes, human processes, or other processes that we are going to work with as data scientists ‚Äì reflected in both the amount of information that is gathered, the type of information that is gathered, and the types of random variables that are manifest as a result?",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Defining Random Variables</span>"
    ]
  },
  {
    "objectID": "02-random-variables.html#moving-between-pdf-and-cdf",
    "href": "02-random-variables.html#moving-between-pdf-and-cdf",
    "title": "2¬† Defining Random Variables",
    "section": "2.7 Moving Between PDF and CDF",
    "text": "2.7 Moving Between PDF and CDF\nThe book defines pmf and cmf first as a way of developing intuition and a way of reasoning about these concepts. It then moves to defining continuous density functions, which is many ways are easier to work with although they lack the means of reasoning about them intuitively. Continuous distributions are defined in the book, and more generally, in terms of the cdf, which is the cumulative distribution function. There are technical reasons for this choice of definition, some of which are signed in the footnotes on the page where the book presents it.\nMore importantly for this course, in Definition 1.2.15 the book defines the relationship between cdf and pdf in the following way:\n\nFor a continuous random variable \\(X\\) with CDF \\(F\\), the probability density function of \\(X\\) is\n\\[\n  f(x) = \\left. \\frac{d F(u)}{du} \\right|_{u=x}, \\forall x \\in \\mathbb{R}.\n\\]\nThe implies, further, that for a random variable \\(X\\) with PDF \\(f\\), the cumulative density function of \\(X\\) is:\n\\[\nF(x) = \\int f(x) dx, \\forall x \\in \\mathbb{R}.\n\\]\n\n\nHow does this definition, which relates pdf and cdf by a means of differentiation and integration, fit with the ideas that we just developed in the context of walking to and from campus?\n\n\nSuppose that you learn than a particular random variable, \\(X\\) has the following function that describes its pdf, \\(f_{x}(x) = \\frac{1}{10}x\\). Also, suppose that you know that the smallest value that is possible for this random variable to obtain is 0.\n\nWhat is the CDF of \\(X\\)?\nWhat is the maximum possible value that \\(x\\) can obtain? How did you develop this answer, using the Kolmogorov axioms of probability?\nWhat is the cumulative probability of an outcome up to 0.5?\nWhat is the probability of an outcome between 0.25 and 0.75? Produce an answer to this in two ways:\n\nUsing the \\(PDF\\)\nUsing the \\(CDF\\)",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Defining Random Variables</span>"
    ]
  },
  {
    "objectID": "02-random-variables.html#joint-density",
    "href": "02-random-variables.html#joint-density",
    "title": "2¬† Defining Random Variables",
    "section": "2.8 Joint Density",
    "text": "2.8 Joint Density\nWorking with a single random variable helps to develop our understanding of how to relate the different features of a pdf and a cdf through differentiation and integration. However, there‚Äôs not really that much else that we can do; and, there is probably very little in our professional worlds that would look like a single random variable in isolation.\nWe really start to get to something useful when we consider joint density functions. Joint density functions describe the probability that both of two random variables. That is, if we are working with random variables \\(X\\) and \\(Y\\), then the joint density function provides a probability statement for \\(P(X \\cap Y)\\).\nIn this course, we might typically write this joint density function as \\(f_{X,Y}(x,y) = f(\\cdot)\\) where \\(f(\\cdot)\\) is the actual function that represents the joint probability. The \\(f(\\cdot)\\) means, essentially, ‚Äúsome function‚Äù where we just have not designated the specifics of the function; you might think of this as a generic function.\n\n2.8.1 Example: Uniform Joint Density\nSuppose that we know that two variables, \\(X\\) and \\(Y\\) are jointly uniformly distributed within the the support \\(x \\in [0,4], y \\in [0,4]\\). We have a requirement, imposed by the Kolmogorov Axioms that all probabilities must be non-zero, and that the total probability across the whole support must be one.\n\nCan you use these facts to determine answers to the following:\n\nWhat kind of shape does this joint pdf have?\nWhat is the specific function that describes this shape?\nIf you draw this shape on three axes, and \\(X\\), and \\(Y\\), and a \\(P(X,Y)\\), what does this plot look like?\nHow do you get from the joint density function, to a marginal density function for \\(X\\)?\nHow do you get form the joint density function, to a marginal density function for \\(Y\\)?\nHow do you get from these marginal density functions of \\(X\\) and \\(Y\\) back to the joint density? Is this always possible?\n\n\n\n\n2.8.2 Examples: Thinking Through Many Plots\n\n\n\n\n\n\n\n\n2.8.3 Triangle Math\nAfter considering the intuition for the triangle distribution, do the following: Write down the function that accords with the figure that you‚Äôre seeing above.1\n\nWhat is a full statement of the PDF of this image?\nWhat is the marginal distribution of \\(X\\), \\(f_{X}(x)\\)?\nWhat is the marginal distribution of \\(Y\\), \\(f_{Y}(y)\\)?\nUsing the definition of independence, are \\(X\\) and \\(Y\\) independent of each other?\nWhat is the CDF of \\(X\\), \\(F_{X}(x)\\)?\n\n\n\n2.8.4 Saddle Sores\nSuppose that you know that two random variables, \\(X\\) and \\(Y\\) are jointly distributed with the following pdf:\n\\[\nf_{X,Y}(x,y) =\n  \\begin{cases}\n    a * (x^{2} - y^{2} + 1), & 0 &lt; x &lt; 1, 0 &lt; y &lt; 1 \\\\\n    0, & \\text{otherwise.}\n  \\end{cases}\n\\]\nThis joint pdf is similar to the pdf that you can visualize above, under the distribution called ‚Äúsaddle‚Äù. The difference between this function and the image above is that the function bounds the with support of \\(x\\) and \\(y\\) on the range \\([0,1]\\). This is to make the math easier for us in the next step.\n\nCan you use these facts to determine the following?\n\nWhat value of \\(a\\) makes this a valid joint pdf?\nWhat is the marginal pdf of \\(x\\)? That is, what is \\(f_{x}(x)\\)?\nWhat is the conditional pdf of \\(X\\) given \\(Y\\)? That is, what is \\(f_{x|y}(x,y)\\)?\nGiven these facts, would you say that \\(X\\) and \\(Y\\) are dependent or independent?\nIf the support for this joint distribution were instead \\([0,4]\\) (rather than \\([0,1]\\)), how would the shape of the distribution change?",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Defining Random Variables</span>"
    ]
  },
  {
    "objectID": "02-random-variables.html#computing-different-distributions.",
    "href": "02-random-variables.html#computing-different-distributions.",
    "title": "2¬† Defining Random Variables",
    "section": "2.9 Computing Different Distributions.",
    "text": "2.9 Computing Different Distributions.\nSuppose that random variables \\(X\\) and \\(Y\\) are jointly continuous, with joint density function given by,\n\\[\nf(x,y) =\n  \\begin{cases}\n    c, & 0 \\leq x \\leq 1, 0 \\leq y \\leq x \\\\\n    0, & otherwise\n\\end{cases}\n\\]\nwhere \\(c\\) is a constant.\n\nDraw a graph showing the region of the X-Y plane with positive probability density.\nWhat is the constant \\(c\\)?\nCompute the marginal density function for \\(X\\). (Be sure to write a complete expression)\nCompute the conditional density function for \\(Y\\), conditional on \\(X=x\\). (Be sure to specify for what values of \\(x\\) this is defined)",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Defining Random Variables</span>"
    ]
  },
  {
    "objectID": "02-random-variables.html#conditional-probability",
    "href": "02-random-variables.html#conditional-probability",
    "title": "2¬† Defining Random Variables",
    "section": "2.10 Conditional Probability",
    "text": "2.10 Conditional Probability\nConditional probability is incredible. In fact, without exaggeration, almost all of data science is an exercise in making statements about conditional probability distributions. Don‚Äôt believe us?\n\nWhat is the goal of a ‚Äúcustomer churn‚Äù model or a conversion model?\nWhat is the goal of a language-completion model?\nWhat is the goal of flight-departures model?\n\n\nIf we possessed the whole information about a process; if we had the CDF that governed probability of occurrences, what kinds of statements would we be able to make? Would we even need data?\n\n\nUsing the distribution above, produce a statement of conditional probability, \\(f_{Y|X}(y|x)\\).",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Defining Random Variables</span>"
    ]
  },
  {
    "objectID": "02-random-variables.html#visualizing-distributions-via-simulation",
    "href": "02-random-variables.html#visualizing-distributions-via-simulation",
    "title": "2¬† Defining Random Variables",
    "section": "2.11 Visualizing Distributions Via Simulation",
    "text": "2.11 Visualizing Distributions Via Simulation\nTo this point in the course, we have focused on concepts in ‚Äúthe population‚Äù with no reference to samples. This is on purpose! We want to develop the theory that defines the best possible predictor if we knew everything (if we know formula of the function that maps from \\(\\omega \\rightarrow \\mathbb{R}\\), and we know the probability of each \\(\\omega \\in \\Omega\\) then we know everything). Beginning in week 5 of the course, we will talk about ‚Äúapproximating‚Äù (which we will call estimating) this best possible predictor with a limited sample of data.\nHowever, at this point, to help build your working understanding, or intuition, for what is happening, we are going to work on a way to simulate draws from a population. In some places, people might refer to these as Monte Carlo methods ‚Äì this is because the method was developed by von Neumann & Ulam during World War II, and they needed a way to talk about it using a code name. They chose Monte Carlo after a famous casino in Monaco.\n\n2.11.1 Example: The Uniform Distribution\n\nYou: ‚ÄúGosh. There sure are a lot of examples that use the uniform distribution. That must be a really important statistical distribution.‚Äù\nInstructor: ‚ÄúNah. Not really. We‚Äôre just using the uniform a bunch so that we don‚Äôt get too lost in doing math while we‚Äôre working with these concepts.‚Äù\n\nWe‚Äôll start with a simple uniform distribution, but then we‚Äôll make it a little more complex in a moment.\nWe can use R to simulate draws from a probability distribution function by providing it with the name of the distribution that we‚Äôre considering, the support of that distribution, or other features of the distribution. In the case of the uniform, the entire distribution is can be described just from it support.\nSo, suppose that you had a uniform distribution that had positive probability on the range \\([1.1, 4.3]\\). Why these? No particular reason. That is, suppose\n\\[\nf_{X}(x) =\n  \\begin{cases}\n    a & 1.1 \\leq x \\leq 4.3 \\\\\n    0 & otherwise\n  \\end{cases}\n\\]\nWhat does this distribution ‚Äúlook like‚Äù? Because it is a uniform, you might have a sense that it will be a horizontal line. But, what is the height of that line? Aha! We could do the math to figure it out, or we could generate an approximation using a simulation.\nIn the code below, we are going to create an object called samples_uniform that stores the results of the runif function call.\n\nsamples_uniform &lt;- runif(n=1000, min=1.1, max=4.3)\n\nWhat is happening inside runif?\nWhen you‚Äôre writing you own code, you can pull up the documentation for this (and any) function using a question mark, i.e.¬†?, followed by the function name ‚Äì ?runif.\nBut, we can speed this up slightly by simply telling you that n is the number of samples to take from the population; min is the low-end of the support, and max is the high-end of the support.\nIf we look into this object, we can see the results of the function call. Below, we will show the first \\(20\\) elements of the samples_uniform object.\n\nsamples_uniform[1:20]\n\n [1] 2.234654 3.208821 2.230534 4.134750 4.004317 3.188793 1.753870 3.559672\n [9] 2.429026 2.947665 3.912242 1.143845 3.854465 2.023560 2.952931 3.728450\n[17] 2.629490 1.708903 3.068128 2.338122\n\n\n(Notice that R is a \\(1\\) index language (python is a zero-index language).)\nWith this object created, we can plot a density of the data and then learn from this histogram what the pdf looks like.\n\nplot_full_data &lt;- ggplot() + \n  aes(x=1:length(samples_uniform), y=samples_uniform) + \n  geom_point()  + \n  labs(\n    title = 'Showing the Data', \n    y     = 'Sample Value', \n    x     = 'Index')\n\nplot_density &lt;- ggplot() + \n  aes(x=samples_uniform) + \n  geom_density(bw=0.1)   + \n  labs(\n    title = 'Showing the PDF', \n    y     = 'Probability of Drawing Value', \n    x     = 'Sample Value')\n\n(plot_full_data | (plot_density + coord_flip())) / \n  plot_density \n\n\n\n\n\n\n\n\nInteresting. From what we can see here, there does not appear to be any discernible pattern. This leaves us with two options: either, we might reduce the resolution that we‚Äôre using to view this pattern, or we might take more samples and hold the resolution constant. Below, two different plots show these differing approaches, and are very explicit about the code that creates them.\n\nsamples_uniform_moar &lt;- runif(n = 1000000, min = 1.1, max = 4.3)\n\n\nplot_low_res &lt;- ggplot()   + \n  aes(x = samples_uniform) + \n  geom_density(bw = 0.1)   + \n  lims(y = c(0,0.4))       + \n  labs(title = 'Low Res, Low Data')\n\nplot_high_res &lt;- ggplot()       + \n  aes(x = samples_uniform_moar) + \n  geom_density(bw = 0.01)       + \n  lims(y = c(0,0.4))            + \n  labs(title = 'High Res, More Data')\n\nplot_low_res | plot_high_res\n\n\n\n\n\n\n\n\n\n\n2.11.2 Example: The Normal Distribution\nFolks might have some prior beliefs about the Normal distribution. Don‚Äôt worry, we‚Äôll cover this later in the course. But, this is the distribution that you have in mind when you‚Äôre thinking of a ‚Äúbell curve‚Äù.\nWe can use the same method to visualize a normal distribution as we did for a uniform distribution. In this case, we would issue the call rnorm, together with the population parameters that define the population. At this point in the course, we do not expect that you will know these (and, actually memorizing these facts are not a core focus of the course), but you can look them up if you like. Truthfully, statistics wikipedia is very good.\nDo do you notice anything about the runif and the rnorm calls that we have identified? Both seem to name the distribution: \\(unif \\approx uniform\\) and \\(norm \\approx normal\\), but prepened with a r? This is for ‚Äúrandom draw‚Äù.\nBase R is loaded with a pile of basic statistics distributions, which you can look into using ?distributions.\n\nsamples_normal &lt;- rnorm(n = 100000, mean = 18, sd = 4)\n\nLike before, we could look at the first \\(20\\) of these samples.\n\nsamples_normal[1:20]\n\n [1] 25.98688 14.16542 26.92970 13.69004 10.07124 10.60580 17.57015 15.92421\n [9] 20.83161 20.26717 18.16140 14.18662 25.86656 12.39303 15.31269 17.63924\n[17] 12.54036 20.33083 22.81328 14.53269\n\n\nAnd, from here we could visualize this distribution.\n\nggplot() + \n  aes(x = samples_normal) + \n  geom_density() + \n  labs(title = 'Visualization of this Normal Distribution')\n\n\n\n\n\n\n\n\n\n2.11.2.1 Combining This Ability\n\nConsider three random variables \\(A, B, C\\). Suppose,\n\\[\n\\begin{aligned}    \n  A & \\sim Uniform(min=1.1, max=4.3) \\\\     \n  B & \\sim Normal(mean=18, sd=4)     \\\\     \n  C &= A + B  \\end{aligned}\n\\]\nAnd, suppose that \\(B\\) is a random variable that is described by the normal density that we considered earlier. Suppose that \\(A\\) and \\(B\\) are independent of each other.\nFinally, suppose that \\(C = A + 2B\\).\nWhat does \\(C\\) look like?\n\nAlthough this is a simple function applied to a random variable ‚Äì a legal move ‚Äì the math would be tedious. What if, instead, one used this simulation method to get a sense for the distribution?\n\nsamples_A &lt;- runif(n = 10000, min = 1.1, max = 4.3)\nsamples_B &lt;- rnorm(n = 10000, mean = 18, sd = 4)\n\nsamples_C &lt;- samples_A + samples_B\n\n\nplot_C &lt;- ggplot() + \n  aes(x = samples_C) + \n  geom_density()\n\nplot_C_and_A_and_B &lt;- ggplot()   + \n  geom_density(aes(x = samples_A), color = '#003262') + \n  geom_density(aes(x = samples_B), color = '#FDB515') + \n  geom_density(aes(x = samples_C), color = 'darkred')\n\nplot_C_and_A_and_B",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Defining Random Variables</span>"
    ]
  },
  {
    "objectID": "02-random-variables.html#review-of-terms",
    "href": "02-random-variables.html#review-of-terms",
    "title": "2¬† Defining Random Variables",
    "section": "2.12 Review of Terms",
    "text": "2.12 Review of Terms\nRemember some of the key terms we learned in the async:\n\nJoint Density Function\nConditional Distribution\nMarginal Distribution",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Defining Random Variables</span>"
    ]
  },
  {
    "objectID": "02-random-variables.html#footnotes",
    "href": "02-random-variables.html#footnotes",
    "title": "2¬† Defining Random Variables",
    "section": "",
    "text": "Notice, that in general, this kind of curve fitting isn‚Äôt really a common data science task. Instead, this is just a learning task that lets the class assess their understanding of the definitions of random variables.‚Ü©Ô∏é",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Defining Random Variables</span>"
    ]
  },
  {
    "objectID": "03-summarizing-distributions.html",
    "href": "03-summarizing-distributions.html",
    "title": "3¬† Summarizing Distributions",
    "section": "",
    "text": "3.1 Learning Objectives\nAt the end of the live session and homework this week, students will be able to:",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Summarizing Distributions</span>"
    ]
  },
  {
    "objectID": "03-summarizing-distributions.html#learning-objectives",
    "href": "03-summarizing-distributions.html#learning-objectives",
    "title": "3¬† Summarizing Distributions",
    "section": "",
    "text": "Understand the importance of thinking in terms of random variables, while;\nBeing able to appreciate that it is not typically possible to fully model the world with a single function.\nArticulate why we need a target for a model, and propose several possible such targets.\nJustify why expectation is a good model, why variance is a reasonable model, and how covariance relates two-random variables with a common joint distribution.\nProduce summaries of location and relationship given a particular functional form for a random variable.",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Summarizing Distributions</span>"
    ]
  },
  {
    "objectID": "03-summarizing-distributions.html#class-announcements",
    "href": "03-summarizing-distributions.html#class-announcements",
    "title": "3¬† Summarizing Distributions",
    "section": "3.2 Class Announcements",
    "text": "3.2 Class Announcements\nWhere have we come from, and where are we going?\n\n3.2.1 What is in the rearview mirror?\n\n\nStatisticians create a population model to represent the world; random variables are the building blocks of such a model.\nWe can describe the distribution of a random variable using:\n\nA CDF for all random variables\nA PMF for discrete random variables\nA PDF for continuous random variables\n\nWhen we have multiple random variables,\n\nThe joint PMF/PDF describes how they behave together\nThe marginal PMF/PDF describes one variable in isolation\nThe conditional PMF/PDF describes one variable given the value of another\n\n\n\n\n\n3.2.2 Today‚Äôs Lesson\nWhat might seem frustrating about this probability theory system of reasoning is that we are building a castle in the sky ‚Äì a fiction. We‚Äôre supposing that there is some function that describes the probability that values are generated. In reality, there is no such generative function; it is extremely unlikely (though we‚Äôll acknowledge that it is possible) that the physical reality we believe we exist within is just a complex simulation that has been programmed with functions by some unknown designer.\nEspecially frustrating is that we‚Äôre supposing this function, and then we‚Äôre further saying,\n\n‚ÄúIf only we had this impossible function; and if only we also had the ability to take an impossible derivative of this impossible function, then we could‚Ä¶‚Äù\n\n\n3.2.2.1 Single number summaries of a single random variable\nBut, here‚Äôs the upshot!\nWhat we are doing today is laying the baseline for models that we will introduce next week. Here, we are going to suggest that there are radical simplifications that we can produce that hold specific guarantees, no matter how complex the function that we‚Äôre reasoning about.\nIn particular, in one specific usage of the term best we will prove that the Expectation operation is the best one-number summary of any distribution. To do so, we will define a term, variance, which is the squared deviations from the expectation of a variable that describes how ‚Äúspread out‚Äù is a variable. Then, we will define a concept that is the mean squared error that is the square of the distance between a model prediction and a random variable‚Äôs realization. The key realization is that when the model predicts the expectation, then the MSE is equal to the variance of the random variable, which is the smallest possible value it could realize.\n\n\n3.2.2.2 Single number summaries of relationships between random variables\nAlthough the single number summaries are incredibly powerful, that‚Äôs not enough for today‚Äôs lesson! We‚Äôre also going to suggest that we can create a measure of linear dependence between two variables that we call the ‚Äúcovariance‚Äù, and a related, re-scaled version of this relationship that is called the correlation.\n\n\n\n3.2.3 Future Attractions\n\nA predictor is a function that provides a value for one variable, given values of some others.\nUsing our summary tools, we will define a predictor‚Äôs error and then minimize it.\nThis is a basis for linear regression",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Summarizing Distributions</span>"
    ]
  },
  {
    "objectID": "03-summarizing-distributions.html#discussion-of-terms",
    "href": "03-summarizing-distributions.html#discussion-of-terms",
    "title": "3¬† Summarizing Distributions",
    "section": "3.3 Discussion of Terms",
    "text": "3.3 Discussion of Terms\n\n3.3.1 Expected Value\nWe define the expected value to be the following for a continuous random variable:",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Summarizing Distributions</span>"
    ]
  },
  {
    "objectID": "03-summarizing-distributions.html#expected-value-1",
    "href": "03-summarizing-distributions.html#expected-value-1",
    "title": "3¬† Summarizing Distributions",
    "section": "3.4 Expected Value",
    "text": "3.4 Expected Value\nFor a continuous random variable \\(X\\) with PDF \\(f\\), the expected value of \\(X\\), written \\(E[X]\\) is\n\\[\nE[X] = \\int_{-\\infty}^{\\infty}xf_{X}(x) dx\n\\]",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Summarizing Distributions</span>"
    ]
  },
  {
    "objectID": "03-summarizing-distributions.html#computing-examples",
    "href": "03-summarizing-distributions.html#computing-examples",
    "title": "3¬† Summarizing Distributions",
    "section": "3.5 Computing Examples",
    "text": "3.5 Computing Examples\n\n3.5.1 Expected Value of Education [discrete random variable]\n\nThe expected value of a discrete random variable \\(X\\) is the weighted average of the values in the range of \\(X\\).\nSuppose that \\(X\\) represents the number of years of education that someone has completed, and so has a support that ranges from \\(0\\) years of education, up to \\(28\\) years of education. (Incidentally, Mark Labovitz has about 28 years of education.)\nYou can then think of\n\n\n\n\n\n\n\n\n\n\n\n\nWithout using specific numbers, describe the process you would use to calculate the expected value of this distribution.\n\n\n\n\n3.5.2 Using a formula\n\nDoes the following formula match with your intuitive description of the expected value? Why, or why not?\n\n\\[\n  \\begin{aligned}\n  E[X] &= \\sum_{x \\in \\{EDU\\}} x \\cdot f(x) \\\\\n       &= \\sum_{x=0}^{x=28} x\\cdot P(X=x)\n  \\end{aligned}\n\\]",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Summarizing Distributions</span>"
    ]
  },
  {
    "objectID": "03-summarizing-distributions.html#computing-by-hand",
    "href": "03-summarizing-distributions.html#computing-by-hand",
    "title": "3¬† Summarizing Distributions",
    "section": "3.6 Computing by Hand",
    "text": "3.6 Computing by Hand\n\n3.6.1 Compute the Expected Value\nLet \\(X\\) represent the result of one roll of a 6 sided die where the events \\(\\omega \\in \\Omega\\) are mapped using a straightforward function: \\(X(\\omega):\\) is a function that counts the number of spots that are showing, and maps the number of dots to the corresponding integer, \\(\\mathbb{Z}\\).\n\nCalculating by hand, what is the expected value \\(X\\), which we write as \\(E[X]\\)?\nAfter you have calculated \\(E[X]\\): Is it possible that the result of a roll is this value?\n\nblank_lines(20)\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n\n\n3.6.2 Playing a Gnome Game, Part 1\n\n\nSuppose that, out on a hike in the hills above campus, you happen across a gnome who asks you if you would like to play the following game:\n\nYou pay the gnome a dollar, and guess a number between 0 and 6. So, let \\(g \\in \\mathbb{R}: 0 \\leq g \\leq 6\\).\nAfter you make your guess, the gnome rolls a dice, which comes up with a value \\(d \\in \\mathbb{Z}: d \\in \\{1,2,3,4,5,6\\}\\).\nThe gnome pays you \\(p = 0.25 \\times |d - g|\\).\nFirst question: What is the best guess you can make?\nSecond Question: Should you play this game?\n\n\n\nFill this in by hand.\n\n\nblank_lines(20)\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n\n\n3.6.3 Compute the Variance\nLet \\(X\\) represent the result of one roll of a 6 sided die.\n\nCalculating by hand, what is the variance of \\(X\\)?\n\nblank_lines(20)\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n\n\n3.6.4 Playing a Gnome Game, Part 2\n\n\nHow much do you expect to make on any particular time that you play the game with the best strategy?\n\n\nblank_lines(20)",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Summarizing Distributions</span>"
    ]
  },
  {
    "objectID": "03-summarizing-distributions.html#expected-value-by-code",
    "href": "03-summarizing-distributions.html#expected-value-by-code",
    "title": "3¬† Summarizing Distributions",
    "section": "3.7 Expected Value by Code",
    "text": "3.7 Expected Value by Code\n\n3.7.1 Expected Value of a Six-Sided Die\nLet \\(X\\) represent the result of one roll of a 6 sided die.\n\nBuild an object to represent the whole sample space, \\(\\Omega\\) of a six sided die.\nDetermine what probabilities to assign to each value of that object.\nWrite the code to run the expectation algorithm that you just performed by hand.\n\n\ndie &lt;- data.frame(\n  value = 'fill this in',\n  prob  = 'fill this in'\n)\n\n\n\n3.7.2 Variance of a Six-Sided Die\nLet \\(X\\) represent the result of one roll of a 6 sided die. Using what you know about the definition of variance, write a function that will compute the variance of your die object.\n\nvariance_function &lt;- function(die) { \n  ## fill this in\n  mu = 'fill this in'   ## you should index to the correct column\n  var = 'fill this in'  ## for each, and use the correct function\n  \n  return(var)\n}\n\nvariance_function(die)\n\n[1] \"fill this in\"\n\n\n\nSuppose that you had to keep the values the same on the die (that is the domain of the outcome still had to be the countable set of integers from one to six), but that you could modify the actual random process. Maybe you could sand off some of the corners on the die, or you could place weights on one side so that the side is less likely to come up. In this case, \\(\\omega \\in \\{1,2,3,4,5,6\\}\\), but you‚Äôre able to make a new \\(f_{D}(d)\\).\n\nHow would you change the probability distribution to decrease the variance of this random variable?\nWhat is the smallest value that you can generate for this random variable? Use the variance_function from above to actually compute this variance.\nWhat is the largest value of variance that you can generate for this random variable? Use the variance_function from above to actually compute this variance.\n\n\n\nNow suppose that you again had an equal probability of every outcome, but you were to apply a function to the number of spots that are showing on the die. Rather that each dot contributing one value to the random variable, instead the random variable‚Äôs outcome is the square of the number of spots.\n\nHow would this change the mean?\nHow would this change the variance?",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Summarizing Distributions</span>"
    ]
  },
  {
    "objectID": "03-summarizing-distributions.html#practice-computing",
    "href": "03-summarizing-distributions.html#practice-computing",
    "title": "3¬† Summarizing Distributions",
    "section": "3.8 Practice Computing",
    "text": "3.8 Practice Computing\n\n3.8.1 Single Variable\n\nSuppose that \\(X\\) has the following density function:\n\\[\n  f_{X}(x) = \\begin{cases}\n    6x(1 - x), & 0 &lt; x &lt; 1 \\\\\n    0, & otherwise \\\\\n  \\end{cases}\n\\]\n\nFind \\(E[X]\\).\nFind \\(E[X^2]\\).\nFind \\(V[X]\\).\n\n\n\n\n3.8.2 Joint Density\n\n3.8.2.1 Discrete Case: Calculate Covariance\nIn the reading, you saw that we define covariance to be:\n\\[\n\\begin{aligned}\nCov[X,Y]    &= E[(E[X] - X)^{2}(E[Y] - Y)^{2}] \\\\\n            &= E[XY] - E[X]E[Y]\n\\end{aligned}\n\\]\nAnd, correlation to be a rescaled version of covariance:\n\\[\n\\begin{aligned}\nCor[X,Y]    & \\equiv \\rho[X,Y] \\\\\n            & = \\frac{Cov[X,Y]}{\\sigma_{X}\\sigma_{Y}} \\\\\n\\end{aligned}            \n\\]\n\nSuppose that \\(X\\) and \\(Y\\) are discrete random variables, where \\(X\\) represents number of office hours attended, and \\(Y\\) represents owning a cat. Furthermore, suppose that \\(X\\) and \\(Y\\) have the joint pmf,\n\n\n\nf(x,y)\ny=0\ny=1\n\n\n\n\nx=0\n0.10\n0.35\n\n\nx=1\n0.05\n0.05\n\n\nx=2\n0.10\n0.35\n\n\n\n\nCalculate the covariance of \\(X\\) and \\(Y\\).\nAre X and Y independent? Why or why not?\n\n\n\n\n3.8.2.2 Continuous Case: Calculate Covariance\n\nSuppose that \\(X\\) and \\(Y\\) have joint density \\(f_{X,Y}(x,y) = 8xy, 0 \\leq y &lt; x \\leq 1.\\)\n\nBreak into groups to find \\(\\operatorname{Cov}[X,Y]\\)\n\n\n\nSuppose that \\(X\\) and \\(Y\\) are random variables with joint density\n\\[\nf_{X,Y}(x,y) =\n\\begin{cases}\n1, & -y &lt; x &lt; y, 0 &lt; y &lt; 1 \\\\\n0, & \\textrm{elsewhere}\n\\end{cases}\n\\]\nShow that \\(\\operatorname{Cov}[X,Y] = 0\\) but that \\(X\\) and \\(Y\\) are dependent.",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Summarizing Distributions</span>"
    ]
  },
  {
    "objectID": "03-summarizing-distributions.html#write-code",
    "href": "03-summarizing-distributions.html#write-code",
    "title": "3¬† Summarizing Distributions",
    "section": "3.9 Write Code",
    "text": "3.9 Write Code\nSuppose that you have a random variable with a gnarly probability distribution function:\n\\[\n  f_{X}(x) = \\frac{3*\\left(x - 2x^2 + x^3\\right)}{2}, 0\\leq x\\leq 2\n\\]\nIf you had to pick a single value that minimizes the \\(MSE\\) of this function, what would it be?\n\nFirst, how would you approach this problem analytically. By this, we mean, ‚Äúhow would you solve this with the closed form answer?\nSecond, how might you approach this problem computationally. By this, we mean, ‚Äúhow might you write code that would produce a numeric approximation of the closed form solution?‚Äù Don‚Äôt worry about actually writing the code ‚Äì we‚Äôll have done that for you, but what is the process (called in our world, algorithm) that you would use to determine the value that produces the smallest \\(MSE\\)?\n\n\npdf_fun &lt;- function(x) { \n  (3/2)*(x - (2*x^2) + x^3)\n}\n\n\nsupport &lt;- seq(from=0, to=2, by=0.01)\n\n\nggplot() + \n  geom_function(fun = pdf_fun) + \n  xlim(min(support), max(support)) + \n  labs(\n    title = \"A Gnarly Plot\"\n  )\n\n\n\n\n\n\n\n\n\nexpected_value &lt;- function(value, prob){\n  sum(value * prob)\n}\n\nmse &lt;- function(c) { \n  expected_value(\n    value = (support - c)^2, \n    prob  = pdf_fun(support)\n  )  \n}\n\nmpe &lt;- function(c, power) { \n  expected_value(\n    value = (support - c)^power, \n    prob  = pdf_fun(support)\n  )\n}\n\n\nmean_absolute_error &lt;- function(c) { \n  x_values &lt;- pdf_fun(support)\n  mae_     &lt;- mean(abs(x_values - c))\n}\n\nmean_square_error &lt;- function(c) { \n  x_values &lt;- pdf_fun(support)\n  mse_     &lt;- sum(((x_values - c)^2) * x_values)\n  return(mse_)\n}\n\nmean_cubic_error &lt;- function(c) { \n  x_values &lt;- pdf_fun(support) \n  mce_     &lt;- mean((x_values - c)^3)\n}\n\nmean_quadratic_error &lt;- function(c) { \n  x_values &lt;- pdf_fun(support)\n  mqe_     &lt;- mean((x_values - c)^4)\n  return(mqe_)\n}\n\nmean_power_error &lt;- function(c, power) { \n  x_values &lt;- pdf_fun(support)\n  m_power_e_     &lt;- mean((x_values - c)^power)\n  return(m_power_e_)\n}\n\n\nmean_absolute_error  &lt;- Vectorize(mean_absolute_error)\nmean_square_error    &lt;- Vectorize(mean_square_error)\nmean_cubic_error     &lt;- Vectorize(mean_cubic_error)\nmean_quadratic_error &lt;- Vectorize(mean_quadratic_error)\nmean_power_error     &lt;- Vectorize(mean_power_error)\n\n\nmae_ &lt;- mean_absolute_error(\n  c = support\n)\nmse_ &lt;- mean_square_error(\n  c = support\n  )\nmce_ &lt;- mean_cubic_error(\n  c = support\n)\nmqe_ &lt;- mean_quadratic_error(\n  c = support\n)\n\n\nabsolute_error_ &lt;- optim(\n  par = 0, \n  fn = mean_absolute_error, \n  method = 'Brent', \n  lower = 0, upper = 2\n  )$par\n\nsquared_error_ &lt;- optim(\n  par = 0, \n  fn = mean_square_error, \n  method = \"Brent\", \n  lower = 0, upper = 2\n  )$par\ncubic_error_ &lt;- optim(\n  par = 0, \n  fn = mean_cubic_error, \n  method = \"Brent\", \n  lower = 0, upper = 2\n  )$par\nquadratic_error_ &lt;- optim(\n  par = 0, \n  fn = mean_quadratic_error, \n  method = \"Brent\", \n  lower = 0, upper = 2\n  )$par\n\n\nall_plots &lt;- ggplot() + \n  ## add lines \n  geom_line(aes(x = support, y = scale(mse_)), color = \"#003262\")  + \n  geom_line(aes(x = support, y = scale(mae_)), color = \"#FDB515\")  + \n  geom_line(aes(x = support, y = scale(mce_)), color = \"seagreen\") + \n  geom_line(aes(x = support, y = scale(mqe_)), color = \"darkred\")  +\n  ## add optimal solution indicators\n  geom_segment(\n    aes(x = squared_error_, \n    xend = squared_error_, \n    y = -2, \n    yend = -1), \n    arrow = arrow(length = unit(0.25, \"cm\")),\n    color = \"#003262\")  + \n  geom_segment(\n    aes(x = absolute_error_, \n    xend = absolute_error_, \n    y = -.2, \n    yend = -1.2), \n    arrow = arrow(length = unit(0.25, \"cm\")),\n    color = \"#FDB515\")  + \n  geom_segment(\n    aes(x = cubic_error_, \n    xend = cubic_error_, \n    y = -1, \n    yend = -2), \n    arrow = arrow(length = unit(0.25, \"cm\")),\n    color = \"seagreen\")  + \n  geom_segment(\n    aes(x = quadratic_error_, \n    xend = quadratic_error_, \n    y = -2, \n    yend = -1), \n    arrow = arrow(length = unit(0.25, \"cm\")),\n    color = \"darkred\")  + \n  labs(\n    title    = \"All Errors on a common scale\", \n    subtitle = \"Different Errors lead to different solutions.\", \n    y        = \"Error Magnitude\", \n    x        = \"Choice of X\"\n  )\n\nall_plots",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Summarizing Distributions</span>"
    ]
  },
  {
    "objectID": "04-conditional-expectation-and-BLP.html",
    "href": "04-conditional-expectation-and-BLP.html",
    "title": "4¬† Conditional Expectation and The BLP",
    "section": "",
    "text": "4.1 Thunder Struck",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Conditional Expectation and The BLP</span>"
    ]
  },
  {
    "objectID": "04-conditional-expectation-and-BLP.html#thunder-struck",
    "href": "04-conditional-expectation-and-BLP.html#thunder-struck",
    "title": "4¬† Conditional Expectation and The BLP",
    "section": "",
    "text": "thunder struck",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Conditional Expectation and The BLP</span>"
    ]
  },
  {
    "objectID": "04-conditional-expectation-and-BLP.html#learning-objectives",
    "href": "04-conditional-expectation-and-BLP.html#learning-objectives",
    "title": "4¬† Conditional Expectation and The BLP",
    "section": "4.2 Learning Objectives",
    "text": "4.2 Learning Objectives\nAt the end of this weeks learning, which includes the asynchronous lectures, reading the textbook, this live session, and the homework associated with the concepts, student should be able to\n\nRecognize that the conditional expectation function, the CEF, is a the pure-form, best-possible predictor of a target variable given information about other variables.\nProduce the conditional expectation function as a predictor, given joint densities of random variables.\nAppreciate that the best linear predictor, which is a restriction of predictors to include only those that are linear combinations of variables, can produce reasonable predictions and anticipate that the BLP forms the target of inquiry for regression.",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Conditional Expectation and The BLP</span>"
    ]
  },
  {
    "objectID": "04-conditional-expectation-and-BLP.html#class-announcements",
    "href": "04-conditional-expectation-and-BLP.html#class-announcements",
    "title": "4¬† Conditional Expectation and The BLP",
    "section": "4.3 Class Announcements",
    "text": "4.3 Class Announcements\n\n4.3.1 Test 1 is releasing to you this week.\nThe first test is releasing this week. There are review sessions scheduled for this week, practice tests available, and practice problems available. The format for the test is posted in the course discussion channel. In addition to your test, your instructor will describe your responsibilities that are due next week.",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Conditional Expectation and The BLP</span>"
    ]
  },
  {
    "objectID": "04-conditional-expectation-and-BLP.html#roadmap",
    "href": "04-conditional-expectation-and-BLP.html#roadmap",
    "title": "4¬† Conditional Expectation and The BLP",
    "section": "4.4 Roadmap",
    "text": "4.4 Roadmap\n\n4.4.1 Rearview Mirror\nOver the past three weeks, we have talked about how statisticians approach the world, using probability theory to help them make sense of the missing information in the world. This probability theory allows for very general representations of the world; and, if we had the functions that defined the probability distributions, we could do a lot! But, because we don‚Äôt live in a simulation (probably‚Ä¶) we don‚Äôt have access to the actual functions that describe the probability of events occurring.\nWe can summarize complex random variables in ways that are useful. We can characterize central tendency using the concept of expected value which is the probability weighted ‚Äúaverage‚Äù of the outcomes. While there are lots of other ways to characterize central tendency, expectation has some nice properties, namely, that it minimizes mean squared error and also that it is part of the representation of variance. Variance is a measure of dispersion of a random variable, or ‚Äúhow far away from the center‚Äù outcomes might occur. It is defined as the probability weighted ‚Äúaverage‚Äù of the squared deviation. Finally, covariance is a measure of linear dependency between two random variables, and it has a form that is remarkably similar to variance.\n\n\n4.4.2 This week\n\nWe look at situations with one or more ‚Äúinput‚Äù random variables, and one ‚Äúoutput.‚Äù\nConditional expectation summarizes the output, given values for the inputs.\nThe conditional expectation function (CEF) is a predictor ‚Äì a function that yields a value for the output, give values for the inputs.\nThe best linear predictor (BLP) summarizes a relationship using a line / linear function.\n\n\n\n4.4.3 Coming Attractions\n\nOLS regression is a workhorse of modern statistics, causal analysis, etc\n\nIt is also the basis for many other models in classical stats and machine learning\n\nThe target that OLS estimates is exactly the BLP, which we‚Äôre learning about this week.",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Conditional Expectation and The BLP</span>"
    ]
  },
  {
    "objectID": "04-conditional-expectation-and-BLP.html#conditional-expectation-function-cef",
    "href": "04-conditional-expectation-and-BLP.html#conditional-expectation-function-cef",
    "title": "4¬† Conditional Expectation and The BLP",
    "section": "4.5 Conditional Expectation Function (CEF)",
    "text": "4.5 Conditional Expectation Function (CEF)\nThink back to remember the definition of the expectation of \\(Y\\):\n\\[\n  E[Y] = \\int_{-\\infty}^\\infty y \\cdot f_{Y}(y) dy\n\\]\nWhen you read the right hand side of that expectation operator, what are the ‚Äúingredients‚Äù that are within the integral that produce the expected value? (Hint: there are two.)\nThis week, the async reading and lectures add a new concept, the conditional expectation of \\(Y\\) given \\(X\\):\n\\[\n  E[Y|X=x] =  \\int_{-\\infty}^\\infty y \\cdot f_{Y|X}(y|x) dy\n\\]\nWhen you read the right hand side of that conditional expectation operator, what are the ‚Äúingredients‚Äù that are within the integral that produce the conditional expected value? (Hint: there are two.)\nIn what ways are these ingredients similar and in what ways are the different from the expected value above?\n\n4.5.1 Compare Expectation and Conditional Expectation\n\nThink back by a week: What desirable properties of a predictor does the expectation possess? What makes these properties desirable?\nThink about this week: How, if at all, does the conditional expectation improve on these desirable properties? How does it accomplish this?\n\n\n\n4.5.2 Contrast Expectation and Conditional Expectation\n\nContrast \\(E[Y]\\) and \\(E[Y|X]\\). For example, when you look at how these operators are ‚Äúshaped‚Äù, how are their components similar or different?1\nWhat is \\(E[Y|X]\\) a function of? What are ‚Äúinput‚Äù variables to this function?\nWhat, if anything, is \\(E[E[Y|X]]\\) a function of?",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Conditional Expectation and The BLP</span>"
    ]
  },
  {
    "objectID": "04-conditional-expectation-and-BLP.html#computing-the-cef",
    "href": "04-conditional-expectation-and-BLP.html#computing-the-cef",
    "title": "4¬† Conditional Expectation and The BLP",
    "section": "4.6 Computing the CEF",
    "text": "4.6 Computing the CEF\n\nSuppose that random variables \\(X\\) and \\(Y\\) are jointly continuous, with joint density function given by,\n\n\\[\nf(x,y) =\n  \\begin{cases}\n    2, & 0 \\leq x \\leq 1, 0 \\leq y \\leq x \\\\\n    0, & otherwise\n\\end{cases}\n\\]\nWhat does the joint PDF of this function look like?\n\n\n\n\n\n\n\n\n\n\n4.6.1 Simple Quantities\nTo begin with, let‚Äôs compute the simplest quantities:\n\nWhat is the expectation of \\(X\\)?\nWhat is the expectation of \\(Y\\)?\nHow would you compute the variance of \\(X\\)? (We‚Äôre not going to do it live).\n\n\n\n4.6.2 Conditional Quantities\n\n4.6.2.1 Conditional Expectation\nAnd then, let‚Äôs think about how to compute the conditional quantities.\nCompute the \\(CEF[Y|X]\\). Start by writing down the statement of the conditional expectation function, and then, compute the value.\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\nOnce you have computed the \\(CEF[Y|X]\\), use this function to answer the following questions:\n\nWhat is the conditional expectation of \\(Y\\), given that \\(X=x=0\\)?\nWhat is the conditional expectation of \\(Y\\), given that \\(X=x=0.5\\)?\nWhat is the conditional expectation of \\(X\\), given that \\(Y=y=0.5\\)?\n\n\n\n4.6.2.2 Conditional Variance\n\nWhat is the conditional variance function?2\n\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n\nWhich of the two of these has a lower conditional variances?\n\n\\(V[Y|X=0.25]\\); or,\n\\(V[Y|X=0.75]\\).\n\nHow does \\(V[Y]\\) compare to \\(V[Y|X=1]\\)? Which is larger?\n\n\n\n\n4.6.3 Conditional Expectation",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Conditional Expectation and The BLP</span>"
    ]
  },
  {
    "objectID": "04-conditional-expectation-and-BLP.html#minimizing-the-mse",
    "href": "04-conditional-expectation-and-BLP.html#minimizing-the-mse",
    "title": "4¬† Conditional Expectation and The BLP",
    "section": "4.7 Minimizing the MSE",
    "text": "4.7 Minimizing the MSE\n\n4.7.1 Minimizing MSE\n\nTheorem 2.2.20 states,\n\nThe CEF \\(E[Y|X]\\) is the ‚Äúbest‚Äù predictor of \\(Y\\) given \\(X\\), where ‚Äúbest‚Äù means it has the smallest mean squared error (MSE).\n\nOh yeah? As a breakout group, ride shotgun with us as we prove that the conditional expectation is the function that produces the smallest possible Mean Squared Error.\nSpecifically, you group‚Äôs task is to justify every transition from one line to the next using concepts that we have learned in the course: definitions, theorems, calculus, and algebraic operations.\n\n\n\n4.7.2 The pudding (aka: ‚ÄúWhere the proof is‚Äù)\nWe need to find such function \\(g(X): \\mathbb{R} \\to \\mathbb{R}\\) that gives the smallest mean squared error.\nFirst, let MSE be defined as it is in Definition 2.1.22.\n\nFor a random variable \\(X\\) and constant \\(c \\in \\mathbb{R}\\), the mean squared error of \\(X\\) about \\(c\\) is \\(E[(x-c)^2]\\).\n\nSecond, let us note that since \\(g(X)\\) is just a function that maps onto \\(\\mathbb{R}\\), that for some particular value of \\(X=x\\), \\(g(X)\\) maps onto a constant value.\n\nDeriving a Function to Minimize MSE\n\n\\[\n\\begin{aligned}\n  E[(Y - g(X))^2|X]\n      &= E[Y^2 - 2Yg(X) + g^2(X)|X]                                \\\\\n      &= E[Y^2|X] + E[-2Yg(X)|X] + E[g^2(X)|X]                     \\\\\n      &= E[Y^2|X] - 2g(X)E[Y|X] + g^2(X)E[1|X]                     \\\\\n      &= (E[Y^2|X] - E^2[Y|X]) + (E^2[Y|X] - 2g(X)E[Y|X] + g^2(X)) \\\\\n      &= V[Y|X] + (E^2[Y|X] - 2g(X)E[Y|X] + g^2(X))                \\\\\n      &= V[Y|X] + (E[Y|X] - g(X))^2                                \\\\\n\\end{aligned}\n\\]\nNotice too that we can use the Law of Iterated Expectations to do something useful. (This is a good point to talk about how this theorem works in your breakout groups.)\n\\[\n\\begin{aligned}\n  E[(Y-g(X))^2] &= E\\big[E[(Y-g(X))^2|X]\\big]     \\\\\n    &=E\\big[V[Y|X]+(E[Y|X]-g(X))^2\\big]           \\\\\n    &=E\\big[V[Y|X]\\big]+E\\big[(E[Y|X]-g(X))^2\\big]\\\\\n\\end{aligned}\n\\]\n\n\\(E[V[Y|X]]\\) doesn‚Äôt depend on \\(g\\); and,\n\\(E[(E[Y|X]-g(X))^2] \\geq 0\\).\n\n\\(\\therefore g(X) = E[Y|X]\\) gives the smallest \\(E[(Y-g(X))^2]\\)\n\n\n4.7.3 The Implication\nIf you are choosing some \\(g\\), you can‚Äôt do better than \\(g(x) = E[Y|X=x]\\).\n\n\n4.7.4 Working with a gnarly function and a lot of data.\nThis is risky, because it makes you want data, and we don‚Äôt get it yet. We haven‚Äôt earned our stripes and we‚Äôre still figuring out what works in the perfect world of the population rather than the grimy world of data. ü§´\n\ndat |&gt;  \n  ggplot() + \n  aes(x = x, y = y) + \n  geom_point(alpha=0.25, size=.1)\n\n\n\n\n\n\n\n\n\nl &lt;- loess(y ~ x, data = dat, span = .1)\n# ask for student prediction functions\nmod_lm &lt;- lm(y ~ x, data = dat)\n\npredict.lm(mod_lm, newdata = data.frame(x))[1:10]\n\n        1         2         3         4         5         6         7         8 \n 69.69431  11.49217  57.90516 -38.89537 101.16402  46.07524  31.17777  53.39701 \n        9        10 \n-29.15588 -30.21568 \n\ndat &lt;- dat |&gt; \n  mutate(\n    pred_loess = predict(l, x), \n    pred_lm    = as.numeric(predict(mod_lm))\n    ## fill student prdictions here\n    )\n\ndat |&gt; head()\n\n           x          y pred_loess   pred_lm\n1 7.35431023  46.173460  45.690340  69.69431\n2 3.43506857   3.508006   8.948197  11.49217\n3 6.56044725  34.006288  26.385841  57.90516\n4 0.04205036 -22.280495 -26.316350 -38.89537\n5 9.47343120 152.196999 153.988564 101.16402\n6 5.76383888  13.205788  15.214396  46.07524\n\n\nLet‚Äôs pull in a mean squared error function:\n\nmse &lt;- function(truth, prediction) { \n  mse_ &lt;- mean((truth - prediction)^2)\n  return(mse_)\n  }\n\n\nmse(truth=dat$y, prediction=dat$pred_loess)\n\n[1] 8.964149\n\n\n\ndat |&gt;  \n  ggplot() + \n  aes(x = x, y = y) + \n  geom_point(alpha=0.25, size=.1) + \n  geom_line(aes(x = x, y = pred_loess), color = \"red\") + \n  geom_line(aes(x = x, y = pred_lm), color = \"blue\")\n\n\n\n\n\n\n\n\nWhat would happen if, rather than having this much data, you had much much less?\n\ndat_small &lt;- dat |&gt;  \n  slice_sample(n = 50) |&gt;  \n  add_row(x=5, y=50) |&gt;  \n  arrange(x)\n\nl_small &lt;- dat_small |&gt;  \n  loess(formula = y ~ x, span = .1, data = _)\n\nlm_small &lt;- dat_small |&gt;  \n  lm(formula = y ~ x, data = _)\n\ndat_small &lt;- dat_small |&gt;  \n  mutate(\n    pred_loess_small = predict(l_small), \n    pred_lm_small    = predict(lm_small)\n  )\n\ndat_small |&gt;  \n  ggplot() + \n  geom_point(aes(x = x, y = y)) + \n  geom_line(aes(x = x, y = pred_loess_small)) + \n  geom_line(aes(x = x, y = pred_lm_small))",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Conditional Expectation and The BLP</span>"
    ]
  },
  {
    "objectID": "04-conditional-expectation-and-BLP.html#working-with-the-blp",
    "href": "04-conditional-expectation-and-BLP.html#working-with-the-blp",
    "title": "4¬† Conditional Expectation and The BLP",
    "section": "4.8 Working with the BLP",
    "text": "4.8 Working with the BLP\nWhy Linear?\n\nIn some cases, we might try to estimate the CEF. More commonly, however, we work with linear predictors. Why?\n\n\n\nWe don‚Äôt know joint density function of \\(Y\\). So, it is ‚Äúdifficult‚Äù to derive a suitable CEF.\nTo estimate flexible functions requires considerably more data. Assumptions about distribution (e.g.¬†a linear form) allow you to leverage those assumptions to learn ‚Äòmore‚Äô from the same amount of data.\nOther times, the CEF, even if we could produce an estimate, might be so complex that it isn‚Äôt useful or would be difficult to work with.\nAnd, many times, linear predictors (which might seem trivially simple) actually do a very good job of producing predictions that are ‚Äòclose‚Äô or useful.\n\n\n\n4.8.1 Continuous BLP\n\nRecall the PDF that we worked with earlier to produce the \\(CEF[Y|X]\\).\n\n\\[\nf(x,y) =\n  \\begin{cases}\n    2, & 0 \\leq x \\leq 1, 0 \\leq y \\leq x \\\\\n    0, & otherwise\n\\end{cases}\n\\]\nFind the \\(BLP\\) for \\(Y\\) as a function of \\(X\\). What, if anything, do you notice about this \\(BLP\\) and the \\(CEF\\)?",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Conditional Expectation and The BLP</span>"
    ]
  },
  {
    "objectID": "04-conditional-expectation-and-BLP.html#footnotes",
    "href": "04-conditional-expectation-and-BLP.html#footnotes",
    "title": "4¬† Conditional Expectation and The BLP",
    "section": "",
    "text": "Note, when we say ‚Äúshaped‚Äù here, we‚Äôre referring to the deeper concept of a statistical functional. A statistical functional is a function of a function that maps to a real number. So, if \\(T\\) is the functional that we‚Äôre thinking of, \\(\\mathcal{F}\\) is a family of functions that it might operate on, and \\(\\mathbb{R}\\) is the set of real numbers, a statistical functional is just \\(T: \\mathcal{F} \\rightarrow \\mathbb{R}\\). The Expectation statistical functional, \\(E[X]\\) always has the form \\(\\int x f_{X}(x)dx\\).)‚Ü©Ô∏é\nTake a moment to strategize just a little bit before you get going on this one. There is a way to compute this value that is easier than another way to compute this value.‚Ü©Ô∏é",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Conditional Expectation and The BLP</span>"
    ]
  },
  {
    "objectID": "sampling-and-testing.html",
    "href": "sampling-and-testing.html",
    "title": "Sampling and Testing",
    "section": "",
    "text": "In this section we will introduce the basics of sampling and testing.",
    "crumbs": [
      "Sampling and Testing"
    ]
  },
  {
    "objectID": "05-sampling-theory.html",
    "href": "05-sampling-theory.html",
    "title": "5¬† Learning from Random Samples",
    "section": "",
    "text": "5.1 Goals, Framework, and Learning Objectives",
    "crumbs": [
      "Sampling and Testing",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Learning from Random Samples</span>"
    ]
  },
  {
    "objectID": "05-sampling-theory.html#goals-framework-and-learning-objectives",
    "href": "05-sampling-theory.html#goals-framework-and-learning-objectives",
    "title": "5¬† Learning from Random Samples",
    "section": "",
    "text": "5.1.1 Class Announcements\n\n\nYou‚Äôre done with probability theory. Yay!\nYou‚Äôre also done with your first test. Double Yay!\nWe‚Äôre going to have a second test in a few weeks. Then we‚Äôre done testing for the semester Yay?\n\n\n\n\n5.1.2 Learning Objectives\n\nAt the end of this week, students will be able to\n\nUnderstand what iid sampling is, and evaluate whether the assumption of iid sampling is sufficiently plausible to engage in frequentist modeling.\nAppreciate that with iid sampling, summarizing functions of random variables are, themselves, random variables with probability distributions and values that they obtain.\nRecall the definition of an estimator,\nRecall definition of an estimator, state and understand the desirable properties of estimators, and evaluate whether an estimator possesses those desirable properties.\nDistinguish between the concepts of {expectation & sample mean}, {variance & unbiased sample variance estimator, sampling-based variance in the sample mean}.\n\n\n\n\n5.1.3 Roadmap\n\n5.1.3.1 Where We‚Äôre Going ‚Äì Coming Attractions\n\n\nWe‚Äôre going to start bringing data into our work\nFirst, we‚Äôre going to develop a testing framework that is built on sampling theory and reference distributions: these are the frequentist tests.\nSecond, we‚Äôre going to show that OLS regression is the sample estimator of the BLP. This means that OLS regression produces estimates of the BLP that have known convergence properties.\nThird, we‚Äôre going combine the frequentist testing framework with OLS estimation to produce a full regression testing framework.\n\n\n\n\n5.1.3.2 Where We‚Äôve Been ‚Äì Random Variables and Probability Theory\nStatisticians create a model (also known as the population model) to represent the world. This model exists as joint probability densities that govern the probabilities that any series of events occurs at the same time. This joint probability of outcomes can be summarized and described with lower-dimensional summaries like the expectation, variance, covariance. While the expectation is a summary that contains information on about one marginal distribution (i.e.¬†the outcome we are interested in) we can produce predictive models that update, or condition the expectation based on other random variables. This summary, the conditional expectation is the best possible (measured in terms of minimizing mean squared error) predictor of an outcome. We might simplify this conditional expectation predictor in many ways; the most common is to simplify to the point that the predictor is constrained to be a line or plane. This is known as the Best Linear Predictor.\n\n\n5.1.3.3 Where we Are\n\nWe want to fit models ‚Äì use data to set their parameter values.\nA sample is a set of random variables\nSample statistics are functions of a sample, and they are random variables\nUnder iid and other assumptions, we get useful properties:\n\nStatistics may be consistent estimators for population parameters\nThe distribution of sample statistics may be asymptotically normal",
    "crumbs": [
      "Sampling and Testing",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Learning from Random Samples</span>"
    ]
  },
  {
    "objectID": "05-sampling-theory.html#key-terms-and-assumptions",
    "href": "05-sampling-theory.html#key-terms-and-assumptions",
    "title": "5¬† Learning from Random Samples",
    "section": "5.2 Key Terms and Assumptions",
    "text": "5.2 Key Terms and Assumptions\n\n5.2.1 IID\nWe use an abbreviation for the sampling process that under girds our frequentist statistics. That abbreviation, IID, while short, contains two powerful requirements of our data sampling process.\n\nIID sampling is:\n\nIndependent. The first I in the abbreviation, this independence requirement is similar to the independence concept that we‚Äôve used in the probability theory section of the course. When samples are independent, the result of any one sample is not informative about the value of any of the other samples.\nIdentically Distributed. The ID in the abbreviation, this requirement means that all samples are drawn from the same distribution.\n\n\nIt might be tempting to imagine that IID samples are just ‚Äúrandom samples‚Äù, but it is worth noting that IID sampling has the two specific requirements noted above, and that these requirements are more stringent than a ‚Äúrandomness‚Äù criteria.\nWhen we are thinking about IID samples, and evaluating whether the sample do, in point of fact, meet both of the requirements, it is crucial to make an explicit statement about the reference population that is under consideration.\n\nFor example, suppose that you were interested in learning about life-satisfaction and your reference population are the peoples who live in the United States. Further, suppose that you decide to produce an estimate of this using a sample drawn from UC Berkeley undergraduate students during RRR week? There are several flaws in this design:\n\n\nThere is a key research design issue: a sample drawn from Berkeley undergraduates is going to be essentially uninformative of a US resident reference population!\nThere is a key statistical issue: the population of Berkeley undergraduates are not really an independent sample from the entire US resident reference population. Once you learn the age of someone from the Berkeley student population, you can make an conditional guess about the age of the next sample that will be closer than was possible before the first sample. The same goes for life-satisfaction: When you learn about the life-satisfaction from the first undergrad (who is miserable because they have their Stat 140 final coming up) while they are studying for their finals) you can make a conditionally better guess about the satisfaction of the next undergrad.\n\nNotice that these violations of the IID requirements only arise because our reference population is the US resident population. If, instead, the reference population were ‚ÄúBerkeley undergrads‚Äù then the sampling process would satisfy the requirements of an IID process.\n\n\nHow, or why, can a change in the reference population make an identical sampling process move from one that we can consider IID to one that we cannot consider IID?\n\n\n\n5.2.1.1 It this IID?\n\nFor each of the following scenarios, is the IID assumption plausible?\n\nCall a random phone number. If someone answers, interview all persons in the household. Repeat until you have data on 100 people.\nCall a random phone number, interview the person if they are over 30. Repeat until you have data on 100 people.\nRecord year-to-date price change for 20 largest car manufacturers.\nMeasure net exports per GDP for all 195 countries recognized by the UN.",
    "crumbs": [
      "Sampling and Testing",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Learning from Random Samples</span>"
    ]
  },
  {
    "objectID": "05-sampling-theory.html#estimators",
    "href": "05-sampling-theory.html#estimators",
    "title": "5¬† Learning from Random Samples",
    "section": "5.3 Estimators",
    "text": "5.3 Estimators\nIn our presentation of this week‚Äôs materials, we choose to switch the presentation of statistics and estimators, electing to discuss properties that we would like estimators to posses, before we actually introduce any specific estimators.\n\n5.3.1 Three properties of estimators\n\nWhat are the three desirable properties of estimators?\n\n\n\n\n\nIs one these properties more important than another? If you had to force-rank these properties in terms of their importance, which is the most, and which the least important? Why?",
    "crumbs": [
      "Sampling and Testing",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Learning from Random Samples</span>"
    ]
  },
  {
    "objectID": "05-sampling-theory.html#estimator-property-biased-or-unbiased",
    "href": "05-sampling-theory.html#estimator-property-biased-or-unbiased",
    "title": "5¬† Learning from Random Samples",
    "section": "5.4 Estimator Property: Biased or Unbiased?",
    "text": "5.4 Estimator Property: Biased or Unbiased?\n\n\nFirst, for a general case: Suppose that you have chosen some particular estimator, \\(\\hat{\\theta}\\) to estimate some characteristic, \\(\\theta\\) of a random variable. How do you know if this estimator is unbiased?\nSecond, for a specific case: Define the ‚Äúsample average‚Äù to be the following: \\(\\frac{1}{n}\\sum_{i=1}^{N} x_{i}\\). Prove that this sample average estimator is an unbiased estimator of \\(E[X]\\).\nThird (easier), for a different specific case: Define the ‚Äúsmample smaverage‚Äù to be the following \\(\\frac{1}{n^2}\\sum_{i=1}^{N} x_{i}\\). Prove that the smample smaverage is a biased estimator of \\(E[X]\\).\nFourth (harder): Define the geometric mean to be \\[\\left(\\prod_{i=1}^{N}x_{i}\\right)^{\\frac{1}{N}}\\]. Prove that the geometric mean is a biased estimator of \\(E[X]\\).\n\n\n\n5.4.1 Is it unbiased, with data?\nSuppose that you‚Äôre getting data from the following process:\n\nrandom_distribution &lt;- function(number_samples) { \n  \n  d1 &lt;- c(1.0, 2.0)\n  d2 &lt;- c(1.1, 2.1)\n  d3 &lt;- c(1.5, 2.5)\n\n  distribution_chooser = sample(x=1:3, size=1)\n  \n  if(distribution_chooser == 1) { \n    x_ &lt;- runif(n=number_samples, min=d1[1], max=d1[2])  \n  } else if(distribution_chooser == 2) { \n    x_ &lt;- runif(n=number_samples, min=d2[1], max=d2[2]) \n  } else if(distribution_chooser == 3) { \n    x_ &lt;- runif(n=number_samples, min=d3[1], max=d3[2])\n  }\n  return(x_)\n}\n\nrandom_distribution(number_samples=10)\n\n [1] 1.427992 1.564263 1.453100 1.517203 1.289734 1.502558 1.777703 1.992369\n [9] 1.877814 1.154499\n\n\n\nmean(random_distribution(number_samples=10000))\n\n[1] 1.596825\n\n\nNotice that, there are two forms of inherent uncertainty in this function:\n\nThere is uncertainty about the distribution that we are getting draws from; and,\nWithin a distribution, we‚Äôre getting draws at random from a population distribution.\n\nThis class of function, the r* functions, are the implementation of random generative processes within the R language. Look into ?distributions as a class to see more about this process.\nSuppose that you chose to use the same sample average estimator as a means of producing an estimate of the population expected value, \\(E[X]\\). Suppose that you get the following draws:\n\ndraws &lt;- random_distribution(number_samples=10)\ndraws\n\n [1] 1.872052 2.179062 1.684139 1.569817 2.130975 2.282554 2.284383 1.540814\n [9] 1.922027 2.192010\n\n\n\nmean(draws)\n\n[1] 1.965783\n\n\n\nIs this sample average an unbiased estimator for the population expected value? How do you know?",
    "crumbs": [
      "Sampling and Testing",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Learning from Random Samples</span>"
    ]
  },
  {
    "objectID": "05-sampling-theory.html#estimator-property-consistency",
    "href": "05-sampling-theory.html#estimator-property-consistency",
    "title": "5¬† Learning from Random Samples",
    "section": "5.5 Estimator Property: Consistency",
    "text": "5.5 Estimator Property: Consistency\nFoundations makes another of their jokes when they write, on page 105,\n\n‚ÄúConsistency is a simple notion: if we had enough data, the probability that our estimate \\(\\hat{\\theta}\\) would be far from the truth, \\(\\theta\\), would be small.‚Äù\n\n\nHow do we determine if a particular estimator, \\(\\hat{\\theta}\\) is a consistent estimator for our parameter of interest?\nThere are at least two ways:\n\nThe estimator is unbiased, and has a sampling variance that decreases as we add data; or,\nWe can use Chebyshev‚Äôs to place a bound on the estimator, showing that as we add data, the estimator converges in probability to \\(\\theta\\).\n\n\nThe first notion of convergence requires an understanding of sampling variance:\nThe sampling variance of an estimator is a statement about how much dispersion due to random sampling, is present in the estimator. We defined the variance of a random variable to be \\(E\\left[(X - E[X])^{2}\\right]\\), The sampling variance uses this same definition, but we work with it slightly differently when we are considering sampling variance.\nIn particular, when we are considering sampling variance, we do not typically got as far as actually computing the variance of the underlying random variable? Why? Because, if we‚Äôre working in a sampling scenario, it is unlikely that we have access to the underlying function that governs the PDF of the random variable.\nInstead, we typically start from a statement of the estimator that is under consideration, and apply the variance operator against that estimator. Consider, for example, forming a statement about the sampling variance of the sample average.\nLet \\(\\overline{X} \\equiv\\) ‚Äúsample average‚Äù \\(\\equiv \\frac{1}{n}\\sum_{i=1}^{n}X_{i}\\) be the normal form of the sample average.\nEarlier, we proved that \\(\\overline{X}\\) is an unbiased estimator of \\(E[X]\\).\n\nWhat is the sampling variance of the sample average?\n\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n\nUsing the statement that you have just produced, would you say that the sample average is a consistent estimator for the population expectation of a random variable?",
    "crumbs": [
      "Sampling and Testing",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Learning from Random Samples</span>"
    ]
  },
  {
    "objectID": "05-sampling-theory.html#understanding-sampling-distributions",
    "href": "05-sampling-theory.html#understanding-sampling-distributions",
    "title": "5¬† Learning from Random Samples",
    "section": "5.6 Understanding Sampling Distributions",
    "text": "5.6 Understanding Sampling Distributions\nHow do sampling distributions change as we add data to them? This is going to both motivate convergence, and also play forward into the Central Limit Theorem. Let‚Äôs work through an example that begins with a case that we can think through and draw ourselves. Once we feel pretty good about the very small sample, then we will rely on R to do the work when we expand the example beyond what we can draw ourselves.\n\nSuppose that \\(X\\) is a Bernoulli random variable representing an unfair coin. Suppose that the coin has a 70% chance of coming up heads: \\(P(X=1) = 0.7\\).\n\nTo begin, suppose that you take that coin, and you toss it two times: you have an iid sample of size 2, \\((X_1,X_2)\\).\nWhat is the sampling distribution of the sample average, of this sample of size two?\nOn the axes below, draw the probability distribution of \\(\\overline X = \\frac{X_1+X_2}{2}\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat if you took four samples? What would the sampling distribution of \\(\\overline{X}\\) look like? Draw this onto the axis above.\nExplain the difference between a population distribution and the sampling distribution of a statistic.\nWhy do we want to know things about the sampling distribution of a statistic?\n\n\nWe are going to write a function that, essentially, just wraps a built-in function with a new name and new function arguments. This is, generally, bad coding practice ‚Äì because it is changing the default lexicon than a collaborator needs to be aware of ‚Äì but it is useful for teaching purposes here.\n\nThe number_of_simulations argument to the toss_coin function basically just adjusts the precision of our simulation study.\nLet‚Äôs set, and keep this at \\(1000\\) simulations. But, if you‚Äôre curious, you could set this to be \\(5\\), or \\(10\\) and evaluate what happens.\n\n\ntoss_coin &lt;- function(\n    number_of_simulations=1000, \n    number_of_tosses=2, \n    prob_heads=0.7) { \n  \n  ## number of simulations is just how many times we want to re-run the experiment\n  ## number of tosses is the number of coins we're going to toss.\n  number_of_heads &lt;- rbinom(n=number_of_simulations, size=number_of_tosses, prob=prob_heads)\n  sample_average  &lt;- number_of_heads / number_of_tosses\n  return(sample_average)\n}\n\ntoss_coin(number_of_simulations=10, number_of_tosses=2, prob_heads=0.7)\n\n [1] 1.0 0.5 1.0 0.5 0.5 1.0 1.0 1.0 0.5 0.5\n\n\n\nncoins &lt;- 10\n\ncoin_result_005 &lt;- toss_coin(\n  number_of_simulations = 10000,\n  number_of_tosses = ncoins, \n  prob_heads = 0.005\n  )\n\ncoin_result_05 &lt;- toss_coin(\n  number_of_simulations = 10000,\n  number_of_tosses = ncoins, \n  prob_heads = 0.5\n  )\n\nplot_005 &lt;- ggplot() + \n    aes(x=coin_result_005) + \n    geom_histogram(bins=50) + \n    geom_vline(xintercept=0.005, color='#003262', linewidth=2)\n\nplot_05 &lt;- ggplot() + \n    aes(x=coin_result_05) + \n    geom_histogram(bins=50) + \n    geom_vline(xintercept=0.5, color='#003262', linewidth=2)\n\nplot_005 / \n  plot_05\n\n\n\n\n\n\n\n\n\nIn the plot that you have drawn above, pick some value, \\(\\epsilon\\) that is the distance away from the true expected value of this distribution.\n\nWhat proportion of the sampling distribution is further away than \\(E[X] \\pm \\epsilon\\)?\nWhen we toss the coin only two times, we can quickly draw out the distribution of \\(\\overline{X}\\), and can form a statement about the \\(P(E[X] - \\epsilon \\leq  \\overline{X} \\leq E[X] + \\epsilon)\\).\nWhat if we toss the coin ten times? We can still use the IID nature of the coin to figure out the true \\(P(\\overline{X} = 0), P(\\overline{X} = 1), \\dots, P(\\overline{X} = 10)\\), but it is going to start to take some time. This is where we rely on the simulation to start speeding up our learning.\nAs we toss more and more coins, \\(\\overline X_{(100)} \\rightarrow \\overline X_{(10000)}\\) what will the value of \\(\\overline X\\) get closer to? What law generates this, and why does this law generate this result?",
    "crumbs": [
      "Sampling and Testing",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Learning from Random Samples</span>"
    ]
  },
  {
    "objectID": "05-sampling-theory.html#write-code-to-demo-the-central-limit-theorem-clt",
    "href": "05-sampling-theory.html#write-code-to-demo-the-central-limit-theorem-clt",
    "title": "5¬† Learning from Random Samples",
    "section": "5.7 Write Code to Demo the Central Limit Theorem (CLT)",
    "text": "5.7 Write Code to Demo the Central Limit Theorem (CLT)\nWhen you were reading for this week, did you sense the palpable joy of the authors when they were writing about the central limit theorem?\n\nWe now preset what is often considered the most profound and important theorem in statistics.\n\nWow. What excitement.\nOn its own, the result that across a broad range of generative functions the distribution of sample averages converges in distribution to follow a normal distribution would be a statistical curiosity. Along the lines of ‚Äúdid you know that dinosaurs might have had feathers,‚Äù or, ‚Äúavocado trees reproduce using ‚Äôprotogynous dichogamy‚Äù. While these factoids might be useful on your quiz-bowl team, they don‚Äôt get us very far down the line as practicing data scientists.\nHowever, there is a very useful consequence of this convergence in distribution that we will explore in detail over the coming two weeks: because so many distributions produce sample averages that converge in distribution to a normal distribution, we can put together a testing framework for sample averages that works for an agnostic set of random variables. Wait for that next week, but know that there‚Äôs a reason that we‚Äôre as excited about this statement as we are.\n\n5.7.1 Part 1\nTo begin with, we will use fair coins that have an equal probability landing heads and tails.\n\n\nModify the function argument below so that it conduct one simulations, and in each simulation tosses ten coins, each with an equal probability of landing heads and tails. Look into the toss_coin function: is there a point that this function is producing a sample average? If so, where?\nSave values from the toss_coin function into an object, called sample_mean.\n\n\n\n# toss_coin()\n\n\nThe sample mean is a random variable ‚Äì it is a function that maps from a random generative process‚Äô sample space (the number of heads shown on dice) to the real numbers. To try to make this clear, visualize a larger number of simulations on the toss_coin function. That is, increase the number_of_simulations to be 10, or 100. and plot a histogram of the results. This is quite similar to what we have done earlier.\n\n\n# toss_coin()\n\n\nIf you replicate the simulation with ten coins enough times, will the distribution ever look normal? Why or why not?\n\n\n\n5.7.2 Part 2\nFor this part, we‚Äôll continue to study a fair coin.\n\nWhat happens if you change the number of coins that you‚Äôre tossing? Here, set number_of_simulations=1000, and examine what happens if you alter the number of coins that you‚Äôre tossing? Is there a point where this distribution starts to ‚Äúlook normal‚Äù to you? (Later in the semester, we‚Äôll formalize a test for this ‚Äúlooks normal‚Äù concept).\n\n\n\n5.7.3 Part 3\n\nWhat would happen if the coin was very, very unfair? For this part, study a coin that has a prob_heads=0.01. This is an example of a highly skewed random variable.\nStart your study here tossing three coins, number_of_coins=3. What does this distribution look like?\n\n\nWhat happens as you increase the number of coins that you‚Äôre tossing? Is there a point that the distribution starts to look normal?\n\n\n\n5.7.4 Discussion Questions About the CLT\n\n\nHow does the skewness of the population distribution affect the applicability of the Central Limit Theorem? What lesson can you take for your practice of statistics?\nName a variable you would be interested in measuring that has a substantially skewed distribution.\nOne definition of a heavy tailed distribution is one with infinite variance. For example, you can use the rcauchy command in R to take draws from a Cauchy distribution, which has heavy tails. Do you think a ‚Äúheavy tails‚Äù distribution will follow the CLT? What leads you to this intuition?",
    "crumbs": [
      "Sampling and Testing",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Learning from Random Samples</span>"
    ]
  },
  {
    "objectID": "05-sampling-theory.html#errors-with-standard-errors",
    "href": "05-sampling-theory.html#errors-with-standard-errors",
    "title": "5¬† Learning from Random Samples",
    "section": "5.8 Errors with Standard Errors",
    "text": "5.8 Errors with Standard Errors\nTalking about variance and sampling variance is hard, because the terms sound very similar, but have important distinctions in what they mean. For example, the ‚Äúvariance‚Äù is not the same as the ‚Äúunbiased sample variance‚Äù which is also not the same as the ‚Äúsampling variance of the sample average‚Äù. :sob:\nStandard errors are a statement about the sampling variance of the sample average. But, related to this concept are the ideas of the Population Variance, the Plug-In Estimator for the Sample Variance, the Unbiased Sample Variance, and, finally, the Sampling Variance of the Sample Average (i.e the Standard Error).\nHow are each of these concepts related to one another, and how can we keep them all straight? As a group, fill out the following columns?\n\nFor the Estimator Properties column, here we‚Äôre considering, principally biased/unbiased and consistent/inconsistent.\n\n\n\n\n\n\n\n\n\n\n\n\nPopulation Concept\nPop Notation\nSample Estimator\nSample Notation\nEstimator Properties\nSampling Variance of Sample Estimator\n\n\n\n\nExpected Value\n\n\n\n\n\n\n\nPopulation Variance\n\n\n\n\n\n\n\nPopulation Covariance\n\n\n\n\n\n\n\nCEF\n\n\n\n\n\n\n\nBLP",
    "crumbs": [
      "Sampling and Testing",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Learning from Random Samples</span>"
    ]
  },
  {
    "objectID": "06-hypothesis-testing.html",
    "href": "06-hypothesis-testing.html",
    "title": "6¬† Hypothesis Testing",
    "section": "",
    "text": "What is Frequentist testing doing?\nThis testing framework works on samples of data, and applies estimators to produce estimates of population parameters that are fundamentally unknown and unknowable. Despite this unknown and unknowable population target, with some carefully written down estimators we can rely on the convergence characteristics of some estimators to produce useful, reliable results.\nWe begin with the one-sample t-test. The one-sample t-test relies on the sample average as an estimator of a population expectation. In doing so, it relies on the effectiveness of the Weak Law of Large Numbers and the Central Limit Theorem to guarantee that the estimator that converges in probability to the population expectation, while also converging in distribution to a Gaussian distribution.\nThese two convergence concepts permit a data scientist to make several inferences based on data:\nThis framework begins a exceedingly important task that we must understand, and undertake when we are working as data scientists: Producing our best-estimate, communicating how we arrived at that estimate, what (if any) guarantees that estimate provides, and crucially all limitations of our estimate.",
    "crumbs": [
      "Sampling and Testing",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "06-hypothesis-testing.html#learning-objectives",
    "href": "06-hypothesis-testing.html#learning-objectives",
    "title": "6¬† Hypothesis Testing",
    "section": "6.1 Learning Objectives",
    "text": "6.1 Learning Objectives\n\nUnderstand the connection between random variables, sampling, and statistical tests.\nApply the Frequentist testing framework in a simple test ‚Äì the one-sample t-test.\nAnticipate that every additional Frequentist test is a closely related variant of this test.",
    "crumbs": [
      "Sampling and Testing",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "06-hypothesis-testing.html#class-announcements",
    "href": "06-hypothesis-testing.html#class-announcements",
    "title": "6¬† Hypothesis Testing",
    "section": "6.2 Class Announcements",
    "text": "6.2 Class Announcements\n\nYou will be taking your second test this week. The test follows the same format as the first test, which we will discuss in live session. The test will cover:\n\nUnit 4: Conditional Expectation and the Best Linear Predictor; and,\nUnit 5: Learning from Random Samples.\n\n\n\nLike the first test, our goal is to communicate to you what concepts we think are important, and then to test those concepts directly, and fairly. The purpose of the test is to give you an incentive to review what you have learned through probability theory, and then to demonstrate that you can produce work based on that knowledge.\nThere is another practice test on Gradescope, and in the GitHub repository.\n\n\nIn rosier news, we‚Äôre moving out of the only pencil and paper section of this course, and bringing what we have learned out into the dirty world of data. This means a few things:\n\n\nIf you haven‚Äôt yet worked through the R Bridge Course that is available to you, working on this bridge course will be useful for you (after you complete your test). The goal of the course is to get you up and running with reasonably successful code and workflows for the data-based portion of the course.\n\n\nWe will assign teams, and begin our work on Lab 1 in Live Session next week. This is a two-week, group lab that you will work on with three total team-mates. The lab will cover some of the fundamentals of hypothesis tests,",
    "crumbs": [
      "Sampling and Testing",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "06-hypothesis-testing.html#roadmap",
    "href": "06-hypothesis-testing.html#roadmap",
    "title": "6¬† Hypothesis Testing",
    "section": "6.3 Roadmap",
    "text": "6.3 Roadmap\nLooking Backwards\n\nStatisticians create a model to represent the world\nWe saw examples of estimators, which approximate model parameters we‚Äôre interested in.\nBy itself, an estimate isn‚Äôt much good; we need to capture the uncertainty in the estimate.\nWe‚Äôve seen two ways to express uncertainty in an estimator: standard errors and confidence intervals.\n\nToday\n\nWe introduce hypothesis testing\n\nA hypothesis test also captures uncertainty, but in relation to a specific hypothesis.\n\n\nLooking Ahead\n\nWe‚Äôll build on the one-sample t-test, to introduce several other statistical tests.\nWe‚Äôll see how to choose a test from different alternatives, with an eye on meeting the required assumptions, and maximizing power.",
    "crumbs": [
      "Sampling and Testing",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "06-hypothesis-testing.html#what-does-a-hypothesis-test-do",
    "href": "06-hypothesis-testing.html#what-does-a-hypothesis-test-do",
    "title": "6¬† Hypothesis Testing",
    "section": "6.4 What does a hypothesis test do?",
    "text": "6.4 What does a hypothesis test do?\n\nWhat are the two possible outcomes of a hypothesis test?\nWhat are the four-possible combinations of (a) hypothesis test result; and (b) state of the world?\nDoes a hypothesis test always have to report a result that is consistent with the state of the world? What does it mean if it does, and what does it mean if it does not.\nWhat if you made up your own testing framework, called the {Your Last Name‚Äôs} Groundhog test. Which is literally a groundhog looking to see its shadow. Because you made the test, suppose that you know that it is totally random whether a groundhog sees its shadow. How useful would this test be at separating states of the world?\n\nWhat guarantee do you get if you follow the decision rules properly?\nWhy do we standardize the mean to create a test statistic?\n\n\\[\n  t = \\frac{ \\overline{X}_n - \\mu}{\\sqrt{\\frac{s^2}{n}}}\n\\]",
    "crumbs": [
      "Sampling and Testing",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "06-hypothesis-testing.html#madlib-prompt",
    "href": "06-hypothesis-testing.html#madlib-prompt",
    "title": "6¬† Hypothesis Testing",
    "section": "6.5 Madlib prompt",
    "text": "6.5 Madlib prompt\n\ntone_of_voice     &lt;- ''\nmode_of_speech    &lt;- ''\nsuperlative       &lt;- ''\nscore_on_test     &lt;- 'percent' # should end with percent\nname_of_classmate &lt;- ''\nemotion           &lt;- ''\neating_verb       &lt;- '' #slurp\nvessel            &lt;- '' \nthing_found_in_compost &lt;- ''",
    "crumbs": [
      "Sampling and Testing",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "06-hypothesis-testing.html#madlib-completed",
    "href": "06-hypothesis-testing.html#madlib-completed",
    "title": "6¬† Hypothesis Testing",
    "section": "6.6 Madlib completed",
    "text": "6.6 Madlib completed\nSuppose that a classmate comes to you, and, in a voice , ‚ÄúHey, I‚Äôve got something that is for statistics test preperation. All you‚Äôve got to do to get percent on Test 2 and make is to this of .\nYou‚Äôre skeptical, but also curious because that last test was tough. Good.",
    "crumbs": [
      "Sampling and Testing",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "06-hypothesis-testing.html#accepting-the-null",
    "href": "06-hypothesis-testing.html#accepting-the-null",
    "title": "6¬† Hypothesis Testing",
    "section": "6.7 ‚ÄúAccepting the Null‚Äù",
    "text": "6.7 ‚ÄúAccepting the Null‚Äù\n(For the purposes of this class, and while you‚Äôre talking about testing after the class: the preferred language is to either (a) Reject the null hypothesis, or (b) Fail to reject the null hypothesis.\nAcknowledging that we‚Äôre only 40% of the way through the course, you decide to hire a hungry, underpaid PhD student the School to conduct the experiment to evaluate this claim. They report back, no details about the test, but they do tell you, ‚ÄúWe‚Äôre sure there‚Äôs no effect of .‚Äù\n\nDo you believe them?\nWhat, if any, reasons can you imagine not to believe this conclusion?",
    "crumbs": [
      "Sampling and Testing",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "06-hypothesis-testing.html#manually-computing-a-t-test",
    "href": "06-hypothesis-testing.html#manually-computing-a-t-test",
    "title": "6¬† Hypothesis Testing",
    "section": "6.8 Manually Computing a t-Test",
    "text": "6.8 Manually Computing a t-Test\nIn a warehouse full of power packs labeled as 12 volts we randomly measure the voltage of 7. Here is the data:\n\nvoltage &lt;- c(11.77, 11.90, 11.64, 11.84, 12.13, 11.99,  11.77)\nvoltage\n\n[1] 11.77 11.90 11.64 11.84 12.13 11.99 11.77\n\n\n\nFind the mean and the standard deviation.\n\n\nsample_mean &lt;- mean(voltage)\nsample_sd   &lt;- sd(voltage)\nn           &lt;- length(voltage)\n\ntest_statistic &lt;- (sample_mean - 12) / (sample_sd / sqrt(n))\ntest_statistic\n\n[1] -2.247806\n\n\n\nUsing qt(), compute the t critical value for a hypothesis test for this sample.\n\n\nqt(0.025, df=n-1)\n\n[1] -2.446912\n\n\n\nDefine a test statistic, \\(t\\), for testing whether the population mean is 12.\n\n\ntest_statistic\n\n[1] -2.247806\n\n\n\nCalculate the p-value using the t statistic.\n\n\npt(test_statistic, df=n-1)\n\n[1] 0.03281943\n\n\n\nShould you reject the null? Argue this in two different ways. (Following convention, set \\(\\alpha = .05\\).)\n\n\ntest_stat_function &lt;- function(data, null_hypothesis) { \n  sample_mean &lt;- mean(data)\n  sample_sd   &lt;- sd(data)\n  n           &lt;- length(data)\n  \n  test_statistic &lt;- (sample_mean - null_hypothesis) / (sample_sd / sqrt(n))\n  return(test_statistic)\n}\n\n\ntest_stat_function(data=voltage, null_hypothesis=12) |&gt; \n  pt(df=length(voltage)-1) * 2\n\n[1] 0.06563885\n\n\n\nt.test(\n  x           = voltage, \n  alternative = 'two.sided', \n  mu          = 12)\n\n\n    One Sample t-test\n\ndata:  voltage\nt = -2.2478, df = 6, p-value = 0.06564\nalternative hypothesis: true mean is not equal to 12\n95 percent confidence interval:\n 11.71357 12.01215\nsample estimates:\nmean of x \n 11.86286 \n\n\n\nSuppose you were to use a normal distribution instead of a t-distribution to test your hypothesis. What would your p-value be for the z-test?\n\n\nWithout actually computing it, say whether a 95% confidence interval for the mean would include 12 volts.\nCompute a 95% confidence interval for the mean.",
    "crumbs": [
      "Sampling and Testing",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "06-hypothesis-testing.html#falling-ill-the-general-form-of-a-hypothesis-test",
    "href": "06-hypothesis-testing.html#falling-ill-the-general-form-of-a-hypothesis-test",
    "title": "6¬† Hypothesis Testing",
    "section": "6.9 Falling Ill (The General Form of a Hypothesis Test)",
    "text": "6.9 Falling Ill (The General Form of a Hypothesis Test)\nIn the async content for the week, we‚Äôre really, really clear that we‚Äôre only working with the t-distribution. But, the general ‚Äúform‚Äù of a frequentist hypothesis test is always the same: produce a test statistic; produce a distribution of that test statistic if the null hypothesis were true; then compare the two. Let‚Äôs stretch this application a little bit.\n\nThere is a theory that upcoming tests cause students to fall ill.¬†We have been collecting wellness data from our students for several years (not really‚Ä¶) and we have found the following distribution of illnesses (Notice that this does not tell you anything about how many students we have enrolled over the years):\n\n20 students have reported being ill in the week before Test 2\n10 students have reported being ill in the week after Test 2\n\nThink of wellness/illness as a dichotomous statement.\n\n\nState an appropriate null hypothesis. After you have stated this null hypothesis, can you think about (or, even better) can you produce a distribution of the probability of {0, 1, 2, 3, ‚Ä¶ 30} of the illnesses reported before the test?\n\n\nnull_distribution &lt;- dbinom(0:30, 30, prob = 0.5)\n\nggplot() + \n  aes(x=0:30, y=null_distribution) + \n  geom_col()\n\n\n\n\n\n\n\n\n\nState a rejection criteria. What occurrence in the data would cause you do doubt the plausibility of your null hypothesis?\nWhat do you conclude? Given the data that is presented to you and the null hypothesis, what do you conclude?",
    "crumbs": [
      "Sampling and Testing",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "06-hypothesis-testing.html#data-exercise",
    "href": "06-hypothesis-testing.html#data-exercise",
    "title": "6¬† Hypothesis Testing",
    "section": "6.10 Data Exercise",
    "text": "6.10 Data Exercise\nt-Test Micro Cheat Sheet\nIn order for a t-test to produce valid results, a set of conditions must be satisfied. While the literature refers to these as assumptions, you might do better to refer to these for yourselves as requirements. Meaning, if these requirements for the data generating process are not satisfied, the test does not produce results that hold any statistical guarantees.\n\nMetric variable: The data needs to be numeric\nIID: The data needs to be sampled using an independent, identically distributed sampling process.\nWell-behaved: The data need to demonstate no major deviations from normality, considering sample size\n\nTesting the Home Team Advantage\nThe file ./data/home_team.csv contains data on college football games. The data is provided by Wooldridge and was collected by Paul Anderson, an MSU economics major, for a term project. Football records and scores are from 1993 football season.\n\nhome_team &lt;- read.csv('./data/home_team.csv') |&gt; \n  select(dscore, dinstt, doutstt) |&gt; \n  rename(\n    score_diff               = dscore, \n    in_state_tuition_diff    = dinstt, \n    out_state_tuition_diff   = doutstt\n  )\n\nglimpse(home_team, width = 80)\n\nRows: 30\nColumns: 3\n$ score_diff             &lt;int&gt; 10, -14, 23, 8, -12, 7, -21, -5, -3, -32, 9, 1,‚Ä¶\n$ in_state_tuition_diff  &lt;int&gt; -409, NA, -654, -222, -10, 494, 2, 96, 223, -20‚Ä¶\n$ out_state_tuition_diff &lt;int&gt; -4679, -66, -637, 456, 208, 17, 2, -333, 2526, ‚Ä¶\n\n\nWe are especially interested in the variable, score_diff, which represents the score differential, home team score - visiting team score. We would like to test whether a home team really has an advantage over the visiting team.\n\nThe instructor will assign you to one of two teams. Team 1 will argue that the t-test is appropriate to this scenario. Team 2 will argue that the t-test is invalid. Take a few minutes to examine the data, then formulate your best argument.\n\n\nShould you perform a one-tailed test or a two-tailed test? What is the strongest argument for your answer?\n\n\n## I'm going two-tailed. \n## H0 : No effect of being home or away\n## HA : There IS some effect. \n\n\nExecute the t-test and interpret every component of the output.\n\n\nt.test(x=home_team$score_diff, mu=0, alternative = 'two.sided')\n\n\n    One Sample t-test\n\ndata:  home_team$score_diff\nt = -0.30781, df = 29, p-value = 0.7604\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -8.408919  6.208919\nsample estimates:\nmean of x \n     -1.1 \n\nres &lt;- NA\nfor(i in 1:10000) {\n  res[i] &lt;- mean(rnorm(n=7, sd=sd(home_team$score_diff)))\n}\n\nggplot() + \n  aes(x=res) + \n  geom_density() + \n  geom_vline(xintercept=mean(home_team$score_diff))\n\n\n\n\n\n\n\nhome_team |&gt;  \n  ggplot() + \n  aes(x=abs(score_diff)) + \n  geom_density()\n\n\n\n\n\n\n\n\n\nmean(home_team$score_diff)\n\n[1] -1.1\n\nmean((res &lt; mean(home_team$score_diff))) + mean(res &gt; abs(mean(home_team$score_diff)))\n\n[1] 0.8811\n\n\n\nBased on your output, suggest a different hypothesis that would have led to a different test result. Try executing the test to confirm that you are correct.",
    "crumbs": [
      "Sampling and Testing",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "06-hypothesis-testing.html#assumptions-behind-the-t-test",
    "href": "06-hypothesis-testing.html#assumptions-behind-the-t-test",
    "title": "6¬† Hypothesis Testing",
    "section": "6.11 Assumptions Behind the t-test",
    "text": "6.11 Assumptions Behind the t-test\nFor the following scenarios, what is the strongest argument against the validity of a t-test?\n\nYou have a sample of 50 CEO salaries, and you want to know whether the mean salary is greater than $1 million.\nA nonprofit organization measures the percentage of students that pass an 8th grade reading test in 40 neighboring California counties. You are interested in whether the percentage of students that pass in California is over 80%\nYou have survey data in which respondents assess their own opinion of corgis, with options ranging from ‚Äú1 - extreme disgust‚Äù to ‚Äú5 - affection so intense it threatens my career.‚Äù You want to know whether people on the average like corgis more than 3, representing neutrality.",
    "crumbs": [
      "Sampling and Testing",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "07-comparing-two-groups.html",
    "href": "07-comparing-two-groups.html",
    "title": "7¬† Comparing Two Groups",
    "section": "",
    "text": "7.1 Learning Objectives\nThis week, we introduce the idea of comparing two groups to evaluate whether the data that we have sampled lead us to believe that the population distribution of the random variables are different. Of course, because we don‚Äôt get access to the function that describes the random variable, we can‚Äôt actually know if the populations are different. It is for this reason that we call it statistical inference ‚Äì we are inferring from a sample some belief about the populations.\nAt the conclusion of this week, students will be able to:",
    "crumbs": [
      "Sampling and Testing",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Comparing Two Groups</span>"
    ]
  },
  {
    "objectID": "07-comparing-two-groups.html#learning-objectives",
    "href": "07-comparing-two-groups.html#learning-objectives",
    "title": "7¬† Comparing Two Groups",
    "section": "",
    "text": "Recognize the similarities between all frequentist hypothesis tests.\nEvaluate the conditions that surround the data, and choose a test that is best-powered and justifiable.\nPerform and Interpret the results of the most common statistical tests.",
    "crumbs": [
      "Sampling and Testing",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Comparing Two Groups</span>"
    ]
  },
  {
    "objectID": "07-comparing-two-groups.html#class-announcements",
    "href": "07-comparing-two-groups.html#class-announcements",
    "title": "7¬† Comparing Two Groups",
    "section": "7.2 Class Announcements",
    "text": "7.2 Class Announcements\n\nGreat work completing your final w203 test!\nUnit 7 Homework is Group Homework, due next week.\nThe Hypothesis Testing Lab is released today!\n\nLab is due at Unit 09 Live Session (two weeks): Apply these fundamentals to analyze 2022 election data and write a single, three-page analysis",
    "crumbs": [
      "Sampling and Testing",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Comparing Two Groups</span>"
    ]
  },
  {
    "objectID": "07-comparing-two-groups.html#roadmap",
    "href": "07-comparing-two-groups.html#roadmap",
    "title": "7¬† Comparing Two Groups",
    "section": "7.3 Roadmap",
    "text": "7.3 Roadmap\n\n7.3.1 Rearview Mirror\n\nStatisticians create a population model to represent the world\nA population model has parameters we are interested in\n\nEx: A parameter might represent the effect that a vitamin has on test performance\n\nA null hypothesis is a specific statement about a parameter\n\nEx: The vitamin has zero effect on performance\n\nA hypothesis test is a procedure for rejecting or not rejecting a null, such the probability of a type 1 error is constrained.\n\n\n\n7.3.2 Today\n\nThere are often multiple hypothesis tests you can apply to a scenario.\nOur primary concern is choosing a test with assumptions we can defend.\nSecondarily, we want to maximize power.\n\n\n\n7.3.3 Looking ahead\n\nNext week, we start working with models for linear regression\nWe will see how hypothesis testing is also used for regression parameters.",
    "crumbs": [
      "Sampling and Testing",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Comparing Two Groups</span>"
    ]
  },
  {
    "objectID": "07-comparing-two-groups.html#teamwork-discussion",
    "href": "07-comparing-two-groups.html#teamwork-discussion",
    "title": "7¬† Comparing Two Groups",
    "section": "7.4 Teamwork Discussion",
    "text": "7.4 Teamwork Discussion\n\n7.4.1 Working on Data Science Teams\nData science is a beautiful combination of team-work and individual-work. It provides the opportunity to work together on a data pipeline with people from all over the organization, to deal with technical, statistical, and social questions that are always interesting. While we expect that everyone on a team will be a professional, there is so much range within the pursuit of data science that projects are nearly always collaborative exercises.\nTogether as teams, we\n\nDefine research ambitions and scope\nImagine/envision the landscape of what is possible\nSupport, discuss, review and integrate individual contributions\n\nTogether as individuals we conduct the heads-down work that moves question answering forward. This might be reading papers to determine the most appropriate method to bring to bear on the question, or researching the data that is available, or understanding the technical requirements that we have to meet for this answer to be useful to our organization in real time.\nWhat is your instructor uniquely capable of? Let them tell you! But, at the same time, what would they acknowledge that others are better than them?\nSee, the thing is, because there is so much that has to be done, there literally are very, very few people who are one-stop data science shops. Instead, teams rely on collaboration and joint expertise in order to get good work done.\n\n\n7.4.2 The Problematic Psychology of Data Science\nPeople talk about the impostor syndrome: a feeling of inadequacy or interloping that is sometimes also associated with a fear of under-performing relative to the expectation of others on the team. These emotions are common through data science, academics, computer science. But, these types of emotions are also commonplace in journalism, film-making, and public speaking.\nHas anybody ever had the dream that they‚Äôre late to a test? Or, that that they‚Äôve got to give a speech that they‚Äôre unprepared for? Does anybody remember playing an instrument as a kid and having to go to recitals? Or, play for a championship on a youth sports team? Or, go into a test two?\nWhat are the feelings associated with those events? What might be generating these feelings?\n\n\n\n7.4.3 What Makes an Effective Team?\n\nThis reading on effective teams summarizes academic research to argue:\n\n\nWhat really matters to creating an effective tema is less about who is on the team, and more about how the team works together.\n\nIn your live session, your section might take 7 minutes to read this brief. If so, please read the following sections:\n\nThe problem statement;\nThe proposed solution;\nThe framework for team effectiveness, stopping at the section titled ‚ÄúTool: Help teams determine their own needs.‚Äù\n\n\n‚ÄúPsychological safety refers to an individual‚Äôs perception of the consequences of taking an interpersonal risk. It is a belief that a team is safe for risk taking in the face of being seen as ignorant, incompetent, negative, or disruptive.‚Äù\n‚ÄúIn a team with high psychological safety, teammates feel safe to take risks around their team members. They feel confident that no one on the team will embarrass or punish anyone else for admitting a mistake, asking a question, or offering a new idea.‚Äù\n\n\n\n7.4.4 We All Belong\n\nFrom your experience, can you give an example of taking a personal risk as part of a team?\n\nCan you describe your emotions when contemplating this risk?\nIf you did take the risk, how did the reactions of your teammates affect you?\n\nKnowing the circumstances that generate feelings of anxiety ‚Äì what steps can we take as a section, or a team, to recognize and respond to these circumstances?\n\n\nHow can you add to the psychological safety of your peers in the section and lab teammates?",
    "crumbs": [
      "Sampling and Testing",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Comparing Two Groups</span>"
    ]
  },
  {
    "objectID": "07-comparing-two-groups.html#team-kick-off",
    "href": "07-comparing-two-groups.html#team-kick-off",
    "title": "7¬† Comparing Two Groups",
    "section": "7.5 Team Kick-Off",
    "text": "7.5 Team Kick-Off\nLab 1 Teams\n\nHere are teams for Lab 1!\n\nTeam Kick-Off Conversation\n\nIn a 10 minute breakout with your team, please discuss the following questions:\n\n\nHow much time will you invest in the lab each week?\nHow often will you meet and for how long?\nHow will you discuss, review, and integrate individual work into the team deliverable?\nWhat do you see as the biggest risks when working on a team? How can you contribute to an effective team dynamic?",
    "crumbs": [
      "Sampling and Testing",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Comparing Two Groups</span>"
    ]
  },
  {
    "objectID": "07-comparing-two-groups.html#a-quick-review",
    "href": "07-comparing-two-groups.html#a-quick-review",
    "title": "7¬† Comparing Two Groups",
    "section": "7.6 A Quick Review",
    "text": "7.6 A Quick Review\nReview of Key Terms\n\nDefine each of the following:\n\nPopulation Parameter\nNull Hypothesis\nTest Statistic\nNull Distribution\n\n\nComparing Groups Review\nTake a moment to recall the tests you learned this week. Here is a quick cheat-sheet to their key assumptions.\n\n\n\n\n\n\n\n\npaired/unpaired\nparametric\nnon-parametric\n\n\n\n\nunpaired\nunpaired t-test  - metric var  - i.i.d.  - (not too un-)normal\nWilcoxon rank-sum  ordinal var  i.i.d. \n\n\npaired\npaired t-test  metric var  i.i.d.  (not too un-)normal\nWilcoxon signed-rank  metric var  i.i.d.  difference is symmetric   sign test  ordinal var  i.i.d.",
    "crumbs": [
      "Sampling and Testing",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Comparing Two Groups</span>"
    ]
  },
  {
    "objectID": "07-comparing-two-groups.html#rank-based-tests",
    "href": "07-comparing-two-groups.html#rank-based-tests",
    "title": "7¬† Comparing Two Groups",
    "section": "7.7 Rank Based Tests",
    "text": "7.7 Rank Based Tests\nDarrin Speegle has a nice talk-through, walk through of rank based testing procedures, linked here. We‚Äôll talk through a few examples of this, and then move to estimating against data for the class.",
    "crumbs": [
      "Sampling and Testing",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Comparing Two Groups</span>"
    ]
  },
  {
    "objectID": "07-comparing-two-groups.html#comparing-groups-r-exercise",
    "href": "07-comparing-two-groups.html#comparing-groups-r-exercise",
    "title": "7¬† Comparing Two Groups",
    "section": "7.8 Comparing Groups R Exercise",
    "text": "7.8 Comparing Groups R Exercise\nThe General Social Survey (GSS) is one of the longest running and extensive survey projects in the US. The full dataset includes over 1000 variables spanning demographics, attitudes, and behaviors. The file GSS_w203.RData contains a small selection of a variables from the 2018 GSS.\nTo learn about each variable, you can enter it into the search bar at the GSS data explorer\n\nload('data/GSS_w203.RData')\nsummary(GSS)\n\n           rincome               happy                           sexnow   \n $25000 or more: 851   very happy   : 701   women                   :758  \n $20000 - 24999: 107   pretty happy :1307   man                     :640  \n $10000 - 14999:  94   not too happy: 336   transgender             :  2  \n $15000 - 19999:  61   DK           :   0   a gender not listed here:  1  \n lt $1000      :  33   IAP          :   0   Don't know              :  0  \n (Other)       : 169   NA           :   0   (Other)                 :  0  \n NA's          :1033   NA's         :   4   NA's                    :947  \n     wwwhr           emailhr                     socrel   \n Min.   :  0.00   Min.   :  0.000   sev times a week:382  \n 1st Qu.:  3.00   1st Qu.:  0.000   sev times a mnth:287  \n Median :  8.00   Median :  2.000   once a month    :259  \n Mean   : 13.91   Mean   :  7.152   sev times a year:240  \n 3rd Qu.: 20.00   3rd Qu.: 10.000   almost daily    :217  \n Max.   :140.00   Max.   :100.000   (Other)         :171  \n NA's   :986      NA's   :929       NA's            :792  \n             socommun      numpets          tvhours      \n never           :510   Min.   : 0.000   Min.   : 0.000  \n once a month    :243   1st Qu.: 0.000   1st Qu.: 1.000  \n sev times a week:219   Median : 1.000   Median : 2.000  \n sev times a year:196   Mean   : 1.718   Mean   : 2.938  \n sev times a mnth:174   3rd Qu.: 2.000   3rd Qu.: 4.000  \n (Other)         :215   Max.   :20.000   Max.   :24.000  \n NA's            :791   NA's   :1201     NA's   :793     \n                     major1         owngun   \n business administration: 138   yes    :537  \n education              :  79   no     :993  \n engineering            :  54   refused: 39  \n nursing                :  51   DK     :  0  \n health                 :  42   IAP    :  0  \n (Other)                : 546   NA     :  0  \n NA's                   :1438   NA's   :779  \n\n\nYou have a set of questions that you would like to answer with a statistical test. For each question:\n\nChoose the most appropriate test.\nList and evaluate the assumptions for your test.\nConduct your test.\nDiscuss statistical and practical significance.",
    "crumbs": [
      "Sampling and Testing",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Comparing Two Groups</span>"
    ]
  },
  {
    "objectID": "07-comparing-two-groups.html#the-questions",
    "href": "07-comparing-two-groups.html#the-questions",
    "title": "7¬† Comparing Two Groups",
    "section": "7.9 The Questions",
    "text": "7.9 The Questions\n\n7.9.1 Set 1\n\nDo economics majors watch more or less TV than computer science majors?\n\n\nGSS %&gt;% \n  filter(major1 %in% c('computer science', 'economics')) %&gt;% \n  ggplot() + \n  aes(x = tvhours, fill = major1) + \n  geom_histogram(bins = 10, position = 'dodge')\n\nWarning: Removed 11 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\n\n\n\n\nWhat kinds of tests could be reasonable to conduct? For what part of the data would we conduct these tests?\n\n## The assumptions about the data drive us to the correct test. \n## But, let's ask all the tests that could *possibly* make sense, and see how \n##     matching or mis-matching assumptions changes what we learn. \n\n## Answers are in the next chunk... but don't jump to them right away. \n\n\nDo Americans with pets watch more or less TV than Americans without pets?\n\n\n\n7.9.2 Set 2\n\nDo Americans spend more time emailing or using the web?\n\n\nGSS %&gt;% \n  select(wwwhr, emailhr) %&gt;% \n  drop_na() %$% \n  t.test(x = wwwhr, y = emailhr, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  wwwhr and emailhr\nt = 13.44, df = 1360, p-value &lt; 2.2e-16\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 5.530219 7.420553\nsample estimates:\nmean difference \n       6.475386 \n\nGSS %&gt;% \n  ggplot() + \n  geom_histogram(aes(x = wwwhr),   fill = 'darkblue', alpha = 0.5) + \n  geom_histogram(aes(x = emailhr), fill = 'darkred',  alpha = 0.5)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 986 rows containing non-finite values (`stat_bin()`).\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 929 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\n\n\n\nt.test(\n  x = GSS$wwwhr, \n  y = GSS$emailhr, \n  paired = FALSE\n)\n\n\n    Welch Two Sample t-test\n\ndata:  GSS$wwwhr and GSS$emailhr\nt = 12.073, df = 2398.5, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 5.657397 7.851614\nsample estimates:\nmean of x mean of y \n13.906021  7.151515 \n\n\n\nDo Americans spend more evenings with neighbors or with relatives?\n\n\nwilcox_test_data &lt;- GSS %&gt;% \n  select(socrel, socommun) %&gt;%\n  mutate(\n    family_ordered = factor(\n      x      = socrel, \n      levels = c('almost daily', 'sev times a week', \n                 'sev times a mnth', 'once a month',\n                 'sev times a year', 'once a year', 'never')),\n    friends_ordered = factor(\n      x      = socommun, \n      levels = c('almost daily', 'sev times a week', \n                 'sev times a mnth', 'once a month',\n                 'sev times a year', 'once a year', 'never'))) \n\nTo begin this investigation, we‚Äôve got to look at the data and see what is in it. If you look below, you‚Äôll note that it sure seems that people are spending more time with their family‚Ä¶ erp, actually no. They‚Äôre ‚Äúhanging out‚Äù with their friends rather than taking their mother out to dinner.\n\nwilcox_test_data %&gt;% \n  select(friends_ordered, family_ordered) %&gt;% \n  rename(\n    Friends = friends_ordered, \n    Family  = family_ordered\n  ) %&gt;% \n  drop_na() %&gt;% \n  pivot_longer(cols = c(Friends, Family)) %&gt;%   \n  ggplot() + \n    aes(x=value, fill=name) + \n    geom_histogram(stat='count', position='dodge', alpha=0.7) + \n  scale_fill_manual(values = c('#003262', '#FDB515')) + \n  labs(\n    title    = 'Do Americans Spend Times With Friends or Family?',\n    subtitle = 'A cutting analysis.', \n    fill     = 'Friends or Family', \n    x        = 'Amount of Time Spent') + \n  scale_x_discrete(guide = guide_axis(n.dodge = 2)) +\n  theme_minimal()\n\nWarning in geom_histogram(stat = \"count\", position = \"dodge\", alpha = 0.7):\nIgnoring unknown parameters: `binwidth`, `bins`, and `pad`\n\n\n\n\n\n\n\n\n\nWith this plot created, we can ask if what we observe in the plot is the produce of what could just be sampling error, or if this is something that was unlikely to arise due if the null hypothesis were true. What is the null hypothesis? Well, lets suppose that if we didn‚Äôt know anything about the data that we would expect there to be no difference between the amount of time spent with friends or families.\n\n## risky choice -- casting the factor to a numeric without checking what happens.\nwilcox_test_data %$% \n  wilcox.test(\n    x = as.numeric(family_ordered), \n    y = as.numeric(friends_ordered),\n    paired = FALSE\n  )\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  as.numeric(family_ordered) and as.numeric(friends_ordered)\nW = 716676, p-value &lt; 2.2e-16\nalternative hypothesis: true location shift is not equal to 0\n\n\n\n\n7.9.3 Set 3\n\nAre Americans that own guns or Americans that don‚Äôt own guns more likely to have pets?\nAre Americans with pets happier than Americans without pets?\n\n\n\n7.9.4 Apply to a New Type of Data\n\nIs there a relationship between college major and gun ownership?",
    "crumbs": [
      "Sampling and Testing",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Comparing Two Groups</span>"
    ]
  },
  {
    "objectID": "07-comparing-two-groups.html#simulating-the-effects-of-test-choices",
    "href": "07-comparing-two-groups.html#simulating-the-effects-of-test-choices",
    "title": "7¬† Comparing Two Groups",
    "section": "7.10 Simulating the Effects of Test Choices",
    "text": "7.10 Simulating the Effects of Test Choices\n\ntheme_set(theme_minimal())\n\nberkeley_blue   &lt;- '#003262'\nberkeley_gold   &lt;- '#FDB515'\nberkeley_sather &lt;- '#B9D3B6'\n\n\n7.10.1 Should we use a t-test or a wilcox sign-rank?\nThere is some open discussion in the applied statistics literature about whether we should ever be using a t-test. In particular, if the underlying distribution that generates the data is not normal, than the assumptions of a t-test are not, technically satisfied and the test does not produce results that have nominal p-value coverage. This is both technically and theoretically true; and yet, researchers, data scientists, your instructors, and the entire world runs t-tests as ‚Äútest of first recourse.‚Äù\nWhat is the alternative to conducting a t-test as the test of first recourse? It might be the Wilcox test. The Wilcox test makes a weaker assumption ‚Äì of symmetry around the mean or median ‚Äì which is weaker than the assumption of normality.\nAdditional points of argument, which you will investigate in this worksheet:\n\nIf the underlying data is normal, then the Wilcox test is nearly as well powered as the t-test.\nIf the underlying data is not normal, then the Wilcox test still maintains nominal p-value coverage, whereas the t-test might lose this guarantee.",
    "crumbs": [
      "Sampling and Testing",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Comparing Two Groups</span>"
    ]
  },
  {
    "objectID": "07-comparing-two-groups.html#section",
    "href": "07-comparing-two-groups.html#section",
    "title": "7¬† Comparing Two Groups",
    "section": "7.11 ",
    "text": "7.11 \n\n7.11.1 The Poisson Distribution\nThe poisson distribution has the following PDF:\n\\[\nf_X(x) = \\frac{\\lambda^n e^{-\\lambda}}{n!}\n\\]\nThe key shape parameter for a poisson function is \\(\\lambda\\); we show three different distributions, setting this shape parameter to be 1, 3, and 30 respectively. Notice that the limits on these plots are not set to be the same; for example, the range in the third plot is considerably larger than the first.\n\npois_lambda_1  &lt;- rpois(n=1000, lambda=1)\npois_lambda_3  &lt;- rpois(n=1000, lambda=3)\npois_lambda_30 &lt;- rpois(n=1000, lambda=30)\n\nplot_1  &lt;- ggplot() + aes(x=pois_lambda_1) + geom_histogram(bins=6, fill = berkeley_blue)\nplot_3  &lt;- ggplot() + aes(x=pois_lambda_3) + geom_histogram(bins=10, fill = berkeley_gold)\nplot_30 &lt;- ggplot() + aes(x=pois_lambda_30) + geom_histogram(bins=30, fill = berkeley_sather)\n\nplot_1 / plot_3 / plot_30\n\n\n\n\n\n\n\n\nWhat does this changing distribution do to the p-values?\n\n\n7.11.2 Write a Simulation\n\npois_sim &lt;- function(num_observations, lambda_one, lambda_two) { \n  \n  t_test_result &lt;- rep(NA, 10000)\n  wilcox_result &lt;- rep(NA, 10000)\n  \n  for(i in 1:10000) { \n    group_one &lt;- rpois(n=num_observations, lambda=lambda_one)\n    group_two &lt;- rpois(n=num_observations, lambda=lambda_two)\n  \n    t_test_result[i] &lt;- t.test(group_one, group_two)$p.value\n    wilcox_result[i] &lt;- wilcox.test(x=group_one, y=group_two)$p.value\n  }\n  \n  df &lt;- data.table(\n    p_value = c(t_test_result, wilcox_result), \n    test    = rep(c('t_test', 'wilcox_test'), each = 10000)\n  )\n  \n  return(df)\n}\n\n\nfoo &lt;- pois_sim(20, 1, 2.0)\n\n\nfoo %&gt;%\n  ggplot() +\n  geom_density(aes(x=p_value, color = test)) +\n  scale_color_manual(values = c(berkeley_blue, berkeley_gold))\n\n\n\n\n\n\n\n\nAnd so, the simulation rejects the null at the following rates:\n\nFor the t-test, at a rate of 0.0661353\nFor the Wilcox test, at a rate of 0.0760041\n\n\n\n7.11.3 What if a distribution is much more skewed?\n\nskewed_sim &lt;- function(num_sims=1000, num_observations, alpha_1, beta_1, alpha_2, beta_2) {\n  \n  t_test_result &lt;- rep(NA, num_sims)\n  wilcox_result &lt;- rep(NA, num_sims)\n  \n  for(i in 1:num_sims) { \n    group_one &lt;- rbeta(n=num_observations, shape1 = alpha_1, shape2 = beta_1)\n    group_two &lt;- rbeta(n=num_observations, shape1 = alpha_2, shape2 = beta_2)\n  \n    t_test_result[i] &lt;- t.test(group_one, group_two)$p.value\n    wilcox_result[i] &lt;- wilcox.test(x=group_one, y=group_two)$p.value\n  }\n  \n  dt &lt;- data.table(\n    p_value = c(t_test_result, wilcox_result), \n    test    = rep(c('t_test', 'wilcox_test'), each = num_sims)\n  )\n  \n  return(dt)\n}\n\n\n\n7.11.4 False Rejection Rates\nStart with a distribution that has parameters alpha=2, beta=7.\n\nggplot(data.frame(x=c(0,1)), aes(x)) + \n  stat_function(fun = dbeta, n=100, args=list(shape1=2, shape2=7))\n\n\n\n\n\n\n\n\n\nsame_dist_small_data &lt;- skewed_sim(\n  num_observations=10, \n  alpha_1=2, beta_1=7, \n  alpha_2=2, beta_2=7\n  )\nsame_dist_med_data &lt;- skewed_sim(\n  num_observations=50, \n  alpha_1=2, beta_1=7, \n  alpha_2=2, beta_2=7\n  )\nsame_dist_big_data &lt;- skewed_sim( # haha, \"big data\"\n  num_observations=100, \n  alpha_1=2, beta_1=7, \n  alpha_2=2, beta_2=7\n  )\n\n\nplot_1 &lt;- same_dist_small_data %&gt;%\n  ggplot() +\n  geom_density(aes(x=p_value, color = test), bounds=c(0,1)) +\n  scale_color_manual(values = c(berkeley_blue, berkeley_gold))\nplot_2 &lt;- same_dist_med_data %&gt;%\n  ggplot() +\n  geom_density(aes(x=p_value, color = test), bounds=c(0,1)) +\n  scale_color_manual(values = c(berkeley_blue, berkeley_gold))\nplot_3 &lt;- same_dist_big_data %&gt;%\n  ggplot() +\n  geom_density(aes(x=p_value, color = test), bounds=c(0,1)) +\n  scale_color_manual(values = c(berkeley_blue, berkeley_gold))\n\nplot_1 / plot_2 / plot_3\n\n\n\n\n\n\n\n\n\nT-tests\n\n0.046\n0.051\n0.046\n\nWilcox Tests\n\n0.045\n0.048\n0.05\n\n\n\n\n7.11.5 What about Power to Reject\n\nsmall_diff_small_data &lt;- skewed_sim(\n  num_observations=10, \n  alpha_1=2, beta_1=7, \n  alpha_2=2, beta_2=5\n  )\nsmall_diff_med_data &lt;- skewed_sim(\n  num_observations=50, \n  alpha_1=2, beta_1=7, \n  alpha_2=2, beta_2=5\n  )\nsmall_diff_big_data &lt;- skewed_sim( # haha, \"big data\"\n  num_observations=100, \n  alpha_1=2, beta_1=7, \n  alpha_2=2, beta_2=5\n  )\n\n\nplot_1 &lt;- small_diff_small_data %&gt;%\n  ggplot() +\n  geom_density(aes(x=p_value, color = test), bounds=c(0,1)) +\n  scale_color_manual(values = c(berkeley_blue, berkeley_gold))\nplot_2 &lt;- small_diff_med_data %&gt;%\n  ggplot() +\n  geom_density(aes(x=p_value, color = test), bounds=c(0,1)) +\n  scale_color_manual(values = c(berkeley_blue, berkeley_gold))\nplot_3 &lt;- small_diff_big_data %&gt;%\n  ggplot() +\n  geom_density(aes(x=p_value, color = test), bounds=c(0,1)) +\n  scale_color_manual(values = c(berkeley_blue, berkeley_gold))\n\nplot_1 / plot_2 / plot_3\n\n\n\n\n\n\n\n\n\n\n7.11.6 Paired compared to unpaired tests\n\npaired_sim &lt;- function(num_sims=10000, num_observations, mean_one, mean_two, paired_diff, sd_one, sd_two) { \n  \n  unpaired_test_unpaired_data &lt;- rep(NA, num_sims)\n  unpaired_test_paired_data   &lt;- rep(NA, num_sims)\n  paired_test_unpaired_data   &lt;- rep(NA, num_sims)\n  paired_test_paired_data     &lt;- rep(NA, num_sims)\n  \n  for(i in 1:num_sims) { \n    observation_a1 &lt;- rnorm(n = num_observations, mean = mean_one, sd = sd_one) \n    ## first create unpaired data \n    observation_b &lt;- rnorm(n = num_observations, mean = mean_two, sd = sd_two)\n    ## then, create paired data \n    observation_a2 &lt;- observation_a1 + rnorm(n = num_observations, mean = paired_diff, sd=sd_two)\n    \n    ## run tests\n    unpaired_test_unpaired_data[i] &lt;- t.test(x=observation_a1, y=observation_b,  paired=FALSE)$p.value\n    unpaired_test_paired_data[i]   &lt;- t.test(x=observation_a1, y=observation_a2,  paired=FALSE)$p.value\n    paired_test_unpaired_data[i]   &lt;- t.test(x=observation_a1, y=observation_b,  paired=TRUE)$p.value\n    paired_test_paired_data[i]     &lt;- t.test(x=observation_a1, y=observation_a2, paired=TRUE)$p.value\n  }\n  \n  dt &lt;- data.table(\n    p_value = c(unpaired_test_unpaired_data, unpaired_test_paired_data, \n                paired_test_unpaired_data,   paired_test_paired_data), \n    test    = rep(c('unpaired data, unpaired test', 'paired data, unpaired test', \n                    'unpaired data, paired test', 'paired data, paired test'), each = num_sims)\n  )\n  \n  return(dt)\n}\n\n\nbar &lt;- paired_sim(num_observations = 30,  mean_one = 10, mean_two = 11, paired_diff = 1, sd_one = 4, sd_two = 5)\n\n\npaired_data_plot &lt;- bar[grep('unpaired data', test, invert=TRUE)] %&gt;% \n  ggplot() + \n    aes(x=p_value, color = test, fill = test) + \n    geom_density(alpha=0.25) + \n  labs(title = 'Paired Data')\nunpaired_data_plot &lt;- bar[grep('unpaired data', test, invert=FALSE)] %&gt;% \n  ggplot() + \n    aes(x=p_value, color = test, fill = test) + \n    geom_density(alpha=0.25) + \n  labs(title = 'Unpaired Data')\n\npaired_data_plot / unpaired_data_plot",
    "crumbs": [
      "Sampling and Testing",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Comparing Two Groups</span>"
    ]
  },
  {
    "objectID": "regression.html",
    "href": "regression.html",
    "title": "Regression",
    "section": "",
    "text": "In this section, we talk about regression. There is much talk about regression.",
    "crumbs": [
      "Regression"
    ]
  },
  {
    "objectID": "08-regression-estimation.html",
    "href": "08-regression-estimation.html",
    "title": "8¬† OLS Regression Estimates",
    "section": "",
    "text": "8.1 Learning Objectives\nAt the end of this week, students will be able to:",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>OLS Regression Estimates</span>"
    ]
  },
  {
    "objectID": "08-regression-estimation.html#learning-objectives",
    "href": "08-regression-estimation.html#learning-objectives",
    "title": "8¬† OLS Regression Estimates",
    "section": "",
    "text": "Understand the algorithm that produces the OLS regression estimates.\nFit regressions to produce estimates from a best linear predictor.\nProduce predictions from this OLS regression that are informative about the population.",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>OLS Regression Estimates</span>"
    ]
  },
  {
    "objectID": "08-regression-estimation.html#class-announcements",
    "href": "08-regression-estimation.html#class-announcements",
    "title": "8¬† OLS Regression Estimates",
    "section": "8.2 Class Announcements",
    "text": "8.2 Class Announcements\n\nLab 1 is due next week!\nThere is not a ‚Äúhomework‚Äù assignment this week ‚Äì instead you‚Äôre working on your group‚Äôs lab\nYou‚Äôre doing great - keep it up!",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>OLS Regression Estimates</span>"
    ]
  },
  {
    "objectID": "08-regression-estimation.html#roadmap",
    "href": "08-regression-estimation.html#roadmap",
    "title": "8¬† OLS Regression Estimates",
    "section": "8.3 Roadmap",
    "text": "8.3 Roadmap\nRear-View Mirror\n\nStatisticians create a population model to represent the world.\nSometimes, the model includes an ‚Äúoutcome‚Äù random variable \\(Y\\) and ‚Äúinput‚Äù random variables \\(X_1, X_2,...,X_k\\).\nThe joint distribution of \\(Y\\) and \\(X_1, X_2,...,X_k\\) is complicated.\nThe best linear predictor (BLP) is the canonical way to summarize the relationship.\n\nToday\n\nOLS regression is an estimator for the BLP\nWe‚Äôll discuss the mechanics of OLS\n\nLooking Ahead\n\nTo make regression estimates useful, we need measures of uncertainty (standard errors, tests‚Ä¶).\nThe process of building a regression model looks different, depending on whether the goal is prediction, description, or explanation.",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>OLS Regression Estimates</span>"
    ]
  },
  {
    "objectID": "08-regression-estimation.html#discussion-questions",
    "href": "08-regression-estimation.html#discussion-questions",
    "title": "8¬† OLS Regression Estimates",
    "section": "8.4 Discussion Questions",
    "text": "8.4 Discussion Questions\nSuppose we have random variables \\(X\\) and \\(Y\\).\n\nWhy do we care about the BLP?\nWhat assumptions are needed for OLS to consistently estimate the BLP?",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>OLS Regression Estimates</span>"
    ]
  },
  {
    "objectID": "08-regression-estimation.html#best-linear-predictor-and-ols-regression-as-a-predictor",
    "href": "08-regression-estimation.html#best-linear-predictor-and-ols-regression-as-a-predictor",
    "title": "8¬† OLS Regression Estimates",
    "section": "8.5 Best Linear Predictor and OLS Regression as a Predictor",
    "text": "8.5 Best Linear Predictor and OLS Regression as a Predictor\nWe‚Äôve worked with this function last week: Suppose that random variables \\(X\\) and \\(Y\\) are jointly continuous, with joint density function given by,\n\\[\nf_{X,Y}(x,y) =\n  \\begin{cases}\n    2-x-y, & 0 \\leq x \\leq 1; 0 \\leq y \\leq 1 \\\\\n    0, & otherwise\n  \\end{cases}\n\\]\nWith this function we have previously been able to calculate some quantities that we care about, specifically:\n\nThe covariance between \\(X\\) and \\(Y\\), which we calculated to be: \\(-1/144\\).\nThe variance of \\(X\\), which we calculated to be \\(11/144\\).\n\nWith the two of these, we were able to also write down the best linear predictor,\n\nThe \\(\\beta_{BLP} = Cov[X,Y]/Var[X] = (-1/144) / (11/144) = -1/11\\).\n\\(\\alpha_{BLP} = E[Y] - \\beta_{BLP}E[X] = (5/12) - (-1/11)(5/12) = 71/132\\). (Sorry for the ugly math‚Ä¶)\n\nWe wrote code that would sample from this PDF:\n\njoint_pdf_1 &lt;- function(x_input, y_input) { \n  probs = 2 - x_input - y_input\n  return(probs)\n}\njoint_pdf_1(x_input = 0, y_input = 0)\n\n[1] 2\n\n\nThis next step is the part that requires us to squint a little bit. We‚Äôre going to create a ‚Äúpopulation‚Äù that has 1,000,000 observations in it, and we‚Äôre going to sample from this population in a way that is governed by the joint pdf.\n\nd &lt;- data.frame(\n  expand.grid(\n    x = seq(from = 0, to = 1, length.out = 1000), \n    y = seq(from = 0, to = 1, length.out = 1000))) |&gt; \n  mutate(prob = joint_pdf_1(x_input = x, y_input = y))\ntail(d)\n\n               x y        prob\n999995  0.994995 1 0.005005005\n999996  0.995996 1 0.004004004\n999997  0.996997 1 0.003003003\n999998  0.997998 1 0.002002002\n999999  0.998999 1 0.001001001\n1000000 1.000000 1 0.000000000\n\n\nWith that put together, we can take samples from this data:\n\nsample_10  &lt;- d |&gt; slice_sample(n = 10, weight_by = prob)\nsample_20  &lt;- d |&gt; slice_sample(n = 20, weight_by = prob)\nsample_100 &lt;- d |&gt; slice_sample(n = 100, weight_by = prob)\nsample_200 &lt;- d |&gt; slice_sample(n = 200, weight_by = prob)\n\nsample_10000 &lt;- d |&gt; slice_sample(n = 10000, weight_by = prob)\n\n\nplot_10 &lt;- \n  sample_10 |&gt; \n    ggplot() + \n    aes(x=x, y=y) + \n    geom_point()\nplot_20 &lt;- \n  sample_20 |&gt; \n    ggplot() + \n    aes(x=x, y=y) + \n    geom_point()\nplot_100 &lt;- \n  sample_100 |&gt; \n    ggplot() + \n    aes(x=x, y=y) + \n    geom_point()\nplot_200 &lt;- \n  sample_200 |&gt; \n    ggplot() + \n    aes(x=x, y=y) + \n    geom_point()\n\n(plot_10 | plot_20) / \n  (plot_100 | plot_200)\n\n\n\n\n\n\n\n\n\nmodel_10    &lt;- lm(y ~ x, data = sample_10)\nmodel_100   &lt;- lm(y ~ x, data = sample_100)\nmodel_200   &lt;- lm(y ~ x, data = sample_200)\nmodel_10000 &lt;- lm(y ~ x, data = sample_10000)\n\ncoef(model_10)\n\n(Intercept)           x \n  0.5081318  -0.5460098 \n\n\n\nresults_10 &lt;- NA \nfor(i in 1:100) { \n  sample_10  &lt;- d |&gt; slice_sample(n = 10, weight_by = prob)\n  model_10   &lt;- lm(y ~ x, data = sample_10)\n  results_10[i] &lt;- coef(model_10)['x']\n}\n\nresults_200 &lt;- NA \nfor(i in 1:100) { \n  sample_200  &lt;- d |&gt; slice_sample(n = 200, weight_by = prob)\n  model_200   &lt;- lm(y ~ x, data = sample_200)\n  results_200[i] &lt;- coef(model_200)['x']\n  }\n\n\nplot_10 &lt;- ggplot() + \n  aes(x=results_10) + \n  geom_histogram()\n\nplot_200 &lt;- ggplot() + \n  aes(x=results_200) + \n  geom_histogram()\n  \nplot_10 / \n  plot_200\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>OLS Regression Estimates</span>"
    ]
  },
  {
    "objectID": "08-regression-estimation.html#the-regression-anatomy-formula",
    "href": "08-regression-estimation.html#the-regression-anatomy-formula",
    "title": "8¬† OLS Regression Estimates",
    "section": "8.6 The Regression Anatomy Formula",
    "text": "8.6 The Regression Anatomy Formula\nWe make the claim in live session that we can re-represent a coefficient that we‚Äôre interested in as a function of all the other variable in a regression. That is, suppose that we were interested, initially, in estimating the model:\n\\[\nY = \\hat\\beta_{0} + \\hat\\beta_{1} X_{1} + \\hat\\beta_{2} X_{2} + \\hat\\beta_{3}X_{3} + e\n\\] that we can produce an estimate for \\(\\hat\\beta_{1}\\) by fitting this auxiliary regression,\n\\[\nX_{1} = \\hat\\delta_{0} + \\hat\\delta_2X_2 + \\hat\\delta_3X_3 + r_{1}\n\\]\nAnd then using the residuals, noted as \\(r_1\\) above, in a second auxiliary regression,\n\\[\nY = \\gamma_0 + \\gamma_1 r_1\n\\]\nThe claim that we make in the live session is that there is a guarantee that \\(\\beta_1 = \\gamma_1\\). Here, we are first going to show that this is true, and then we‚Äôre going to reason about what this means, and why this feature is interesting (or at least useful) when we are estimating a regression.\nSuppose that the population model is the following:\n\\[\nX_{1} =\n\\begin{cases}\n  \\frac{1}{10}, & 0 \\leq x \\leq 10, \\\\\n  0, & otherwise\n\\end{cases}\n\\]\n\\[\nX_{2} =\n\\begin{cases}\n  \\frac{1}{10}, & 0 \\leq x \\leq 10, \\\\\n  0, & otherwise\n\\end{cases}\n\\]\n\\[\nX_{3} =\n\\begin{cases}\n  \\frac{1}{10}, & 0 \\leq x \\leq 10, \\\\\n  0, & otherwise\n\\end{cases}\n\\] And, furthermore suppose that \\(Y = g(X_{1}, X_{2}, X_{3}\\), specifically, that:\n\\[\nY = -3 + (1\\cdot X_1) + (2\\cdot X_2) + (3\\cdot X_3)\n\\] Then, because we know the population model, we can produce a single sample from it using the following code:\n\nd &lt;- data.frame(\n  x1 = runif(n = 100, min = 0, max = 10), \n  x2 = runif(n = 100, min = 0, max = 10), \n  x3 = runif(n = 100, min = 0, max = 10)) %&gt;% \n  mutate(y = -3 + 1*x1 + 2*x2 + 3*x3 + rnorm(n = n(), mean = 0, sd = 1))\n\nhead(d)\n\n        x1       x2       x3        y\n1 6.218677 2.651004 5.519509 25.26302\n2 7.208680 5.704039 9.513955 41.61790\n3 4.437298 5.411798 3.212990 22.78072\n4 8.450337 7.524730 8.323597 45.43666\n5 2.313235 7.518738 4.704166 28.82640\n6 6.355665 4.821157 9.534551 41.48268\n\n\nNotice that when we made this data, we included a set of random noise at the end. The idea here is that there are other ‚Äúthings‚Äù in this universe that also affect \\(Y\\), but that we don‚Äôt have access to them. By assumption, what we have measured in this world, \\(X_1, X_2, X_3\\) are uncorrelated with these other features.\n\n8.6.1 Estimate an OLS Regression\nLet‚Äôs begin by producing an estimate of the OLS regression of Y on these X variables. Notice the way that we‚Äôre talking about this:\n\nWe are going to regress Y on \\(X_{1}\\), \\(X_{2}\\), and \\(X_{3}\\).\n\n\nmodel_main &lt;- lm(y ~ x1 + x2 + x3, data = d)\ncoef(model_main)\n\n(Intercept)          x1          x2          x3 \n -3.3488886   0.9996716   2.0585375   3.0065569 \n\n\n\n\n8.6.2 Regression Anatomy and Fritch Waugh Lovell\nThe claim is that we can produce an estimate of \\(\\hat{\\beta}_1\\) using an auxiliary set of regression estimates, and then using the regression from that auxiliary regression.\n\nmodel_aux &lt;- lm(x1 ~ x2 + x3, data = d)\n\nIf we look into the structure of model_aux we can see that there are a ton of pieces in here.\n\nstr(model_aux)\n\nList of 12\n $ coefficients : Named num [1:3] 4.1225 0.0668 0.1081\n  ..- attr(*, \"names\")= chr [1:3] \"(Intercept)\" \"x2\" \"x3\"\n $ residuals    : Named num [1:100] 1.322 1.676 -0.394 2.925 -2.82 ...\n  ..- attr(*, \"names\")= chr [1:100] \"1\" \"2\" \"3\" \"4\" ...\n $ effects      : Named num [1:100] -50.46 -1.77 3.16 2.62 -3.06 ...\n  ..- attr(*, \"names\")= chr [1:100] \"(Intercept)\" \"x2\" \"x3\" \"\" ...\n $ rank         : int 3\n $ fitted.values: Named num [1:100] 4.9 5.53 4.83 5.53 5.13 ...\n  ..- attr(*, \"names\")= chr [1:100] \"1\" \"2\" \"3\" \"4\" ...\n $ assign       : int [1:3] 0 1 2\n $ qr           :List of 5\n  ..$ qr   : num [1:100, 1:3] -10 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 ...\n  .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. ..$ : chr [1:100] \"1\" \"2\" \"3\" \"4\" ...\n  .. .. ..$ : chr [1:3] \"(Intercept)\" \"x2\" \"x3\"\n  .. ..- attr(*, \"assign\")= int [1:3] 0 1 2\n  ..$ qraux: num [1:3] 1.1 1.02 1.07\n  ..$ pivot: int [1:3] 1 2 3\n  ..$ tol  : num 1e-07\n  ..$ rank : int 3\n  ..- attr(*, \"class\")= chr \"qr\"\n $ df.residual  : int 97\n $ xlevels      : Named list()\n $ call         : language lm(formula = x1 ~ x2 + x3, data = d)\n $ terms        :Classes 'terms', 'formula'  language x1 ~ x2 + x3\n  .. ..- attr(*, \"variables\")= language list(x1, x2, x3)\n  .. ..- attr(*, \"factors\")= int [1:3, 1:2] 0 1 0 0 0 1\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. ..$ : chr [1:3] \"x1\" \"x2\" \"x3\"\n  .. .. .. ..$ : chr [1:2] \"x2\" \"x3\"\n  .. ..- attr(*, \"term.labels\")= chr [1:2] \"x2\" \"x3\"\n  .. ..- attr(*, \"order\")= int [1:2] 1 1\n  .. ..- attr(*, \"intercept\")= int 1\n  .. ..- attr(*, \"response\")= int 1\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n  .. ..- attr(*, \"predvars\")= language list(x1, x2, x3)\n  .. ..- attr(*, \"dataClasses\")= Named chr [1:3] \"numeric\" \"numeric\" \"numeric\"\n  .. .. ..- attr(*, \"names\")= chr [1:3] \"x1\" \"x2\" \"x3\"\n $ model        :'data.frame':  100 obs. of  3 variables:\n  ..$ x1: num [1:100] 6.22 7.21 4.44 8.45 2.31 ...\n  ..$ x2: num [1:100] 2.65 5.7 5.41 7.52 7.52 ...\n  ..$ x3: num [1:100] 5.52 9.51 3.21 8.32 4.7 ...\n  ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language x1 ~ x2 + x3\n  .. .. ..- attr(*, \"variables\")= language list(x1, x2, x3)\n  .. .. ..- attr(*, \"factors\")= int [1:3, 1:2] 0 1 0 0 0 1\n  .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. ..$ : chr [1:3] \"x1\" \"x2\" \"x3\"\n  .. .. .. .. ..$ : chr [1:2] \"x2\" \"x3\"\n  .. .. ..- attr(*, \"term.labels\")= chr [1:2] \"x2\" \"x3\"\n  .. .. ..- attr(*, \"order\")= int [1:2] 1 1\n  .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. ..- attr(*, \"response\")= int 1\n  .. .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n  .. .. ..- attr(*, \"predvars\")= language list(x1, x2, x3)\n  .. .. ..- attr(*, \"dataClasses\")= Named chr [1:3] \"numeric\" \"numeric\" \"numeric\"\n  .. .. .. ..- attr(*, \"names\")= chr [1:3] \"x1\" \"x2\" \"x3\"\n - attr(*, \"class\")= chr \"lm\"\n\n\nTo evaluate our claim, we need to find the residuals from this regression. As a knowledge check, what is it that we mean when we say, ‚Äúresidual‚Äù in this sense?\nTo make talking about these easier, here is a plot that might be useful.\n\nd %&gt;% \n  ggplot() + \n  aes(x = x1, y = y) + \n  geom_point() + \n  geom_segment(aes(x = 0, xend = 10, y = 0, yend = 50), color = 'steelblue')\n\n\n\n\n\n\n\n\nIn order to access these residuals, we can ‚Äúaugment‚Äù the dataframe that we used in the model, using the broom::augment function call.\n\nmodel_aux_augmented &lt;- augment(model_aux)\n\nBecause the \\(Y\\) variable wasn‚Äôt included in the regression for model_aux we have to bring of over from the main dataset, which is a little bit‚Ä¶ um, hacky. Forgive these sins.\n\nmodel_aux_augmented$y &lt;- d$y\nmodel_aux_augmented\n\n# A tibble: 100 √ó 10\n      x1    x2    x3 .fitted .resid   .hat .sigma  .cooksd .std.resid     y\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;\n 1  6.22  2.65 5.52     4.90  1.32  0.0239   2.62 0.00215       0.513  25.3\n 2  7.21  5.70 9.51     5.53  1.68  0.0328   2.62 0.00481       0.653  41.6\n 3  4.44  5.41 3.21     4.83 -0.394 0.0142   2.62 0.000111     -0.152  22.8\n 4  8.45  7.52 8.32     5.53  2.93  0.0277   2.61 0.0123        1.14   45.4\n 5  2.31  7.52 4.70     5.13 -2.82  0.0163   2.61 0.00655      -1.09   28.8\n 6  6.36  4.82 9.53     5.48  0.880 0.0342   2.62 0.00139       0.343  41.5\n 7  5.57  7.98 1.57     4.83  0.743 0.0345   2.62 0.000999      0.290  23.5\n 8  9.97  4.51 9.28     5.43  4.55  0.0326   2.58 0.0351        1.77   45.5\n 9  4.14  8.98 8.16     5.60 -1.47  0.0386   2.62 0.00439      -0.573  43.8\n10  6.25  7.35 0.853    4.71  1.54  0.0368   2.62 0.00460       0.601  21.5\n# ‚Ñπ 90 more rows\n\n\nFinally, with this augmented data that has information from the model, we can estimate the model that includes only the residuals as predictors of \\(Y\\).\n\nmodel_two &lt;- lm(y ~ .resid, data = model_aux_augmented)\ncoef(model_two)\n\n(Intercept)      .resid \n 28.5096027   0.9996716 \n\n\nOur claim was that the coefficients from model_main and model_two should be the same.\n\ntest_that(\n  'the model coefficients are equal', \n  expect_equal(\n    as.numeric(coef(model_main)['x1']), \n    as.numeric(coef(model_two)['.resid']))\n)\n\nTest passed ü•≥\n\n\nBut, why is this an interesting, or at least useful, feature to appreciate?\nThis is actually a really famous, relatively recently ‚Äúrediscovered‚Äù proof. If we have a number of variables, one called an outcome and the rest called features, then we can estimate the relationship between the outcome and one feature in the following way:\n\nEstimate the relationship between all the other features and the one that we‚Äôre examining; save the information about the feature we‚Äôre examining which cannot be explained by the other features in some vector.\nRegress the outcome on this leftover information.\n\nSlightly more of what is happening? In the first model, the leftover information is orthogonal to the information posessed in the regression features. In the second model, we can use this orthagonal information to estimate the effect of one variable.",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>OLS Regression Estimates</span>"
    ]
  },
  {
    "objectID": "08-regression-estimation.html#coding-activityr-cheat-sheet",
    "href": "08-regression-estimation.html#coding-activityr-cheat-sheet",
    "title": "8¬† OLS Regression Estimates",
    "section": "8.7 Coding Activity:R Cheat Sheet",
    "text": "8.7 Coding Activity:R Cheat Sheet\nSuppose x and y are variables in dataframe d.\nTo fit an ols regression of Y on X:\nmod &lt;- lm(y ~ x, data = d)\nTo access coefficients from the model object:\nmod$coefficients\nor coef(mod)\nTo access fitted values from the model object:\nmod$fitted\nor fitted(mod)\nor predict(mod)\nTo access residuals from the model object:\nmod$residuals\nor resid(mod)\nTo create a scatterplot that includes the regression line:\nplot(d['x'], d['y'])\nabline(mod)\nor \nd %&gt;% \n  ggplot() + \n  aes(x = x, y = y) + \n  geom_point() + \n  geom_smooth(method = lm)",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>OLS Regression Estimates</span>"
    ]
  },
  {
    "objectID": "08-regression-estimation.html#r-exercise",
    "href": "08-regression-estimation.html#r-exercise",
    "title": "8¬† OLS Regression Estimates",
    "section": "8.8 R Exercise",
    "text": "8.8 R Exercise\nReal Estate in Boston\nThe file hprice1.Rdata contains 88 observations of homes in the Boston area, taken from the real estate pages of the Boston Globe during 1990. This data was provided by Wooldridge.\n\nload('data/hprice1.RData') # provides 3 objects \n\n\nhead(data)\n\n    price assess bdrms lotsize sqrft colonial   lprice  lassess llotsize\n1 300.000  349.1     4    6126  2438        1 5.703783 5.855359 8.720297\n2 370.000  351.5     3    9903  2076        1 5.913503 5.862210 9.200593\n3 191.000  217.7     3    5200  1374        0 5.252274 5.383118 8.556414\n4 195.000  231.8     3    4600  1448        1 5.273000 5.445875 8.433811\n5 373.000  319.1     4    6095  2514        1 5.921578 5.765504 8.715224\n6 466.275  414.5     5    8566  2754        1 6.144775 6.027073 9.055556\n    lsqrft\n1 7.798934\n2 7.638198\n3 7.225482\n4 7.277938\n5 7.829630\n6 7.920810\n\n\n\nAre there variables that would not be valid outcomes for an OLS regression? If so, why?\nAre there variables that would not be valid inputs for an OLS regression? If so, why?\n\n\n8.8.1 Assess the Relationship between Price and Square Footage\n\ndata %&gt;% \n  ggplot() + \n  aes(x=sqrft, y=price) + \n  geom_point()\n\n\n\n\n\n\n\n\nSuppose that you‚Äôre interested in knowing the relationship between price and square footage.\n\nAssess the assumptions of the Large-Sample Linear Model.\nCreate a scatterplot of price and sqrft. Like every plot you make, ensure that the plot minimally has a title and meaningful axes.\n\n\nFind the correlation between the two variables.\nRecall the equation for the slope of the OLS regression line ‚Äì here you can either use Variance and Covariance, or if you‚Äôre bold, the linear algebra. Compute the slope manually (without using lm())\n\n\nRegress price on sqrft using the lm function. This will produce an estimate for the following model:\n\n[ price = {0} + {1} sqrft + e ]\n\ndata %&gt;% \n  ggplot() + \n  aes(x=sqrft, y=lotsize) + \n  geom_point()\n\n\n\n\n\n\n\n\n\nCreate a scatterplot that includes the fitted regression.\n\n\nInterpret what the coefficient means.\n\n\nState what features you are allowing to change and what features you‚Äôre requiring do not change.\nFor each additional square foot, how much more (or less) is the house worth?\n\n\nEstimate a new model (and save it into another object) that includes the size of the lot and whether the house is a colonial. This will estimate the model:\n\n\\[\nprice = \\beta_{0} + \\beta_{1} sqrft + \\beta_{2} lotsize + \\beta_{3} colonial? + e\n\\]\n\nBUT BEFORE YOU DO, make a prediction: What do you think is going to happen to the coefficient that relates square footage and price?\n\nWill the coefficient increase, decrease, or stay the same?\n\n\n\nCompute the sample correlation between \\(X\\) and \\(e_i\\). What guarantees do we have from the book about this correlation? Does the data seem to bear this out?",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>OLS Regression Estimates</span>"
    ]
  },
  {
    "objectID": "08-regression-estimation.html#regression-plots-and-discussion",
    "href": "08-regression-estimation.html#regression-plots-and-discussion",
    "title": "8¬† OLS Regression Estimates",
    "section": "8.9 Regression Plots and Discussion",
    "text": "8.9 Regression Plots and Discussion\nIn this next set of notes, we‚Äôre going to give some data, displayed in plots, and we will try to apply what we have learned in the async and reading for this week to answer questions about each of the scatter plots.\n\n8.9.1 Plot 1\nConsider data that is generated according to the following function:\n\\[\n  Y = 1 + 2x_1 + 3x_2 + e,\n\\]\nwhere \\(x_1 \\sim N(0,2)\\), \\(x_2 \\sim N(0,2)\\) and \\(e\\) is a constant equal to zero.\nFrom this population, you might consider taking a sample of 100 observations, and representing this data in the following 3d scatter plot. In this plot, there are three dimensions, an \\(x_1, x_2\\), and \\(y\\) dimensions.\n\nknitr::include_app(url =\"http://www.statistics.wtf/minibeta01/\")\n\n\n\n\nRotate the cube and explore the data, looking at each face of the cube, including from the top down.\nOne of the lessons that we learned during the random variables section of the course is that every random variable that has been measured can also be marginalized off. You might think of this as ‚Äúcasting down‚Äù data from three dimensions, to only two.\nSketch the following 2d scatter plots, taking care the label your axes. You need not represent all 100 points, but rather create the gestalt of what you see.\n1. \\(Y = f(x_1)\\) (but not \\(x_2\\)) 2. \\(Y = f(x_2)\\) (but not \\(x_1\\)) 3. \\(x2 = f(x_1)\\)\nOnce you have sketched the scatter plots, what line would you fit that minimizes the sum of squared residuals in the vertical direction. Define a residual, \\(\\epsilon\\), to be the vertical distance between the line you draw, and the corresponding point on the input data.\nWhat is the average of the residuals for each of the lines that you have fitted? How does this correspond to the moment conditions discussed in the async? What would happen if you translated this line vertically?\nRotate the cube so that the points ‚Äúfall into line‚Äù. When you see this line, how does it help you describe the function that governs this data?",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>OLS Regression Estimates</span>"
    ]
  },
  {
    "objectID": "09-regression-inference.html",
    "href": "09-regression-inference.html",
    "title": "9¬† OLS Regression Inference",
    "section": "",
    "text": "9.1 Learning Objectives\nAfter this week‚Äôs learning, student will be able to",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>OLS Regression Inference</span>"
    ]
  },
  {
    "objectID": "09-regression-inference.html#learning-objectives",
    "href": "09-regression-inference.html#learning-objectives",
    "title": "9¬† OLS Regression Inference",
    "section": "",
    "text": "Describe how sampling based uncertainty is reflected in OLS regression parameter estimates.\nReport standard errors, and conduct tests for NHST of regression coefficients against zero.\nConduct a regression based analysis, on real data, in ways that begin to explore regression as a modeling tool.",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>OLS Regression Inference</span>"
    ]
  },
  {
    "objectID": "09-regression-inference.html#class-announcements",
    "href": "09-regression-inference.html#class-announcements",
    "title": "9¬† OLS Regression Inference",
    "section": "9.2 Class Announcements",
    "text": "9.2 Class Announcements\n\nCongratulations on finishing your first lab!\nThe next (and the last) lab is coming up in two weeks.\nHomework 09 has been assigned today, and it is due in a week.",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>OLS Regression Inference</span>"
    ]
  },
  {
    "objectID": "09-regression-inference.html#roadmap",
    "href": "09-regression-inference.html#roadmap",
    "title": "9¬† OLS Regression Inference",
    "section": "9.3 Roadmap",
    "text": "9.3 Roadmap\nRear-View Mirror\n\nStatisticians create a population model to represent the world.\nSometimes, the model includes an ‚Äúoutcome‚Äù random variable \\(Y\\) and ‚Äúinput‚Äù random variables \\(X_1, X_2,...,X_k\\).\nThe joint distribution of \\(Y\\) and \\(X_1, X_2,...,X_k\\) is complicated.\nThe best linear predictor (BLP) is the canonical way to summarize the relationship.\nOLS provides a point estimate of the BLP\n\nToday\n\nRobust Standard Error: quantify the uncertainty of OLS coefficients\nHypothesis testing with OLS coefficients\nBootstrapping\n\nLooking Ahead\n\nRegression is a foundational tool that can be applied to different contexts\nThe process of building a regression model looks different, depending on whether the goal is prediction, description, or explanation.",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>OLS Regression Inference</span>"
    ]
  },
  {
    "objectID": "09-regression-inference.html#uncertainty-in-ols",
    "href": "09-regression-inference.html#uncertainty-in-ols",
    "title": "9¬† OLS Regression Inference",
    "section": "9.4 Uncertainty in OLS",
    "text": "9.4 Uncertainty in OLS\n\n9.4.1 Discussion Questions\n\nList as many differences between the BLP and the OLS line as you can.\nIn the following regression table, explain in your own words what the standard error in parentheses means.\n\n\n\n\n\noutcome: sleep hours\n\n\n\n\nmg. melatonin\n0.52\n\n\n\n(0.31)",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>OLS Regression Inference</span>"
    ]
  },
  {
    "objectID": "09-regression-inference.html#understanding-uncertainty",
    "href": "09-regression-inference.html#understanding-uncertainty",
    "title": "9¬† OLS Regression Inference",
    "section": "9.5 Understanding Uncertainty",
    "text": "9.5 Understanding Uncertainty\nImagine three different regression models, each of the following form:\n\\[\n  Y = 0 + \\beta X + \\epsilon\n\\]\nThe only difference is in the error term. The conditional distribution is given by:\n\n\n\nModel\nDistribution of \\(\\epsilon\\) cond. on \\(X\\)\n\n\n\n\nA\nUniform on \\([-.5, +.5]\\)\n\n\nB\nUniform on \\([ - |X|, |X| ]\\)\n\n\nC\nUniform on \\([ -1 + |X|, 1- |X| ]\\)\n\n\n\nA is what we call a homoskedastic distribution. B and C are what we call heteroskedastic. Below, we define R functions that simulate draws from these three distributions.\n\nrA &lt;- function(n, slope=0){\n  x       = runif(n, min = -1,  max = 1)\n  epsilon = runif(n, min = -.5, max = .5)\n  y       = 0 + slope*x + epsilon\n  \n  return(\n    data.frame(x=x, y=y)\n    )\n}\n\nrB &lt;- function(n, slope=0){\n  x       = runif(n, min = -1, max = 1)\n  epsilon = runif(n, min = - abs(x), max =abs(x))\n  y       = 0 + slope*x + epsilon\n  \n  return(\n    data.frame(x=x,y=y)\n    )\n}\n\nrC &lt;- function(n, slope=0){\n  x       = runif(n, min = -1, max = 1)\n  epsilon = runif(n, min = -1 + abs(x), max = 1 - abs(x))\n  y       = 0 + slope*x + epsilon\n\n  return(\n    data.frame(x=x,y=y)\n    )\n}\n\n\ndata &lt;- rbind( \n  data.frame( rA(200), label = 'A'),\n  data.frame( rB(200), label = 'B'),\n  data.frame( rC(200), label = 'C'))\n\n\ndata %&gt;% \n  ggplot(aes(x=x, y=y)) + \n  geom_point() + \n  lims(\n    x = c(-2,2), \n    y = c(-1,1)) + \n  labs(title = 'Samples Drawn from Three Distributions') + \n  facet_grid(rows=vars(label))\n\n\n\n\n\n\n\n\n\n9.5.1 Question 1\nThe following code draws a sample from distribution A, fits a regression line, and plots it. Run it a few times to see what happens. Now explain how you would visually estimate the standard error of the slope coefficient. Why is this standard error important?\n\ndata &lt;-  rA(10, slope=0)\n\ndata %&gt;% \n  ggplot() + \n  aes(x=x, y=y) + \n  geom_point() + \n  geom_smooth(method='lm', formula = 'y ~ x', se=FALSE) + \n  lims(\n    x = c(-2,2), \n    y = c(-1,1)) + \n  labs(title = 'Regression Fit to Distribution A')\n\n\n\n\n\n\n\n\n\ndata_points &lt;- 200\n\nbase_plot_a &lt;- rA(10) %&gt;%  \n  ggplot() + \n  aes(x=x, y=y) + \n  geom_point() + \n  scale_x_continuous(limits = c(-3, 3))\n\nfor(i in 1:100) { \n    base_plot_a &lt;- base_plot_a + rA(data_points) %&gt;% \n      stat_smooth(\n        mapping = aes(x=x, y=y), \n        method  = 'lm',         se = FALSE, \n        formula = 'y~x', fullrange = TRUE,\n        color   = 'grey',    alpha = 0.5,\n        size    = 0.5\n      )\n}\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead.\n\nbase_plot_b &lt;- rB(10) %&gt;%  \n  ggplot() + \n  aes(x=x, y=y) + \n  geom_point() + \n  scale_x_continuous(limits = c(-3, 3))\n\nfor(i in 1:100) { \n    base_plot_b &lt;- base_plot_b + rB(data_points) %&gt;% \n      stat_smooth(\n        mapping = aes(x=x, y=y), \n        method  = 'lm',         se = FALSE, \n        formula = 'y~x', fullrange = TRUE,\n        color   = 'grey',    alpha = 0.5,\n        size    = 0.5\n      )\n  }\n\nbase_plot_c &lt;- rC(10) %&gt;%  \n  ggplot() + \n  aes(x=x, y=y) + \n  geom_point() + \n  scale_x_continuous(limits = c(-3, 3))\n\nfor(i in 1:100) { \n    base_plot_c &lt;- base_plot_c + rC(data_points) %&gt;% \n      stat_smooth(\n        mapping = aes(x=x, y=y), \n        method  = 'lm',         se = FALSE, \n        formula = 'y~x', fullrange = TRUE,\n        color   = 'grey',    alpha = 0.5,\n        size    = 0.5\n      )\n}\n\nbase_plot_a + base_plot_b + base_plot_c + patchwork::plot_layout(axes = \"collect\")\n\n\n\n\n\n\n\n?plot_layout()\n\n\n\n9.5.2 Question 2\nYou have a sample from each distribution, A, B, and C and you fit a regression of Y on X. Which will have the highest standard error for the slope coefficient? Which will have the lowest standard error? Why? (You may want to try experimenting with the function defined above)\n\n\n9.5.3 Question 3\nFor distribution A, perform a simulated experiment. Draw a large number of samples, and for each sample fit a linear regression. Store the slope coefficient from each regression in a vector. Finally, compute the standard deviation for the slope coefficients.\nRepeat this process for distributions B and C. Do the results match your intuition?",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>OLS Regression Inference</span>"
    ]
  },
  {
    "objectID": "09-regression-inference.html#understanding-uncertainty-1",
    "href": "09-regression-inference.html#understanding-uncertainty-1",
    "title": "9¬† OLS Regression Inference",
    "section": "9.6 Understanding Uncertainty",
    "text": "9.6 Understanding Uncertainty\nUnder the relatively stricter assumptions of constant error variance, the variance of a slope coefficient is given by\n\\[\n  V(\\hat{\\beta_j}) = \\frac{\\sigma^2}{SST_j (1-R_j^2)}\n\\]\n\nA similar formulation is given in FOAS as definition 4.2.3,\n\\[\n  \\hat{V}_{C}[\\hat{\\beta}] = \\hat{\\sigma}^2 \\left( X^{T} X \\right)^{-1} \\rightsquigarrow \\hat{\\sigma}^{2}{\\left(\\mathbb{X}^{T}\\mathbb{X}\\right)},\n\\] where \\(\\hat{\\sigma}^{2} = V[\\hat{\\epsilon}]\\)\n\nExplain why each term makes the variance higher or lower:\n\n\\(\\hat{\\sigma}^2\\) is the variance of the error \\(\\hat{\\epsilon}\\)\n\\(SST_j\\) is (unscaled) variance of \\(X_j\\)\n\\(R_j^2\\) is \\(R^2\\) for a regression of \\(X_j\\) on the other \\(X\\)‚Äôs",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>OLS Regression Inference</span>"
    ]
  },
  {
    "objectID": "09-regression-inference.html#r-exercise",
    "href": "09-regression-inference.html#r-exercise",
    "title": "9¬† OLS Regression Inference",
    "section": "9.7 R Exercise",
    "text": "9.7 R Exercise\nReal Estate in Boston\nThe file hprice1.RData contains 88 observations of homes in the Boston area, taken from the real estate pages of the Boston Globe during 1990. This data was provided by Wooldridge.\n\nload('data/hprice1.RData') # provides 3 objects \n\nLast week, we fit a regression of price on square feet.\n\nmodel_one &lt;- lm(price ~ sqrft, data = data)\nmodel_one$df.residual\n\n[1] 86\n\n\nCan you use the pieces that you‚Äôre familiar with to produce a p-value using robust standard errors?\n\nregression_p_value &lt;- function(model, variable) { \n  ## this function takes a model \n  ## and computes a test-statistic, \n  ## then compares that test-statistic against the \n  ## appropriate t-distribution\n  \n  ## you can use the following helper functions: \n  ##  - coef()\n  ##  - vcovHC()\n  df &lt;- model$df.residual\n  \n  # numerator   &lt;- 'fill this in'\n  # denominator &lt;- 'fill this in'\n  \n  numerator   &lt;- coef(model)[variable]\n  denominator &lt;- sqrt(diag(vcovHC(model)))[variable]\n  \n  test_stat_  &lt;- numerator / denominator\n  p_val_      &lt;- 'fill this in'\n  p_val_      &lt;- pt(test_stat_, df = df, lower.tail = FALSE) * 2\n  \n  return(p_val_)\n  }\n\nIf you want to confirm that what you have written is correct, you can compare against the value that you receive from the line below.\n\ncoeftest(model_one, vcov. = vcovHC(model_one))\n\n\nt test of coefficients:\n\n             Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept) 11.204145  39.450563  0.2840    0.7771    \nsqrft        0.140211   0.021111  6.6417 2.673e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\np_value_ &lt;- broom::tidy(coeftest(model_one, vcov. = vcovHC(model_one))) %&gt;% \n  filter(term == 'sqrft') %&gt;% \n  select('p.value') %&gt;% \n  as.numeric()\n\ntest_that(\n  'test that hand coded p-value is the same as the pre-rolled', \n  expect_equal(\n    object   = as.numeric(regression_p_value(model_one, 'sqrft')), \n    expected = p_value_\n  )\n)\n\nTest passed üéâ\n\n\nQuestions\n\nEstimate a new model (and save it into another object) that includes the size of the lot and whether the house is a colonial. This will estimate the model:\n\n\\[\n  price = \\beta_{0} + \\beta_{1} sqrft + \\beta_{2} lotsize + \\beta_{3} colonial? + e\n\\]\n\nBUT BEFORE YOU DO, make a prediction: What do you think is going to happen to the coefficient that relates square footage and price?\n\nWill the coefficient increase, decrease, or stay the same?\nWill the uncertainty about the coefficient increase, decrease, or stay the same?\nConduct an F-test that evaluates whether the model as a whole does better when the coefficients on colonial and lotsize are allowed to estimate freely, or instead are restricted to be zero (i.e.¬†\\(\\beta_{2} = \\beta_{3} = 0\\).\n\n\n\nUse the function vcovHC from the sandwich package to estimate (a) the the heteroskedastic consistent (i.e.¬†‚Äúrobust‚Äù) variance covariance matrix; and (b) the robust standard errors for the intercept and slope of this regression. Recall, what is the relationship between the VCOV and SE in a regression?\n\n\nPerform a hypothesis test to check whether the population relationship between sqrft and price is zero. Use coeftest() with the robust standard errors computed above.\n\n\nUse the robust standard error and qt to compute a 95% confidence interval for the coefficient sqrft in the second model that you estimated. \\(price = \\beta_{0} + \\beta_{1} sqrft + \\beta_{2} lotsize + \\beta_{3} colonial\\).\nBootstrap. The book very quickly talks about bootstrapping which is the process of sampling with replacement and fitting a model. The idea behind the bootstrap is that since the data is generated via an iid sample from the population, that you can simulate re-running your analysis by drawing repeated samples from the data that you have.\n\nBelow is code that will conduct a boostrapping estimator of the uncertainty of the sqrft variable when lotsize and colonial are included in the model.\n\nbootstrap_sqft &lt;- function(d = data, number_of_bootstraps = 1000) { \n  number_of_rows &lt;- nrow(d)\n\n    coef_sqft &lt;- rep(NA, number_of_bootstraps)\n\n    for(i in 1:number_of_bootstraps) { \n      bootstrap_data &lt;- d[sample(x=1:number_of_rows, size=number_of_rows, replace=TRUE), ]  \n      estimated_model &lt;- lm(price ~ sqrft, data = bootstrap_data)\n      coef_sqft[i]    &lt;- coef(estimated_model)['sqrft']\n    }\n  return(coef_sqft)\n}\n\n\nbootstrap_result &lt;- bootstrap_sqft(d = data, number_of_bootstraps = 10000)\n\nWith this, it is possible to plot the distribution of these regression coefficients:\n\nggplot() + \n  aes(x = bootstrap_result) + \n  geom_histogram() + \n  labs(\n    x = 'Estimated Coefficient', \n    y = 'Count', \n    title = 'Bootstrap coefficients for square footage'\n  )\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nCompute the standard deviation of the bootstrapped regression coefficients. How does this compare to the robust standard errors you computed above?\n\ncoeftest(model_one, vcov. = vcovHC(model_one))\n\n\nt test of coefficients:\n\n             Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept) 11.204145  39.450563  0.2840    0.7771    \nsqrft        0.140211   0.021111  6.6417 2.673e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsd(bootstrap_result)\n\n[1] 0.01954638",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>OLS Regression Inference</span>"
    ]
  },
  {
    "objectID": "10-descriptive-models.html",
    "href": "10-descriptive-models.html",
    "title": "10¬† Descriptive Model Building",
    "section": "",
    "text": "10.1 Learning Objectives",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Descriptive Model Building</span>"
    ]
  },
  {
    "objectID": "10-descriptive-models.html#learning-objectives",
    "href": "10-descriptive-models.html#learning-objectives",
    "title": "10¬† Descriptive Model Building",
    "section": "",
    "text": "Understand that models don‚Äôt know what they‚Äôre doing, and it is the role of the data scientist to control and deploy them.\nPractice transforming variables, on the fly, at the time of regression modeling in order to produce an effective, descriptive regression model.\nAppreciate that the random variable understanding of the world that we have constructed is useful for thinking of reshaping, or transforming spaces.",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Descriptive Model Building</span>"
    ]
  },
  {
    "objectID": "10-descriptive-models.html#class-announcements",
    "href": "10-descriptive-models.html#class-announcements",
    "title": "10¬† Descriptive Model Building",
    "section": "10.2 Class Announcements",
    "text": "10.2 Class Announcements\n\nThe Regression Lab begins next week.\n\nYour instructor will divide you into teams.\nAs part of the lab, you will perform a statistical analysis using linear regression models.",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Descriptive Model Building</span>"
    ]
  },
  {
    "objectID": "10-descriptive-models.html#roadmap",
    "href": "10-descriptive-models.html#roadmap",
    "title": "10¬† Descriptive Model Building",
    "section": "10.3 Roadmap",
    "text": "10.3 Roadmap\nRearview Mirror\n\nStatisticians create a population model to represent the world.\nThe BLP is a useful way to summarize the relationship between one outcome random variable \\(Y\\) and input random varibles \\(X_1,...,X_k\\)\nOLS regression is an estimator for the Best Linear Predictor (BLP)\nWe can capture the sampling uncertainty in an OLS regression with standard errors, and tests for model parameters.\n\nToday\n\nThe research goal determines the strategy for building a linear model.\nDescription means summarizing or representing data in a compact, human-understandable way.\nWe will capture complex relationships by transforming data, including using indicator variables and interaction terms.\n\nLooking Ahead\n\nWe will see how model building for explanation is different from building for description.\nThe famous Classical Linear Model (CLM) allows us to apply regression to smaller samples.",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Descriptive Model Building</span>"
    ]
  },
  {
    "objectID": "10-descriptive-models.html#discussion",
    "href": "10-descriptive-models.html#discussion",
    "title": "10¬† Descriptive Model Building",
    "section": "10.4 Discussion",
    "text": "10.4 Discussion\n\n10.4.1 Three modes of model building\n\nRecall the three major modes of model building: Prediction, Description, Explanation.\nWhat is the appropriate mode for each of the following questions?\n\n\nWhat is going on?\nWhy is something going on?\nWhat is going to happen?\n\n\nThink of a research question you are interested in. Which mode is it aligned with?\n\n\n\n10.4.2 The statistical modeling process in different modes\n\nHow does the modeling goal influence each of the following steps in the statistical modeling process?\n\nChoice of variables and transformation\nChoice of model (ols regression, neural nets, random forest, etc.)\nModel evaluation",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Descriptive Model Building</span>"
    ]
  },
  {
    "objectID": "10-descriptive-models.html#r-activity-measuring-the-return-to-education",
    "href": "10-descriptive-models.html#r-activity-measuring-the-return-to-education",
    "title": "10¬† Descriptive Model Building",
    "section": "10.5 R Activity: Measuring the return to education",
    "text": "10.5 R Activity: Measuring the return to education\n\nIn labor economics, a key concept is returns to education.\n\nOur goal is description: what is the relationship between education and wages? We will proceed in two steps:\n\nFirst, we will discuss what the appropriate specifications are.\nThen we will estimate the different models to answer this question.\n\nWe will use wage1 dataset in the wooldridge package in the following sections.\n\n\nwage1 &lt;- wooldridge::wage1\n#names(wage1)\n\nwage1 |&gt; \n  ggplot() + \n  aes(x=wage) + \n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n10.5.1 Transformations\n\n10.5.1.1 Applying and Interpreting Logarithms\n\nWhich of the following specifications best capture the relationship between education and hourly wage? (Hint: Do a quick a EDA)\n\nlevel-level: \\(wage = \\beta_0 + \\beta_1 educ + u\\)\nLevel-log: \\(wage = \\beta_0 + \\beta_1 \\ln(educ)  + u\\)\nlog-level: \\(\\ln(wage) = \\beta_0 + \\beta_1 educ + u\\)\nlog-log: \\(\\ln(wage) = \\beta_0 + \\beta_1 \\ln(educ) + u\\)\n\nWhat is the interpretation of \\(\\beta_0\\) and \\(\\beta_1\\) in your selected specification?\nCan we use \\(R^2\\) or Adjusted \\(R^2\\) to choose between level-level or log-level specifications?\n\nRemember\n\nDoing a log transformation for any reason essentially implies a fundamentally different relationship between outcome (Y) and predictor (X) that we need to capture\n\n\n\n10.5.1.2 Applying and Interpreting Polynomials\n\nThe following specifications include two control variables: years of experience (exper) and years at current company (tenure).\nDo a quick EDA and select the specification that better suits our description goal.\n\n\\(wage = \\beta_0 + \\beta_1 educ + \\beta_2 exper + \\beta_3 tenure + u\\)\n\\(\\begin{aligned}\nwage &= \\beta_0 + \\beta_1 educ + \\beta_2 exper + \\beta_3 exper^2 + \\\\\n& \\beta_4 tenure + \\beta_5 tenure^2 + u\n\\end{aligned}\\)\n\nHow do you interpret the \\(\\beta\\) coefficients?\n\n\n\n10.5.1.3 Applying and Interpreting Indicator variables and interaction terms\n\nIn the following models, first, explain why the indicator variables or interaction terms have been included. Then identify the reference group (if any) and interpret all coefficients.\n\n\\(wage = \\beta_0 + \\beta_1 educ + \\beta_2 I(educ \\geq 12) + u\\)\n\\(wage = \\beta_0 + \\beta_1 educ + \\beta_2 female + u\\)\n\\(wage = \\beta_0 + \\beta_1 educ + \\beta_2 female + \\beta_3 educ*female + u\\)\n\\(\\begin{aligned}\nwage &= \\beta_0 + \\beta_1 female + \\beta_2 I(educ = 2) + \\beta_3 I(educ = 3)\\\\\n&...+ \\beta_{20} I(educ = 20) + u\\\\\n\\end{aligned}\\)\n\n\n\n\n\n10.5.2 Estimation\nEstimating Returns to Education\n\nAnswer the following questions using an appropriate hypothesis test.\n\nIs a year of education associated with changes to hourly wage? (Include experience and tenure without polynomial terms).\nIs the association between wage and experience / wage and tenure non-linear?\nIs there evidence for gender wage discrimination in the U.S.?\nIs there any evidence for a graduation effect on wage?\n\nDisplay all estimated models in a regression table, and discuss the robustness of your results.",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Descriptive Model Building</span>"
    ]
  },
  {
    "objectID": "11-causal-models.html",
    "href": "11-causal-models.html",
    "title": "11¬† Explanatory Model Building",
    "section": "",
    "text": "11.1 Learning Objectives\nAt the end of this week‚Äôs learning, students will be able to",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Explanatory Model Building</span>"
    ]
  },
  {
    "objectID": "11-causal-models.html#learning-objectives",
    "href": "11-causal-models.html#learning-objectives",
    "title": "11¬† Explanatory Model Building",
    "section": "",
    "text": "Remember that most interesting questions in their data analysis are actually causal questions.\nArticulate a particular causal model that describes the world, and evaluate whether a research design and a statistical analysis does an adequate job answering a question about a causal model.\nAppreciate the deep difficulty of causal questions, and how research design guides data collection.",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Explanatory Model Building</span>"
    ]
  },
  {
    "objectID": "11-causal-models.html#class-announcements",
    "href": "11-causal-models.html#class-announcements",
    "title": "11¬† Explanatory Model Building",
    "section": "11.2 Class Announcements",
    "text": "11.2 Class Announcements\nLab 2-Regression\nOverview\n\nSetting: You are data scientists for a maker of products.\nTask: You select your own research question\n\nYour X should be an aspect of product design\nYour Y should be a metric of product success\n\nDeliverable: A statistical analysis that includes\n\nAn introduction that motivates your research question\nA description of your model-building process\nA discussion of statistical assumptions that may be problematic\nA well-formatted regression table with a minimum of 3 specifications\nA conclusion that extracts key lessons from your statistical results\n\n\nThe Report\n- Writing for a collaborating data scientist, what research question have you asked, what answers have you found, and how did you find them?\n\n\n\nDeliverable Name\nWeek Due\nGrade Weight\n\n\n\n\nResearch Proposal\nWeek 12\n10%\n\n\nWithin-Team Review\nWeek 14\n5%\n\n\nFinal Presentation\nWeek 14\n10%\n\n\nFinal Report\nWeek 14\n75%\n\n\n\nTeam Work Evaluation\n\nMost data science work happens on teams.\nOur educational goals include helping you improve in your role as a teammate.\nWe‚Äôll ask you to fill out a confidential evaluation regarding your team dynamics.\n\nFinal Presentation\n\nTeam will present their work in live session 14.\n\nTeams have between 10-15 min dedicated to discussing their work (depending on section size)\nTwo-thirds of the time can be the team presenting\nBUT at least one-third should be asking and answering questions with your peers\nFor example, if teams have 15 minutes total, then plan to present for no more than 10 minutes and structure 5 minutes of questions.",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Explanatory Model Building</span>"
    ]
  },
  {
    "objectID": "11-causal-models.html#roadmap",
    "href": "11-causal-models.html#roadmap",
    "title": "11¬† Explanatory Model Building",
    "section": "11.3 Roadmap",
    "text": "11.3 Roadmap\nRearview Mirror\n\nStatisticians create a population model to represent the world.\nThe BLP is a useful way to summarize relationships in a model, and OLS regression is a way to estimate the BLP.\nOLS regression is a foundational tool that can be applied to questions of description\n\nToday\n\nQuestions of explanation require a substantially different modeling process.\nTo answer causal questions, we must work within a causal theory\nOLS regression is sometimes appropriate for measuring a causal effect,\nBut, only when the model estimated matches the causal theory.\nSo, we must watch out for omitted variable bias, reverse causality, and outcome variables on the right hand side.\n\nLooking Ahead\n\nThe famous Classical Linear Model (CLM) allows us to apply regression to smaller samples.\nWe will address the pervasive issue of false discovery, and ways to be a responsible member of the scientific community.",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Explanatory Model Building</span>"
    ]
  },
  {
    "objectID": "11-causal-models.html#discussion",
    "href": "11-causal-models.html#discussion",
    "title": "11¬† Explanatory Model Building",
    "section": "11.4 Discussion",
    "text": "11.4 Discussion\n\n11.4.1 Path Diagrams\n\\[\n\\begin{matrix}\n\\\\\n\\text{Sleep} \\rightarrow \\text{Feelings of Stress} \\\\\n\\\\\n\\end{matrix}\n\\]\n\nHow would the following fit into this causal path diagram?\n\nAll the other factors in the world that also cause stress but don‚Äôt have a causal relationship with sleep.\nA factor: Coffee Intake\n\nWhat happens if you omit it in your regression?\n\nReverse causality\nAn outcome variable on the RHS: Job Performance\n\nWhat happens if you include it in your regression?",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Explanatory Model Building</span>"
    ]
  },
  {
    "objectID": "11-causal-models.html#an-interlude",
    "href": "11-causal-models.html#an-interlude",
    "title": "11¬† Explanatory Model Building",
    "section": "11.5 An Interlude",
    "text": "11.5 An Interlude\n\n\n11.5.1 Omitted Variable Bias\n\nRecall the equation for omitted variable bias\n\n\n\nWhat specific regressions do \\(\\beta_2\\) and \\(\\gamma_1\\) come from?",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Explanatory Model Building</span>"
    ]
  },
  {
    "objectID": "11-causal-models.html#r-exercise",
    "href": "11-causal-models.html#r-exercise",
    "title": "11¬† Explanatory Model Building",
    "section": "11.6 R Exercise",
    "text": "11.6 R Exercise\n\n11.6.1 Omitted Variable Bias in R\nThe file htv.RData contains data from the 1991 National Longitudinal Survey of Youth, provided by Wooldridge. All people in the sample are males age 26 to 34. The data is interesting here, because it includes education, stored in the variable educ, and also a score on an ability test, stored in the variable abil.\n\nload('./data/htv.RData')\n\ndata &lt;- data |&gt;  \n  rename(\n    ability    = abil, \n    education  = educ, \n    north_east = ne, \n    north_cent = nc, \n    potential_experience = exper, \n    edu_mother = motheduc, \n    edu_father = fatheduc, \n    divorce_14 = brkhme14, \n    siblings   = sibs, \n    tuition_17 = tuit17, \n    tuition_18 = tuit18) |&gt;  \n  mutate(\n    education_f = cut(education, breaks = c(0,12,16,100))) |&gt; \n  select(-c(ctuit, expersq, lwage))\n  \nglimpse(data)\n\nRows: 1,230\nColumns: 21\n$ wage                 &lt;dbl&gt; 12.019231, 8.912656, 15.514334, 13.333333, 11.070‚Ä¶\n$ ability              &lt;dbl&gt; 5.0277381, 2.0371704, 2.4758952, 3.6092398, 2.636‚Ä¶\n$ education            &lt;int&gt; 15, 13, 15, 15, 13, 18, 13, 12, 13, 12, 12, 12, 1‚Ä¶\n$ north_east           &lt;int&gt; 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1‚Ä¶\n$ north_cent           &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ west                 &lt;int&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0‚Ä¶\n$ south                &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ potential_experience &lt;int&gt; 9, 8, 11, 6, 15, 8, 13, 14, 9, 9, 13, 14, 4, 8, 7‚Ä¶\n$ edu_mother           &lt;int&gt; 12, 12, 12, 12, 12, 12, 13, 12, 10, 14, 9, 12, 17‚Ä¶\n$ edu_father           &lt;int&gt; 12, 10, 16, 12, 15, 12, 12, 12, 12, 12, 10, 16, 1‚Ä¶\n$ divorce_14           &lt;int&gt; 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0‚Ä¶\n$ siblings             &lt;int&gt; 1, 4, 2, 1, 2, 2, 5, 4, 3, 1, 2, 1, 1, 3, 2, 2, 1‚Ä¶\n$ urban                &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1‚Ä¶\n$ ne18                 &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1‚Ä¶\n$ nc18                 &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ south18              &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ west18               &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ urban18              &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1‚Ä¶\n$ tuition_17           &lt;dbl&gt; 7.582914, 8.595144, 7.311346, 9.499537, 7.311346,‚Ä¶\n$ tuition_18           &lt;dbl&gt; 7.260242, 9.499537, 7.311346, 10.162070, 7.311346‚Ä¶\n$ education_f          &lt;fct&gt; \"(12,16]\", \"(12,16]\", \"(12,16]\", \"(12,16]\", \"(12,‚Ä¶\n\n\n\nwage_plot &lt;- data |&gt;  \n  ggplot() + \n  aes(x=wage, fill=education_f) + \n  geom_histogram(bins=30)\nability_plot &lt;- data |&gt;  \n  ggplot() + \n  aes(x=ability, fill=education_f) + \n  geom_histogram(bins=30)\nwage_by_ability_plot &lt;- data |&gt;  \n  ggplot() + \n  aes(x=ability, y=wage, color=education_f) + \n  geom_point()\n  \n\n(wage_plot | ability_plot) / \n  wage_by_ability_plot\n\n\n\n\n\n\n\n\nAssume that the true model is,\n\n\n\n11.6.2 Questions:\n\nAre we able to directly measure ability? If so, how would you propose to measure it?\nIf not, what do we measure and how is this measurement related to ability? And there is a lot of evidence to suggest that standardized tests are not a very good proxy. But for now, let‚Äôs pretend that we really are measuring ability.\nUsing R, estimate (a) the true model, and (b) the regression of ability on education.\nWrite down the expression for what omitted variable bias would be if you couldn‚Äôt measure ability.\n\nAdd this omitted variable bias to the coefficient for education to see what it would be.\nNow evaluate your previous result by fitting the model, \\[wage = \\alpha_0 + \\alpha_1 educ + w\\]\nDoes the coefficient for the relationship between education and wages match what you estimated earlier?\nWhy or why not?\nReflect on your results:\nWhat does the direction of omitted variable bias suggest about OLS estimates of returns to education?\n\nWhat does this suggest about the reported statistical significance of education?",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Explanatory Model Building</span>"
    ]
  },
  {
    "objectID": "11-causal-models.html#research-design-strategies",
    "href": "11-causal-models.html#research-design-strategies",
    "title": "11¬† Explanatory Model Building",
    "section": "11.7 Research Design Strategies",
    "text": "11.7 Research Design Strategies\nHopefully you feel like, ‚ÄúGolly. It would be really, really hard to assert some causal model and know that it is actually true.‚Äù How does this lead you to think about the role of research design in setting up your data collection?\n\n\nIf you could do the experiment to determine the effect of education on wages, how would you do it?\nIf you cannot do the experiment to determine the effect of education on wages, what are some options for where to look for data? What would you hope these areas provide to you?",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Explanatory Model Building</span>"
    ]
  },
  {
    "objectID": "11-causal-models.html#discussion-1",
    "href": "11-causal-models.html#discussion-1",
    "title": "11¬† Explanatory Model Building",
    "section": "11.8 Discussion",
    "text": "11.8 Discussion\nThe Direction of Omitted Variable Bias\n\nFor each regression, estimate whether omitted variable bias is towards zero or away from zero.\n\n\n\n\n\n\n\n\nRegression Output\nOmitted Variable\n\n\n\n\n\\(\\widehat{grade} = 72.1 + 0.4\\ attendance\\)\n\\(time\\_studying\\)\n\n\n\\(\\widehat{lifespan} = 87.4 - 1.2\\ cigarettes\\)\n\\(exercise\\)\n\n\n\\(\\widehat{lifespan} = 87.4 - 1.2\\ cigarettes\\)\n\\(time\\_socializing\\)\n\n\n\\(\\widehat{wage} = 14.0 + 2.1\\ grad\\_education\\)\n\\(experience\\)\n\n\n\\(\\widehat{wage} = 14.0 + 2.1\\ grad\\_education\\)\ndesire to effect \\(social\\_good\\)\n\n\n\\(\\widehat{literacy} = 54 + 12\\ network\\_access\\)\n\\(wealth\\)",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Explanatory Model Building</span>"
    ]
  },
  {
    "objectID": "12-classical-linear-model.html",
    "href": "12-classical-linear-model.html",
    "title": "12¬† The Classical Linear Model",
    "section": "",
    "text": "12.1 Learning Objectives\nAt the end of this week‚Äôs learning students will be able to",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>The Classical Linear Model</span>"
    ]
  },
  {
    "objectID": "12-classical-linear-model.html#learning-objectives",
    "href": "12-classical-linear-model.html#learning-objectives",
    "title": "12¬† The Classical Linear Model",
    "section": "",
    "text": "Describe the assumptions of the classical linear model (sometimes referred to as the Gauss-Markov Assumptions) and what each assumption contributes to the estimator.\nEvaluate using empirical methods, whether each of the assumptions are likely to be true of the population data generating function.\nAssess whether the guarantees that are provided by the classical linear model‚Äôs requirements are likely to ever be true, including within data the student is likely to encounter.",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>The Classical Linear Model</span>"
    ]
  },
  {
    "objectID": "12-classical-linear-model.html#class-announcements",
    "href": "12-classical-linear-model.html#class-announcements",
    "title": "12¬† The Classical Linear Model",
    "section": "12.2 Class Announcements",
    "text": "12.2 Class Announcements\n\nLab 2 Deliverable and Dates\n\nResearch Proposal (Today)\nWithin-Team Review (Week 14)\nFinal Report (Week 14)\nFinal Presentation (Week 14)",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>The Classical Linear Model</span>"
    ]
  },
  {
    "objectID": "12-classical-linear-model.html#roadmap",
    "href": "12-classical-linear-model.html#roadmap",
    "title": "12¬† The Classical Linear Model",
    "section": "12.3 Roadmap",
    "text": "12.3 Roadmap\nRearview Mirror\n\nStatisticians create a population model to represent the world.\nThe BLP is a useful summary for a relationship among random variables.\nOLS regression is an estimator for the Best Linear Predictor (BLP).\nFor a large sample, we only need two mild assumptions to work with OLS\n\nTo know coefficients are consistent\nTo have valid standard errors, hypothesis tests\n\n\nToday\n\nThe Classical Linear Model (CLM) allows us to apply regression to smaller samples.\nThe CLM requires more to be true of the data generating process, to make coefficients, standard errors, and tests meaningful in small samples.\nUnderstanding if the data meets these requirements (often called assumptions) requires considerable care.\n\nLooking Ahead\n\nThe CLM ‚Äì and the methods that we use to evaluate the CLM ‚Äì are the basis of advanced models (inter alia time-series)\n(Week 13) In a regression studies (and other studies), false discovery is a widespread problem. Understanding its causes can make you a better member of the scientific community.",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>The Classical Linear Model</span>"
    ]
  },
  {
    "objectID": "12-classical-linear-model.html#the-classical-linear-model",
    "href": "12-classical-linear-model.html#the-classical-linear-model",
    "title": "12¬† The Classical Linear Model",
    "section": "12.4 The Classical Linear Model",
    "text": "12.4 The Classical Linear Model\nComparing the Large Sample Model and the CLM\n\n12.4.1 Part 1\n\nWe say that in small samples, more needs be true of our data for OLS regression to ‚Äúwork.‚Äù\n\nWhat do we mean when we say ‚Äúwork‚Äù?\n\nIf our goals are descriptive, how is a ‚Äúworking‚Äù estimator useful?\nIf our goals are explanatory, how is a ‚Äúworking‚Äù estimator useful?\nIf our goals are predictive, are the requirements the same?\n\n\n\n\n\n12.4.2 Part 2\n\n\nSuppose that you‚Äôre interested in understanding how subsidized school meals benefit under-resourced students in San Francisco East Bay region.\n\nUsing the tools from DATASCI 201, refine this question to a data science question.\nSuppose that there exists two possible data sources to answer the question you have formed:\n\nA large amount (e.g.¬†10,000 data points) of individual-level data about income, nutrition and test scores, self-reported by individual families who have opted in to the study.\n\nA relatively smaller amount (e.g.¬†500 data points) of Government data about school district characteristics, including district-level college achievement; county-level home prices, and state-level tax receipts.\n\n\nWhat are the tradeoffs to using one or the other data source?\n\n\n\n\n\n12.4.3 Part 3\n\n\nSuppose you elect to use the relatively larger sample of individual-level data.\n\nWhich of the large-sample assumptions do you expect are valid, and which are problematic?\n\nOr, suppose that you elect to use the relatively smaller sample of school-district-level data.\n\nWhich of the CLM assumptions do you expect are valid, and which do you expect are most problematic?\n\nWhat was the research question that you identified?\nWhat would a successful answer accomplish?\n\n\n\n\n12.4.4 Part 4\n\n\nWhich data source, the individual or the district-level, do you think is more likely to produce a successful answer?\n\n\n\n\n12.4.5 Part 5\nProblems with the CLM Requirements\n\nThere are five requirements for the CLM\n\nIID Sampling\nLinear Conditional Expectation\nNo Perfect Collinearity\nHomoskedastic Errors\nNormally Distributed Errors\n\nFor each of these requirements:\n\nIdentify one concrete way that the data might not satisfy the requirement.\nIdentify what the consequence of failing to satisfy the requirement would be.\nIdentify a path forward to satisfy the requirement.",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>The Classical Linear Model</span>"
    ]
  },
  {
    "objectID": "12-classical-linear-model.html#r-exercise",
    "href": "12-classical-linear-model.html#r-exercise",
    "title": "12¬† The Classical Linear Model",
    "section": "12.5 R Exercise",
    "text": "12.5 R Exercise\n\nlibrary(tidyverse)\nlibrary(wooldridge)\nlibrary(car)\nlibrary(lmtest)\nlibrary(sandwich)\nlibrary(stargazer)\n\nIf you haven‚Äôt used the mtcars dataset, you haven‚Äôt been through an intro applied stats class!\nIn this analysis, we will use the mtcars dataset which is a dataset that was extracted from the 1974 Motor Trend US magazine, and comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973-74 models). The dataset is automatically available when you start R.\nFor more information about the dataset, use the R command: help(mtcars)\n\ndata(mtcars)\nglimpse(mtcars)\n\nRows: 32\nColumns: 11\n$ mpg  &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,‚Ä¶\n$ cyl  &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,‚Ä¶\n$ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16‚Ä¶\n$ hp   &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180‚Ä¶\n$ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,‚Ä¶\n$ wt   &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.‚Ä¶\n$ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18‚Ä¶\n$ vs   &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,‚Ä¶\n$ am   &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,‚Ä¶\n$ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,‚Ä¶\n$ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,‚Ä¶\n\n\n\n12.5.1 Questions:\n\nUsing the mtcars data, quickly reason about the variables that we‚Äôre interested in studying:\n\n\nmtcars %&gt;% \n  ggplot() + \n  aes(x=mpg) +\n  geom_histogram(bins=10)\n\n\n\n\n\n\n\nmtcars %&gt;% \n  select(mpg, disp, hp, wt, drat) %&gt;% \n  pairs(pch=19)\n\n\n\n\n\n\n\n\n\nUsing the mtcars data, run a linear regression to find the relationship between miles per gallon (mpg) on the left-hand-side as a function of displacement (disp), gross horsepower (hp), weight (wt), and rear axle ratio (drat) on the right-hand-side. That is, fit a regression of the following form:\n\n\\[\n\\widehat{mpg} = \\hat{\\beta_{0}} + \\hat{\\beta}_{1} disp + \\hat{\\beta}_{2}horse\\_power + \\hat{\\beta}_{3}weight + \\hat{\\beta}_{4}drive\\_ratio\n\\]\n\nFor each of the following CLM assumptions, assess whether the assumption holds. Where possible, demonstrate multiple ways of assessing an assumption. When an assumption appears violated, state what steps you would take in response.\n\nI.I.D. data\nLinear conditional expectation\nNo perfect collinearity\nHomoskedastic errors\nNormally distributed errors\n\n\n\n# goal:\n# consequence if violated:\n\n\n# goal:\n# consequence if violated:\n\n\n# goal:\n# consequence if violated:\n\n\n# goal:\n# consequence if violated:\n\n\n# goal:\n# consequence if violated:\n\n\nIn addition to the above, assess to what extent (imperfect) collinearity is affecting your inference.\nInterpret the coefficient on horsepower.\nPerform a hypothesis test to assess whether rear axle ratio has an effect on mpg. What assumptions need to be true for this hypothesis test to be informative? Are they?\nChoose variable transformations (if any) for each variable, and try to better meet the assumptions of the CLM (which also maintaining the readability of your model).\n(As time allows) report the results of both models in a nicely formatted regression table.",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>The Classical Linear Model</span>"
    ]
  },
  {
    "objectID": "13-reproducible-research.html",
    "href": "13-reproducible-research.html",
    "title": "13¬† Reproducible Research",
    "section": "",
    "text": "13.1 Learning Objectives",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Reproducible Research</span>"
    ]
  },
  {
    "objectID": "13-reproducible-research.html#class-announcements",
    "href": "13-reproducible-research.html#class-announcements",
    "title": "13¬† Reproducible Research",
    "section": "13.2 Class Announcements",
    "text": "13.2 Class Announcements",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Reproducible Research</span>"
    ]
  },
  {
    "objectID": "13-reproducible-research.html#roadmap",
    "href": "13-reproducible-research.html#roadmap",
    "title": "13¬† Reproducible Research",
    "section": "13.3 Roadmap",
    "text": "13.3 Roadmap\nRearview Mirror\nToday\nLooking Ahead",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Reproducible Research</span>"
    ]
  },
  {
    "objectID": "13-reproducible-research.html#what-data-science-hopes-to-accomplish",
    "href": "13-reproducible-research.html#what-data-science-hopes-to-accomplish",
    "title": "13¬† Reproducible Research",
    "section": "13.4 What data science hopes to accomplish",
    "text": "13.4 What data science hopes to accomplish\n\nAs a data scientist, our goal is to learn about the world:\n\nTheorists and theologians build systems of explanations that are consistent with themselves\nAnalysts build systems of explanations that are consistent with the past\nScientists build systems of explanations that usefully predict events, or data, that hasn‚Äôt yet been seen",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Reproducible Research</span>"
    ]
  },
  {
    "objectID": "13-reproducible-research.html#learning-from-data",
    "href": "13-reproducible-research.html#learning-from-data",
    "title": "13¬† Reproducible Research",
    "section": "13.5 Learning from Data",
    "text": "13.5 Learning from Data\n\nAs a data scientist, the way we learn about the world is through the streams of data that real world events produce\n\nMachine processes\nPolitical outcomes\nCustomer actions\n\nThe watershed moment in our field has been the profusion of data available, from many places, that is richer than at any other point in our past.\n\nIn 251, and 266 we place structure on data series like audio, video and text that are transcendently rich\nIn 261 we bring together flows of data that are generated at massive scales\nIn 209 we ask, ‚ÄúHow can we take data, and produce a new form of it that is most effectively understood by the human visual and interactive mind?",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Reproducible Research</span>"
    ]
  },
  {
    "objectID": "13-reproducible-research.html#data-science-and-statistics",
    "href": "13-reproducible-research.html#data-science-and-statistics",
    "title": "13¬† Reproducible Research",
    "section": "13.6 Data Science and Statistics",
    "text": "13.6 Data Science and Statistics\n\nSo why statistics?\nAnd why the way we‚Äôve chosen to approach statistics in 203?",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Reproducible Research</span>"
    ]
  },
  {
    "objectID": "13-reproducible-research.html#why-statistics-a-closing-argument-for-statistics",
    "href": "13-reproducible-research.html#why-statistics-a-closing-argument-for-statistics",
    "title": "13¬† Reproducible Research",
    "section": "13.7 Why Statistics?: A Closing Argument for Statistics",
    "text": "13.7 Why Statistics?: A Closing Argument for Statistics\n\nBusiness, policy, education and medical decisions are made by humans based on data\nA central task when we observe some pattern in data is to infer whether the pattern will occur in some novel context\nStatistics, as we practice it in 203, allows us to characterize:\n\nWhat we have seen\nWhat we could have seen\nWhether any guarantees exist about what we have seen\nWhat we can infer about the population\n\nSo that we can either describe, explain or predict behavior.",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Reproducible Research</span>"
    ]
  },
  {
    "objectID": "13-reproducible-research.html#course-goals",
    "href": "13-reproducible-research.html#course-goals",
    "title": "13¬† Reproducible Research",
    "section": "13.8 Course Goals",
    "text": "13.8 Course Goals\n\n13.8.1 Course Section III: Purpose-Driven Models\n\nStatistical models are unknowing transformations of data\n\nBecause they‚Äôre built on the foundation of probability, we have certain guarantees what a model ‚Äúsays‚Äù\nBecause they‚Äôre unknowing, the models themselves know-not what they say.\n\nAs the data scientist, bring them alive to achieve our modeling goals\nIn Lab 2 we have expanded our ability to parse the world using regression, built a model that accomplishes our goals, and done so in a way that brings the ability to test under a ‚Äúnull‚Äù scenario\n\nKey insight: regression is little more than conditional averages\n\n\n\n\n13.8.2 Course Section II: Sampling Theory and Testing\n\nUnder very general assumptions, sample averages follow a predictable, known, distribution ‚Äì the Gaussian distribution\nThis is true, even when the underlying probability distribution is very complex, or unknown!\nDue to this common distribution, we can produce reliable, general tests!\nIn Lab 1 we computed simple statistics, and used guarantees from sampling theory to test whether these differences were likely to arise under a ‚Äúnull‚Äù scenario\n\n\n\n13.8.3 Course Section I: Probability Theory\n\nProbability theory\n\nUnderlies modeling and regression (Part III);\nUnderlies sampling, inference, and testing (Part II)\nEvery model built in every corner of data science\n\n\nWe can:\n\nModel the complex world that we live in using probability theory;\nMove from a probability density function that is defined in terms of a single variable, into a function that is defined in terms of many variables\nCompute useful summaries ‚Äì i.e.¬†the BLP, expected value, and covariance ‚Äì even with highly complex probability density functions.\n\n\n\n13.8.4 Statistics as a Foundation for MIDS\n\nIn w203, we hope to have laid a foundation in probability that can be used not only in statistical applications, but also in every other machine learning application that are likely to ever encounter",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Reproducible Research</span>"
    ]
  },
  {
    "objectID": "13-reproducible-research.html#reproducibility-discussion",
    "href": "13-reproducible-research.html#reproducibility-discussion",
    "title": "13¬† Reproducible Research",
    "section": "13.9 Reproducibility Discussion",
    "text": "13.9 Reproducibility Discussion\nGreen Jelly Beans\n\nWhat went wrong here?\n\n13.9.1 Discussion\nStatus Update You have a dataset of the number of Facebook status updates by day of the week. You run 7 different t-tests, one for posts on Monday (versus all other days), or for Tuesday (versus all other days), etc. Only the test for Sunday is significant, with a p-value of .045, so you throw out the other tests.\nShould you conclude that Sunday has a significant effect on number of posts? (How can you address this situation responsibly when you publish your results?)\nSuch Update As before, you have a dataset of the number of Facebook status updates by day of the week. You do a little EDA and notice that Sunday seems to have more ‚Äústatus updates‚Äù than all other days, so you recode your ‚Äúday of the week‚Äù variable into a binary one: Sunday = 1, All other days = 0. You run a t-test and get a p-value of .045. Should you conclude that Sunday has a significant effect on number of posts?\nSunday Funday Suppose researcher A tests if Monday has an effect (versus all other days), Researcher B tests Tuesday (versus all other days), and so forth. Only Researcher G, who tests Sunday finds a significant effect with a p-value of .045. Only Researcher G gets to publish her work. If you read the paper, should you conclude that Sunday has a significant effect on number of posts?\nSunday Repentence What if researcher G above is a sociologist that chooses to measure the effect of Sunday based on years of observing the way people behave on weekends? Researcher G is not interested in the other tests, because Sunday is the interesting day from her perspective, and she wouldn‚Äôt expect any of the other tests to be significant.\nDecreasing Effect Sizes Many observers have noted that as studies yielding statistically significant results are repeated, estimated effect sizes go down and often become insignificant. Why is this the case?",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Reproducible Research</span>"
    ]
  }
]