[["index.html", "Statistics for Data Science Live Session Introduction", " Statistics for Data Science UC Berkeley, School of Information 2022-01-13 Live Session Introduction This is the live session work space for the course. Our goal with this repository, is that we’re able to communicate ahead of time our aims for each week, and that you can prepare accordingly. "],["blooms-taxonomy.html", "Bloom’s Taxonomy", " Bloom’s Taxonomy An effective rubric for student understanding is attributed to Bloom (1956). Referred to as Bloom’s Taxonomy, this proposes that there is a hierarchy of student understanding; that a student may have one level of reasoning skill with a concept, but not another. The taxonomy proposes to be ordered: some levels of reasoning build upon other levels of reasoning. In the learning objective that we present in for each live session, we will also identify the level of reasoning that we hope students will achieve at the conclusion of the live session. Remember A student can remember that the concept exists. This might require the student to define, duplicate, or memorize a set of concepts or facts. Understand A student can understand the concept, and can produce a working technical and non-technical statement of the concept. The student can explain why the concept is, or why the concept works in the way that it does. Apply A student can use the concept as it is intended to be used against a novel problem. Analyze A student can assess whether the concept has worked as it should have. This requires both an understanding of the intended goal, an application against a novel problem, and then the ability to introspect or reflect on whether the result is as it should be. Evaluate A student can analyze multiple approaches, and from this analysis evaluate whether one or another approach has better succeeded at achieving its goals. Create A student can create a new or novel method from axioms or experience, and can evaluate the performance of this new method against existing approaches or methods. "],["probability-spaces.html", "Unit 1 Probability Spaces", " Unit 1 Probability Spaces Probability is a system of reasoning about the world in the face of incomplete information. In this course, we’re going to develop an understanding of the implications of core parts of this theory, how this theory was developed, and how these implications relate to every other part of the practice of data science. "],["learning-objectives.html", "1.1 Learning Objectives", " 1.1 Learning Objectives At the end of this week’s learning, student will be able to: Find and access all of the course materials Develop a course of study that is builds toward success Apply the axioms of probability to make a valid statement Solve word problems through the application of probability and math rules "],["course-learning-objectives.html", "1.2 Course Learning Objectives", " 1.2 Course Learning Objectives At the end of this course, students will be able to: Understand the building blocks of probability theory that prepare learners for the study of statistical models. Understand the mathematical objects of probability theory and be able to apply their properties. Understand how high-level concepts from calculus and linear algebra are related to common procedures in data science. Translate between problems that are defined in business or research terms into problems that can be solved with math. Understand and apply statistical models in common situations. Understand the theory of statistics to prepare students for inferrential statements. Understand model parameters and high level strategies to estimate them: means, least squares, and maximum likelihood. Choose an appropriate statistic, and conduct a hypothesis test in the Neyman-Pearson framework. Interpret the results of a statistical test, including statistical significance and practical significance. Recognize limitations of the Neyman-Pearson hypothesis testing framework and be a conscientious participant in the scientific process Analyze a research question using a linear regression framework. Explore and wrangle data with the intention of understanding the information and relationships that are (and are not) present Identify the goals of your analysis Build a model that achieves the goals of an analysis Interpret the results of a model and communicate them in manner appropriate to the audience. Identify their audience and report process and findings in a manner appropriate to that audience. Construct regression oriented reports that provide insight for stakeholders. Construct technical documents of process and code for collaboration and reproducability with peer data scientists. Read, understand, and assess the claims that are made in technical, regression oriented reports Contribute proficient, basic work, using industry standard tools and coding practices to a modern data science team. Demonstrate programming proficiency by translating statistical problems into code. Understand and incorporate best practices for coding style and data carpentry Utilize industry standard tooling for collaboration "],["introductions.html", "1.3 Introductions", " 1.3 Introductions 1.3.1 Instructor Introductions The instructors for the course come to the program, and to statistics from different backgrounds. Instructors hold PhDs in statistics, astrophysics, biology, political science, information. 1.3.2 What does a statistician look like? A statistician looks like YOU! Identity shapes how people approach and understand their world. We would like to acknowledge that we have limited diversity of identity among the instructors for this course. However, every one of the instructors shares a core identity as an empathetic educator that wants to understand your strengths, areas for growth, and unique point of view that is shaped by who you are. It doesn’t matter if you’ve never taken a stats class before, or if you’re reviewing using this class. There will be challenges for everyone to overcome. It doesn’t matter how old or young you are. We will all be learning frequentist statistics which is timeless. The color of your skin doesn’t matter; nor does whether you identify as a woman or a man or trans or non-binary; neither does your sexual orientation. There are legacies of exclusion and discrimination against people due to these identities. We will not continue to propagate those legacies and instead will work to controvert those discriminations to build a diverse community of learning in line with the University’s Principles of Community. "],["student-introductions.html", "1.4 Student Introductions", " 1.4 Student Introductions Please take 90 seconds to tell us: Your name as you would like to be called; Where you dial in from; What brings you to an interest in data science; and, Any other interests, or identities that you would like your classmates and instructor to know about. Please, try to keep these intros to just 90 seconds. We’ve a lot to cover this week! "],["probability-theory.html", "1.5 Probability Theory", " 1.5 Probability Theory Probability Probability is a system of reasoning that we use to model the world under incomplete information. This model underlies virtually every other model you’ll ever use as a data scientist. picard In this course, probability theory builds out to random variables; when combined with sampling theory we are able to develop p-values (which are also random variables) and an inferential paradigm to communicate what we know and how certain a statement we can make about it. In introduction to machine learning, literally the first model that you will train is a naive bayes classifier, which is an application of Bayes’ Theorem, trained using an iterative fitting algorithm. Later in machine learning, you’ll be fitting non-linear models, but at every point the input data that you are supplying to your models are generated from samples from random variables. That the world can be represented by random variables (which we will cover in the coming weeks) means that you can transform – squeeze and smush, or stretch and pull – variables to heighten different aspects of the variables to produce the most useful information from your data. As you move into NLP, you might think of generative text as a conditional probability problem: given some particular set of words as an input, what is the most likely next word or words that someone might type? Beyond the direct instrumental value that we see working with probability, there are two additional aims that we have in starting the course in the manner. First, because we are starting with the axioms of probability as they apply to data science statistics, students in this course develop a much fuller understanding of classical statistics than students in most other programs. Unfortunately, it is very common for students and then professionals to see statistics as a series of rules that have to be followed absolutely and without deviation. In this view of statistics, there are distributions to memorize; there are repeated problems to solve that require the rote application of some algebraic rule (i.e. compute the sample average and standard deviation of some vector); and, there are myriad, byzantine statistical tests to memorize and apply. In this view of statistics, if the real-world problem that comes to you as a data scientist doesn’t clearly fit into a box, there’s no way to move forward. Statistics like this is not fun. In the way that we are approaching this course, we hope that you’re able to learn why certain distributions (like the normal distribution) arise repeatedly, and why we can use them. We also hope that because you know how sampling theory and random variables combine, that you can be more creative and inventive to solve problems that you haven’t seen before. The second additional aim that we have for this course is that it can serve as either an introduction or a re-introduction to reading and making arguments using the language of math. For some, this will be a new language; for others, it may have been some years since they have worked with the language; for some, this will feel quite familiar. New algorithms and data science model advancements nearly always developed in the math first, and then applied into algorithms second. In our view, being a literate reader of graduate- and professional-level math is a necessary skill for any data scientist that is going to keep astride of the field as it continues to develop and these first weeks of the course are designed to bring everyone back into reading and reasoning in the language. "],["working-with-a-sample-space.html", "1.6 Working with a Sample Space", " 1.6 Working with a Sample Space 1.6.1 Working with a Sample Space, Part I You roll two six-sided dice: How would you define an appropriate sample space, \\(\\Omega\\)? How many elements exist in \\(\\Omega\\)? What is an appropriate event space, and how many elements does it have? Give an example of an event. 1.6.2 Working with a Sample Space, Part II For a random sample of 1,000 Berkeley students: How would you define an appropriate sample space, \\(\\Omega\\)? How big is \\(\\Omega\\)? How many elements does it contain? What is an example of an event for this scenario? Can a single person be represented in the space twice? Why or why not? 1.6.3 Working with a Sample Sapce, Part III Suppose that you’re sitting in a surf lineup, and you have to pick a wave that is the right height. Too small, and you won’t get anywhere, too large and you’ll get crushed. What sample space is appropriate to represent the height of a single wave, \\(\\Omega\\)? How big is \\(\\Omega\\)? How many elements does it contain? What is an example of an event that could be part of the event space? What sample space is appropriate to represent the height of the next 10 waves? How large is this sample space? To represent 10 waves, you should use \\(\\mathbb{R}^{10}\\). It is curious mathematical fact that \\(\\mathbb{R}\\) and \\(\\mathbb{R}^{10}\\) actually have the same cardinality – there are the same number of elements in each of these sets and there exists a function – i.e. a one-to-one mapping – between these sets. "],["proofs-style-counts.html", "1.7 Proofs: Style Counts", " 1.7 Proofs: Style Counts In each week of a class, you are either caught up or behind. The probability that you are caught up in Week 1 is 0.7. If you are caught up in a given week, the probability that you will be caught up in the next week is 0.7. If you are behind in a given week, the probability that you will be caught up in the next week is 0.4. What is the probability that you are caught up in week 3? Identify as many ways to improve this proof as you can: If you are caught up in a week, there are two possibilities for the previous week: caught up and behind. Let \\(P(C)\\) be the probability of being caught up. In week 1, \\(P(C) = .7\\). The probability of being behind is \\(P(B) = 1 - .7 = .3\\). We first break down the probability for week 2: \\[P(C) = .7 \\cdot .7 + .3 \\cdot .4 = .65\\] Now we can repeat the process for week 3: \\[P(C) = .65 * .7 + .35 * .4 = .595\\] "],["a-practice-problem.html", "1.8 A practice problem", " 1.8 A practice problem Let’s go and work on a practice problem over on the course practice problem website. link here "],["concluding-reminders.html", "1.9 Concluding Reminders", " 1.9 Concluding Reminders Welcome! Before next live session: Complete the homework that builds on this unit Complete all videos and reading for unit 2 "],["defining-random-variables.html", "Unit 2 Defining Random Variables", " Unit 2 Defining Random Variables ## ── Attaching packages ──────────────────────────────────────────────────────────────────────────── tidyverse 1.3.1 ── ## ✓ ggplot2 3.3.5 ✓ purrr 0.3.4 ## ✓ tibble 3.1.5 ✓ dplyr 1.0.7 ## ✓ tidyr 1.1.4 ✓ stringr 1.4.0 ## ✓ readr 2.0.2 ✓ forcats 0.5.1 ## ── Conflicts ─────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() "],["learning-objectives-1.html", "2.1 Learning Objectives", " 2.1 Learning Objectives At the end of this week’s course of study (which includes the async, sync, and homework) students should be able to Remember that random variable are neither random, or variables, but instead that they are a foundational object that we can use to reason about a world. Understand that the intuition developed by the use of set-theory probability maps into the more expressive space of random variables Apply the appropriate mathematical transformations to move between joint, marginal, and conditional distributions. From the axioms of probability, it is possible to build a whole, expressive modeling system (that need not be grounded at all in the minutia of the world). With this probability model in place, we can describe how frequently events in the random variable will occur. When variable are dependent upon each other, we can utilize information that is encoded in this dependence in order to make predictions that are closer to the truth than predictions made without this information. There is both a beauty and a tragedy when reasoning about random variables: we describe random variables using their joint density function. The beauty is that by reasoning with such general objects we can produce guarantees that hold in every case, no matter the function that stands in for the joint density function. The tragedy is that in the “real world”, the world where we are going to eventually going to train and deploy our models, we are never provided with this joint density function. In examples that we compute, to provide a chance to reason about these objects, we will typically write down a specific function that a random variable represents, despite the limitation that we cannot directly observe this function in the wild. "],["class-announcements.html", "2.2 Class Announcements", " 2.2 Class Announcements 2.2.1 Homework You should have turned in your first homework. The solution set for this homework is scheduled to be released to you in two days. The solution set contains a full explanation of how we solved the questions posed to you. You can expect that feedback for this homework will be released back to you within seven days. You can start working on your second homework when we are out of this class. 2.2.2 Study Groups It is a very good idea for you to create a recurring time to work with a set of your classmates. Working together will help you solve questions more effectively, quickly, and will also help you to learn how to communicate what you do and do not understand about a problem to a group of collaborating data scientists. And, working together with a group will help you to find people who share data science interests with you. 2.2.3 Course Resources There are several resources to support your learning. A learning object last week was that you would be introduced to each of these systems. Please continue to make sure that you have access to the: Library VPN to read all of the scholarly content in the known universe, including the course textbook. Course LMS Page "],["using-definitions-of-random-variables.html", "2.3 Using Definitions of Random Variables", " 2.3 Using Definitions of Random Variables 2.3.1 Random Varaible What is a random variable? Does this definition help you? Definition 2.1 \\iffalse (Random Variable) A random variable is a function \\(X : \\Omega \\rightarrow \\mathbb{R},\\) such that \\(\\forall r \\in \\mathbb{R}, \\{\\omega \\in \\Omega\\}: X(\\omega) \\leq r\\} \\in S\\). Someone, please, read that without using a single “omega”, \\(\\mathbb{R}\\), or other jargon terminology. Instead, someone read this aloud and tell us what each of the concepts mean. The goal of writing with math symbols like this is to be absolutely clear what concepts the author does and does not mean to invoke when they write a definition or a theorem. In a very real sense, this is a language that has specific meaning attached to specific symbols; there is a correspondence between the mathematical language and each of our home languages, but exactly what the relationship is needs to be defined into each student’s home language. What are the key things that random variables allow you to accomplish? Suppose that you were going to try to make a model that predicts the probability of winning “big money” on the machine. Big money might be that you get :cherries: :cherries: :cherries:. Can you do math with :cherries:? Why do we say that random variables are functions? Is there some useful property of these being functions rather than any other quantity? 2.3.2 Functions of Functions Definition 2.2 \\iffalse (Function of a Random Variable) Let \\(g : U \\rightarrow \\mathbb{R}\\) be some function, where \\(X(\\Omega) \\subset U \\subset \\mathbb{R}\\). Then, if \\(g \\circ X : \\Omega \\rightarrow \\mathbb{R}\\) is a random variable, we say that \\(g\\) is a function of X and write \\(g(X)\\) to denote the random variable \\(g \\circ X\\). If a random variable is a function from the real world, or the sample space, or the outcome space to a real number, then what does it mean to define a function of a random variable? At what point does this function work? Does this function change the sample space that is possible to observe? Or, does this function change the real-number that each outcome points to? Example 2.1 \\iffalse (MNIST) Suppose that you are doing some image processing work. To keep things simple, that you are doing image classification in the style of the MNIST dataset. Can someone describe what this task is trying to accomplish? Has anyone done work like this? However, suppose that rather than having good clean indicators for whether a pixel is on or off, instead you have weak indicators – there’s a lot of grey. A lot of the cells are marked in the range \\(0.2 - 0.3\\). How might creating a function that re-maps this grey into more extreme values help your model? Is it possible to “blur” events that are in the outcome space? Does this “blurring” meet the requirements of a function of a random variable, as provided above? "],["pieces-of-a-random-variable.html", "2.4 Pieces of a Random Variable", " 2.4 Pieces of a Random Variable When you look at the definition of a random variable, there are two key pieces that must exist for every random variable. What are these pieces? As a hint, we actually developed one of these key pieces last week when we were discussing elementary probability. Definition 2.3 \\iffalse (Random Variable, Suite) A random variable is a function \\(X : \\Omega \\rightarrow \\mathbb{R},\\) such that \\(\\forall r \\in \\mathbb{R}, \\{\\omega \\in \\Omega\\}: X(\\omega) \\leq r\\} \\in S\\). 2.4.1 Probability Density Functions and Cumulative Density Functions What is a probability mass function? What do the Kolmogorov Axioms mean must be true about any probability mass function (pmf)? Example 2.2 \\iffalse (Berkeley Drivers, No Survivors) You should try driving in Berkeley some time. It is a trip! Without being deliberately ageist, the city is full of ageing hippies driving subarus and making what seem to be stochastic right-or-left turns to buy incense, pottery, or just sourdough bread. Suppose that you are walking to campus, and you have to cross 10 crosswalks, each of which are spaced a block apart. Further, suppose that as you get closer to campus, there are fewer aging hippies, and therefore, there is decreasing risk that you’re hit by a Subaru as you cross the street. Specifically, and fortunately for our math, the risk of being hit decreases linearily with each block that you cross. Finally, campus provides you with the safety reports from last year, and reports that there were 120 student-subaru incideces last year, out of 10,000 student-crosswalk crossings. What is the pmf for the probability that you are involved in a student-subaru incident as you walk across these 10 blocks? What sample space, \\(\\Omega\\) is appropriate to represent this scenario? Suppose that you don’t leave your house – this is a remote program after all! What is your cumulative probability of being involved in a student-subaru incident? What is the cumulative probability cmf for the probability that you are involved in a student-subaru incedent? Suppose that you live three blocks from campus, but your classmate lives five blocks from campus. What is the difference in the cumulative probability? How would you describe the cumulative probability of being hit as you walk closer to campus? That is, suppose that you start 10 blocks away from campus, and are walking to get closer. Is your cumulative probability of being hit on your way to campus increasing or decreasing as you get closer to campus? How would you describe the cumulative probability of being hit as you walk further from campus? That is, suppose that you start on campus, and you’re walking to a bar after classes. Is your cumulative probability of being hit on your way away from campus increasing or decreasing as you get further from campus? "],["discrete-continuous-random-variables.html", "2.5 Discrete &amp; Continuous Random Variables", " 2.5 Discrete &amp; Continuous Random Variables What, if anything is fundamentally different between discrete and continuous random variables? As a way of starting the conversation, consider the following cases: Suppose \\(X\\) is a random variable that describes the time a student spends on w203 homework 1. If you have only granular measurement – i.e. the number of nights spent working on the homework – is this discrete or continuous? If you have the number of hours, is it discrete or continuous? If you have the number of seconds? Or milliseconds? Is it possible that \\(P(X = a) = 0\\) for every point \\(a\\)? For example, that \\(P(X = 3600) = 0\\). Does one of these measures have more information in it than another? How are measurement choices that we make as designers of information capture systems – i.e. the machine processes, human processes, or other processes that we are going to work with as data scientists – reflected in both the amount of information that is gathered, the type of information that is gathered, and the types of random variables that are manifest as a result? "],["moving-between-pdf-and-cdf.html", "2.6 Moving Between PDF and CDF", " 2.6 Moving Between PDF and CDF The book defines pmf and cmf first as a way of developing intuition and a way of reasoning about these concepts. It then moves to defining continuous density functions, which is many ways are easier to work with although they lack the means of reasoning about them intuitively. Continuous distributions are defined in the book, and more generally, in terms of the cdf, which is the cumulative density function. There are technical reasons for this choice of definition, some of which are signed in the footnotes on the page where the book presents it. More importantly for this course, in Definition 1.2.15 the book defines the relationship between cdf and pdf in the following way: Definition 2.4 \\iffalse (Probability Density Function (PDF)) For a continous random variable \\(X\\) with CDF \\(F\\), the probability density function of \\(X\\) is \\[ f(x) = \\left. \\frac{d F(u)}{du} \\right|_{u=x}, \\forall x \\in \\mathbb{R}. \\] How does this definition, which relates pdf and cdf by a means of differentiation and integration, fit with the ideas that we just developed in the context of walking to and from campus? Example 2.3 \\iffalse (Working with a continuous pdf and cdf) Suppose that you learn than a particular random variable, \\(X\\) has the following function that describes its pdf, \\(f_{x}(x) = \\frac{1}{10}x\\). Also, suppose that you know that the smallest value that is possible for this random varaible to obtain is 0. What is the CDF of \\(X\\)? What is the maximum possible value that \\(x\\) can obtain? How did you develop this answer, using the Kolmogorov axioms of probability? What is the cumulative probability of an outcome up to 0.5? What is the probability of an outcome between 0.25 and 0.75? Produce an answer to this in two ways: Using the \\(pdf\\) Using the \\(cdf\\) "],["joint-density.html", "2.7 Joint Density", " 2.7 Joint Density Working with a single random variable helps to develop our understanding of how to relate the different features of a pdf and a cdf through differentiation and integration. However, there’s not really that much else that we can do; and, there is probably very little in our professional worlds that would look like a single random variable in isolation. We really start to get to something useful when we consider joint density functions. Joint density functions describe the probability that both of two random variables. That is, if we are working with random variables \\(X\\) and \\(Y\\), then the joint density function provides a probability statement for \\(P(X \\cap Y)\\). In this course, we might typically write this joint density function as \\(f_{X,Y}(x,y) = f(\\cdot)\\) where \\(f(\\cdot)\\) is the actual function that represents the joint probability. 2.7.1 Uniform Joint Density Suppose that we know that two variables, \\(X\\) and \\(Y\\) are jointly uniformly distributed within the the support \\(x \\in [0,4], y \\in [0,4]\\). We have a requirement, imposed by the Kolmogorov Axioms that all probabilities must be non-zero, and that the total probability across the whole support must be one. Can you use these facts to determine answers to the following: What kind of shape does this joint pdf have? What is the specific function that describes this shape? If you draw this shape on three axes, and \\(X\\), and \\(Y\\), and a \\(P(X,Y)\\), what does this plot look like? How do you get from the joint density function, to a marginal density function for \\(X\\)? How do you get form the joint density function, to a marginal denisty function for \\(Y\\)? How do you get from these marginal density functions of \\(X\\) and \\(Y\\) back to the joint density? Is this alwasy possible? knitr::include_app(&#39;http://www.statistics.wtf/PDF_Explorer/&#39;) 2.7.2 Saddle Sores Suppose that you know that two random variables, \\(X\\) and \\(Y\\) are jointly distributed with the following pdf: \\[ f_{X,Y}(x,y) = begin{cases} a * x^2 * y^2) &amp; 0 &lt; x &lt; 1, 0 &lt; y &lt; 1 0 &amp; otherwise \\\\ \\end{cases} \\] Can you use these faects to determine the following? What value of \\(a\\) makes this a valid joint pdf? What is the marginal pdf of \\(x\\)? That is, what is \\(f_{x}(x)\\)? What is the conditional pdf of \\(X\\) given \\(Y\\)? That is, what is $f_{x|y}(x,y)? Given these facts, would you say that \\(X\\) and \\(Y\\) are dependent or independent? "],["visualizing-distributions-via-simulation.html", "2.8 Visualizing Distributions Via Simulation", " 2.8 Visualizing Distributions Via Simulation 2.8.1 The Visualization Trick Here is the true density function for a normal random variable. Simulate Draws There’s another way to get an approximate idea of what the distribution looks like. Here’s how we take a single draw from a normal distribution with a specific set of features: rnorm(n = 1, mean = 2, sd = 1) ## [1] 2.853123 Repeating the Experiment We want to rerun that experiment 10 times. We take a draw, then rewind time, clear our memory and start over with fresh randomness. To do this in R, an easy way is with the replicate() function. Change the code below so that it repeats the experiment above 10 times, then use hist() to display a plot of the result. simulation &lt;- replicate( n = 10, # should you change this line? expr = 1 + .2 # or this line? ) simulation ## [1] 1.2 1.2 1.2 1.2 1.2 1.2 1.2 1.2 1.2 1.2 Better Visualization Here’s some fancy ggplot code to draw a nice histogram of the result, along with the true density. Remove the first line to make it work with your simulation. short_simulation &lt;- c(1,2,3,2,4,2) true_density &lt;- function(x) { dnorm(x = x, mean = 2, sd = 1) } dat_hist &lt;- data.frame(short_simulation) dat_hist %&gt;% ggplot() + geom_histogram( aes(x = short_simulation, y = ..density..)) + stat_function( aes(x = short_simulation), fun = true_density, color = &#39;darkred&#39;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Repeating the Experiment: Questions What happens to your plot as you increase the number of draws from 10 to 100 to 1000…? In your own words, what is the difference between the distribution and the sample you are taking? The Visualization Trick This is a pretty useful trick. The repetition we’re using has no analogue in the real world – we don’t get to shake up the world like a snow globe a number of times in a row to see what it does. But, when we say “take a draw from the distribution” another way to say this is that we’re simulating the random variable. 2.8.2 Apply the Visualization Trick Part I How can the visualization trick help us? Here’s a problem: Suppose \\(X\\) and \\(Y\\) are independent normal random variables, both with mean 2 and standard deviation 1. Say \\(Z = X + Y\\). What is the distribution of \\(Z\\)? We could do some math to compute the density function of \\(Z\\), but it’s actually quite messy. Instead, let’s use the visualization trick to get an approximate idea. Part II First, write an R function to simulate a single draw from \\(Z\\). Simulate a draw for \\(X\\). Simulate a draw \\(Y\\). Return the sum of the previous draws. rz &lt;- function() { 2 # Replace with your code } Use the previous code to repeat this experiment 10,000 times and plot a histogram. See if you can guess what the the distribution is, and plot your guess on the histogram. 2.8.3 How is simulation useful? Are there situations that you think simulation of this sort might be useful? "],["computing-different-distributions..html", "2.9 Computing Different Distributions.", " 2.9 Computing Different Distributions. Suppose that random variables \\(X\\) and \\(Y\\) are jointly continuous, with joint density function given by, \\[ f(x,y) = \\begin{cases} c, &amp; 0 \\leq x \\leq 1, 0 \\leq y \\leq x \\\\ 0, &amp; otherwise \\end{cases} \\] where \\(c\\) is a constant. Draw a graph showing the region of the X-Y plane with positive probability density. What is the constant \\(c\\)? Compute the marginal density function for \\(X\\). (Be sure to write a complete expression) Compute the conditional density function for \\(Y\\), conditional on \\(X=x\\). (Be sure to specify for what values of \\(x\\) this is defined) "],["understanding-joint-distributions.html", "2.10 Understanding Joint Distributions", " 2.10 Understanding Joint Distributions In this picture, we imagine putting a cake down on the X-Y plane. Take a sharp knife and make two cuts parallel to the X-axis. one is at \\(Y = y\\), the other at \\(Y = y + dy\\). cake Review of Terms Remember some of the key terms we learned in the async: Joint Density Function Conditional Distribution Marginal Distribution Explain each of these three in terms of the cake metaphor. "],["working-with-a-shiny-app.html", "2.11 Working with a Shiny App", " 2.11 Working with a Shiny App knitr::include_app(url = &#39;http://statistics.wtf/betahat/&#39;, height = &#39;1200px&#39;) "],["summarizing-distributions.html", "Unit 3 Summarizing Distributions", " Unit 3 Summarizing Distributions library(tidyverse) "],["learning-objectives-2.html", "3.1 Learning Objectives", " 3.1 Learning Objectives "],["class-announcements-1.html", "3.2 Class Announcements", " 3.2 Class Announcements "],["roadmap.html", "3.3 Roadmap", " 3.3 Roadmap Roadmap – Rearview mirror Statisticians create a population model to represent the world. Random variables are the building blocks of a model. We can describe the distribution of a random variable using: a cdf (all random varibles) a pmf (discrete random variables) a pdf (continuous random variables) When we have multiple random variables, a joint pmf / pdf describes how they behave together a marginal pmf / pdf describes one variable in isolation a conditonal pmf / pdf describes one variable given the value of another Roadmap – Today’s Lesson A joint distribution has more information than we can ever use (or estimate with finite data). To do useful things, we need to summarize certain features we care about: Expectation Variance Covariance Correlation Roadmap – Coming Attractions A predictor is a function that provides a value for one variable, given values of some others. Using our summary tools, we will define a predictor’s error and then minimize it. This is a basis for linear regression "],["proof-strategy-workshop-expectation.html", "3.4 Proof Strategy Workshop: Expectation", " 3.4 Proof Strategy Workshop: Expectation Let \\(Y\\) be the random variable given by \\(X^2\\) where \\(X\\) is a uniform distribution on the support \\([0, 1]\\). What is the expected value of \\(Y\\)? Your instructor will assign you to Proof 1, Proof 2, OR Proof 3. First, study your proof. Second, in breakout rooms, teach your proof to your classmates. Proof 1 We first compute the CDF of Y. For \\(0 \\leq y \\leq 1\\), \\[ F_Y(y) = P(Y \\leq y) = P(X^2 \\leq y) = P(X \\leq \\sqrt{y}) = \\sqrt{y} \\] Next, we take the derivative to get Y’s density for \\(0 \\leq y \\leq 1\\): \\[ f_Y(y) = \\frac{\\partial}{\\partial y} F_Y(y) = \\frac{1}{2} y^{-1/2} \\] (On your assignments, make sure you write a complete expression) Finally, \\[ E[Y] = \\int_0^1 y f_Y(y) dy = \\int_0^1 \\frac{1}{2} y^{1/2} dy = \\frac{1}{3}y^{3/2} \\Bigg|_{y=0}^1 = \\frac{1}{3} - 0 = \\frac{1}{3} \\] Proof 2 Apply the bonus result from HW 2: Note that \\(Y=h(x)\\) where \\(h(x)= x^2\\). For \\(0 \\leq y \\leq 1\\), \\[ f_{Y}(y) = f_X(h^{-1}(y)) \\cdot \\left| \\frac{d}{dy} h^{-1}(y) \\right| = f_X(\\sqrt{y}) \\left| \\frac{d}{dy} \\sqrt{y} \\right| = 1\\cdot \\frac{1}{2} y^{-1/2} \\] (On your assignments, make sure you write a complete expression) Finally, we find the expectation: \\[ E[Y] = \\int_0^1 y f_Y(y) dy = \\int_0^1 \\frac{1}{2} y^{1/2} dy = \\frac{1}{3}y^{3/2} \\Bigg|_{y=0}^1 = \\frac{1}{3} - 0 = \\frac{1}{3} \\] Proof 3 Apply the Law of the Unthinking Statistician (LOTUS): \\[ E[Y] = \\int_{-\\infty}^\\infty x^2 f_X(x) dx = \\int_{-\\infty}^0 x^2 \\cdot 0 + \\int_0^1 x^2 \\cdot 1 dx + \\int_1^{\\infty} x^2 \\cdot 0 \\] \\[ E[Y] = 0 + \\frac{1}{3}x^3 \\Bigg|_0^1 + 0 = \\frac{1}{3} \\] Proof Debrief In words, what is the power of the Law of the Unthinking Statistician? Which was your favorite proof? "],["linearity-of-expectation.html", "3.5 Linearity of Expectation", " 3.5 Linearity of Expectation Expectation is Linear For a random variable \\(X\\) and constants \\(a\\) and \\(b\\): \\[E(aX + b) = aE(X) + b\\] For two random variables \\(X\\) and \\(Y\\): \\[E(X + Y) = E(X) + E(Y)\\]. Using Linearity, Part I Linearity is a very powerful property. Using it can often make your solutions much cleaner. Say you want to find the expectation of a symmetric variable, \\(X\\). For example, here’s a random variable that’s symmetric around 5. Using Linearity, Part II "],["conditional-expectation-and-the-blp.html", "Unit 4 Conditional Expectation and The BLP", " Unit 4 Conditional Expectation and The BLP thunder struck "],["learning-objectives-3.html", "4.1 Learning Objectives", " 4.1 Learning Objectives "],["class-announcements-2.html", "4.2 Class Announcements", " 4.2 Class Announcements "],["roadmap-1.html", "4.3 Roadmap", " 4.3 Roadmap Roadmap – Rearview Mirror Statisticians create a population model to represent the world. \\(E[X], V[X], Cov[X,Y]\\) are “simple” summaries of complex joint distributions, which are hooks for our analyses. They also have useful properties – for example, \\(E[X + Y] = E[X] + E[Y]\\). Roadmap – This week We look at situations with one or more “input” random variables, and one “output.” Conditional expectation summarizes the output, given values for the inputs. The conditional expectation function (CEF) is a predictor – a function that yields a value for the output, give values for the inputs. The best linear predictor (BLP) summarizes a relationship using a line / linear function. Roadmap – Coming Attractions OLS regression is a workhorse of modern statistics, causal analysis, etc It is also the basis for many other models in classical stats and machine learning The target that OLS estimates is exactly the BLP, which we’re learning about this week. "],["conditional-expectation-function-cef.html", "4.4 Conditional Expectation Function (CEF)", " 4.4 Conditional Expectation Function (CEF) Expectation of \\(Y\\): \\[ E[Y] = \\int_{-\\infty}^\\infty y \\cdot f_{Y}(y) dy \\] Conditional expectation of \\(Y\\) given \\(X=x \\in \\text{Supp}[X]\\): \\[ E[Y|X=x] = \\int_{-\\infty}^\\infty y \\cdot f_{Y|X}(y|x) dy \\] Compare and contrast \\(E[Y]\\) and \\(E[Y|X]\\). For example, how are their components similar or different? What is \\(E[Y|X]\\) a function of? What are “input” variables to this function? What is \\(E[E[Y|X]]\\) a function of? (Question 1) In both, we have a probability density function multiplied by the values that we realize; this is basically serving as a “weighting” function, we’re merely changing what that weighting function is! In both cases we’re looking at a “mean” of a distribution – in one it is just a “conditional mean” than a “marginal mean”. (Question 2) We integrate \\(Y\\) out completely by using its every value in the integral, so \\(E\\) will not have \\(Y\\) in the answer – instead, it will remain a function of \\(X\\). \\(f_{Y|X=x}\\) is a function of \\(x\\)! So, the conditional expectation is some function \\(x\\) as well. (Optional to discuss; probably too minute) An observation that some parsing students will make: Without fixing \\(X\\) to some realization (denoted as a “little x”, \\(x\\), then \\(E[Y|X]\\) is a function of \\(X\\) and so the whole statement is a function of a random variable (i.e. \\(E[Y|X]\\) is itself a RV). Once we fix \\(X=x\\), then \\(E[Y|X=x]\\) is fixed to some constant – there is one value that \\(E[Y|X=x]\\) maps to. (Question 3) That isn’t a function of anything! Once you’ve computed \\(E[E[Y|X]]\\), you’ve integrated out all variables! "],["computing-the-cef.html", "4.5 Computing the CEF", " 4.5 Computing the CEF Suppose that random variables \\(X\\) and \\(Y\\) are jointly continuous, with joint density function given by, \\[ f(x,y) = \\begin{cases} 2, &amp; 0 \\leq x \\leq 1, 0 \\leq y \\leq x \\\\ 0, &amp; otherwise \\end{cases} \\] In week 2 we computed the conditional density of this function: \\[ f_{Y|X}(y|x) = \\begin{cases} \\frac{1}{x}, &amp; 0 \\leq y \\leq x \\\\ 0, &amp; \\text{otherwise} \\end{cases} \\] What is the conditional expectation function? What is the conditional variance function? "],["group-exercise.html", "4.6 Group Exercise", " 4.6 Group Exercise 4.6.1 Minimizing MSE Theorem 2.2.20 states: The CEF \\(E[Y|X]\\) is the “best” predictor of \\(Y\\) given \\(X\\), where “best” means it has the smallest mean squared error (MSE). Task: Justify every transition (“=” sign) of the proof below using earlier FOAS concepts, definitions, theorems, calculus, and algebraic operations. Proof: We need to find such function \\(g: R \\to R\\) that gives the smallest \\(E[(Y-g(X))^2]\\). It should turn out that \\(g(X)\\) is actually \\(E[Y|X]\\). Deriving a Function to Minimize MSE \\[\\begin{align*} E[(Y - g(X))^2|X] &amp;= E[Y^2 - 2Yg(X) + g^2(X)|X] \\\\ &amp;= E[Y^2|X] + E[-2Yg(X)|X] + E[g^2(X)|X] \\\\ &amp;= E[Y^2|X] - 2g(X)E[Y|X] + g^2(X)E[1|X] \\\\ &amp;= (E[Y^2|X] - E^2[Y|X]) + (E^2[Y|X] - 2g(X)E[Y|X] + g^2(X)) \\\\ &amp;= V[Y|X] + (E^2[Y|X] - 2g(X)E[Y|X] + g^2(X)) \\\\ &amp;= V[Y|X] + (E[Y|X] - g(X))^2 \\\\ \\end{align*}\\] Then we have: \\[\\begin{align*} E[(Y-g(X))^2] &amp;= E\\big[E[(Y-g(X))^2|X]\\big] \\\\ &amp;=E\\big[V[Y|X]+(E[Y|X]-g(X))^2\\big]\\\\ &amp;=E\\big[V[Y|X]\\big]+E\\big[(E[Y|X]-g(X))^2\\big]\\\\ \\end{align*}\\] \\(E[V[Y|X]]\\) doesn’t depend on \\(g\\); and, \\(E[(E[Y|X]-g(X))^2]\\ge 0\\). \\(\\therefore g(X) = E[Y|X]\\) gives the smallest \\(E[(Y-g(X))^2]\\) Implication: If you are choosing some \\(g\\), you can’t do better than \\(g(x) = E[Y|X=x]\\). 4.6.2 Working with the BLP Why Linear? In some cases, we might try to estimate the CEF. More commonly, however, we work with linear predictors. Why? We don’t know joint density function of \\(Y\\). So, it is “difficult” to derive a suitable CEF. To estimate flexible functions requires considerably more data. Assumptions about distribution (e.g. a linear form) allow you to leverage those assumptions to learn ‘more’ from the same amount of data. Other times, the CEF, even if we could produce an estimate, might be so complex that it isn’t useful or would be difficult to work with. And, many times, linear predictors (which might seem trivially simple) actually do a very good job of producing predictions that are ‘close’ or useful. 4.6.3 Joint Distribution Practice 4.6.3.1 Professorial Mistakes (Discrete RVs) Let the number of questions that students ask be a RV, \\(X\\). Let \\(X\\) take on values: \\(\\{1, 2, 3\\}\\), each with probability \\(1/3\\). Every time a student asks a question, the instructor answers incorrectly with probability \\(1/4\\), independently of other questions. Let the RV \\(Y\\) be number of incorrect responses. Questions: Compute the expectation of \\(Y\\), conditional on \\(X\\), \\(E[Y|X]\\) Using the law of iterated expectations, compute \\(E[Y] = E\\big[E[Y|X]\\big]\\). 4.6.3.2 (Bonus Questions) Working with the same question from the last slide: Compute the expectation of the product of \\(X\\) and \\(Y\\), \\(E(XY)\\) Using the previous result, compute \\(\\text{cov}(X,Y)\\). "],["learning-from-random-samples.html", "Unit 5 Learning from Random Samples", " Unit 5 Learning from Random Samples south hall "],["learning-objectives-4.html", "5.1 Learning Objectives", " 5.1 Learning Objectives "],["class-announcements-3.html", "5.2 Class Announcements", " 5.2 Class Announcements You’re done with probability theory. Congrats! You’re also done with your first test. Congrats! We’re going to have a second test in a few weeks; then we’re done testing for the semester "],["roadmap-2.html", "5.3 Roadmap", " 5.3 Roadmap Where We’re Going – Coming Attractions We’re going to start bringing data into our work First, we’re going to develop a testing framework that is built on sampling theory and reference distributions – t.tests, wilcox.test and the like Second, we’re going to show that OLS regression is the sample estimator of the BLP Third, we’re going to use the testing distribution to test regression coefficients Where We’ve Been – Random Variables and Probability Theory Statisticians create a model (A.K.A. population) to represent the world. That model can be described by parameters like expecation, covariance. So far, these parameter values have come from our imaginations Where we Are We want to fit models – use data to set their parameter values. A sample is a set of random variables Sample statistics are functions of a sample, and they are random variables Under iid and other assumptions, we get useful properties: Statistics may be consistent estimators for population parameters The distribution of sample statistics may be asymptotically normal "],["key-terms-and-assumptions.html", "5.4 Key Terms and Assumptions", " 5.4 Key Terms and Assumptions 5.4.1 Definitions Define each of the following: Sample Sample Statistic Estimator Bias Efficiency Consistency Convergence in Probability Convergence in Distribution 5.4.2 IID For each scenario, is the IID assumption plausible? Call a random phone number. If someone answers, interview all persons in the household. Repeat until you have data on 100 people. Call a random phone number, interview the person if they are over 30. Repeat until you have data on 100 people. Record year-to-date price change for 20 largest car manufacturers. Measure net exports per GDP for all 195 countries recognized by the UN. 5.4.3 Understanding Sampling Distributions Let \\(X\\) be a Bernoulli random variable representing an unfair coin with \\(P(X=1) = 0.7\\). You have an iid sample of size 2, \\((X_1,X_2)\\). Compute the sampling distribution of \\(\\overline X = \\frac{X_1+X_2}{2}\\). Questions: Explain the difference between a population distribution and the sampling distribution of a statistic. As we toss more and more coins, \\(\\overline X_{(100)} \\rightarrow \\overline X_{(10000)}\\) what will the value of \\(\\overline X\\) get closer to? What law generates this, and why does this law generate this result? Why do we want to know things about the sampling distribution of a statistic? "],["uncertainty.html", "5.5 Uncertainty", " 5.5 Uncertainty Which Result is Better? Suppose that you measure salary data among individuals who try different strategies Report out in the following table: Early Rising Mindfulness Retreat MIDS Degree Increase in Salary $1020 $5130 $9200 \\(SE\\) ($350) ($4560) \\(N =\\) 1,000 77 700 (Standard errors in parentheses when available) "],["write-code-to-demo-the-central-limit-theorem-clt.html", "5.6 Write Code to Demo the Central Limit Theorem (CLT)", " 5.6 Write Code to Demo the Central Limit Theorem (CLT) 5.6.1 Motivating the Central Limit Theor (CLT) Standard Errors tell us a lot about the uncertainty in our statistics But we want to say more: How confident are we that this vitamin has a positive effect? How plausible is a mean income $1000 below our estimate? For these questions, we need to know the sampling distribution of our statistic. How is this possible when we don’t know the population distribution? 5.6.2 Sampling from the Bernoulli Distribution in R To demonstrate the CLT, we chose a Bernoulli distribution with parameter \\(p\\). This distribution is very simple This distribution is non-normal, and can be very skewed depending on \\(p\\). First, set p=0.5 so your population distribution is symmetric. Use a variable n to represent your sample size. Initially, set n=3. n &lt;- 3 p &lt;- 0.5 5.6.3 Useful R Commands sample() or rbinom() R doesn’t have a bernoulli function. To simulate draws from a Bernoulli variable, you can either: Use sample Or, use rbinom (the Bernoulli distribution is a special case of a binomial distribution. In this function, size refers to a distribution parameter, not the number of draws.) sample(x=0:1, size=n, replace=TRUE, prob=c(1-p, p)) rbinom(n=n, size=1, prob=p) ## [1] 1 1 0 ## [1] 1 1 0 replicate() To repeat an action, you can use replicate replicate(10, log(10)) ## [1] 2.302585 2.302585 2.302585 2.302585 2.302585 2.302585 2.302585 2.302585 2.302585 2.302585 hist() To quickly visualize your results, try hist hist(x = rnorm(100), main = &quot;Simulated Sample Means&quot;) ggplot() Or, to work with ggplot store these results in a data.frame. d &lt;- data.frame(x = rnorm(100)) d %&gt;% ggplot(aes(x=x)) + geom_histogram() "],["exercise.html", "5.7 Exercise", " 5.7 Exercise Part 1 Throughout this part, we will use fair coins (p = 0.5). Fill in the function below so that it simulates taking n draws from a Bernoulli distribution with parameter p. This is like tossing n coins at the same time. Use the mean function to compute the sample mean – the average of the number of heads that are showing. Make sure that when you run it, you return values in \\(\\{0,1/3,2/3,1\\}\\). experiment = function(n, p){ } The sample mean is a random variable. To understand it, use the visualization trick from a few weeks ago. Use the replicate function to run the above experiment 1000 times, and plot a histogram of the results. If you replicate the experiment enough times, will the distribution ever look normal? Why or why not? Use sd() to check the standard deviation of the sampling distribution of the mean for number_of_coins = 3. What sample size is needed to decrease the standard deviation by a factor of 10? Check that your answer is correct. Part 2 For this part, we’ll continue to study a fair coin. Try different values for the sample size n, and examine the shape of the sampling distribution of the mean. At what point does it look normal to you? Part 3 For this part, we’ll study a very unfair coin. p = 0.01. This is an example of a highly skewed random variable. That roughly means that one tail is a lot longer than the other. For this activity, you can simply use your eyes to gauge how skewed a distribution is. If you prefer, you can also use the skewness command in the univar package to measure skewness. You may hear a rule of thumb that a skewness above 1 or below -1 is a highly skewed distribution. Start with n=3 as before. What do you notice about the shape of the sampling distribution? Try different values for the sample size n, and examine the shape of the sampling distribution of the mean. At what point does it look normal to you? "],["discussion-questions.html", "5.8 Discussion Questions", " 5.8 Discussion Questions How does the skewness of the population distribution affect the applicability of the Central Limit Theorem? What lesson can you take for your practice of statistics? Name a variable you would be interested in measuring that has a substantially skewed distribution. One definition of a heavy tailed distribution is one with infinite variance. For example, you can use the rcauchy command in R to take draws from a Cauchy distribution, which has heavy tails. Do you think a “heavy tails” distribution will follow the CLT? What leads you to this intuition? "],["hypothesis-testing.html", "Unit 6 Hypothesis Testing", " Unit 6 Hypothesis Testing update! "],["learning-objectives-5.html", "6.1 Learning Objectives", " 6.1 Learning Objectives "],["class-announcements-4.html", "6.2 Class Announcements", " 6.2 Class Announcements Test 2 is this week Includes units 4-5 (not 6) There is another practice test on Gradescope Lab 1 starts after live session 7 2 week lab You will work in a group to conduct hypothesis tests "],["roadmap-3.html", "6.3 Roadmap", " 6.3 Roadmap Looking Backwards Statisticians create a model to represent the world We saw examples of estimators, which approximate model parameters we’re interested in. By itself, an estimate isn’t much good; we need to capture the uncertainty in the estimate. We’ve seen two ways to express uncertainty in an estimator: standard errors and confidence intervals. Today We introduce hypothesis testing A hypothesis test also captures uncertainty, but in relation to a specific hypothesis. Looking Ahead We’ll build on the one-sample t-test, to introduce several other statistical tests. We’ll see how to choose a test from different alternatives, with an eye on meeting the required assumptions, and maximizing power. "],["discussion.html", "6.4 Discussion", " 6.4 Discussion 6.4.1 Discussion Questions 1 What are the two possible outcomes of a hypothesis test? What guarantee do you get if you follow the decision rules properly? Why do we standardize the mean to create a test statistic? \\[ t = \\frac{ \\overline{X}_n - \\mu}{\\sqrt{\\frac{s^2}{n}}} \\] 6.4.2 Discussion Questions 2 Explain this joke: "],["manual-computation-of-a-t-test.html", "6.5 Manual Computation of a t-Test", " 6.5 Manual Computation of a t-Test In a warehouse full of power packs labeled as 12 volts we randomly measured the voltage of 7. Here is the data: voltage &lt;- c(11.77, 11.90, 11.64, 11.84, 12.13, 11.99, 11.77) Find the mean and the standard deviation. Using qt(), compute the t critical value for a hypothesis test for this sample. (Following convention, set \\(\\alpha = .05\\).) Define a test statistic, \\(t\\), for testing whether the population mean is 12. Calculate the p-value using the t statistic. Should you reject the null? Argue this in two different ways. Suppose you were to use a normal distribution instead of a t-distribution to test your hypothesis. What would your p-value be for the z-test? Without actually computing it, say whether a 95% confidence interval for the mean would include 12 volts. Compute a 95% confidence interval for the mean. "],["data-exercise.html", "6.6 Data Exercise", " 6.6 Data Exercise t-Test Micro Cheat Sheet Key t-Test Assumptions Metric variable IID No major deviations from normality, considering sample size Testing the Home Team Advantage The file athlet2.Rdata contains data on college football games. The data is provided by Wooldridge and was collected by Paul Anderson, an MSU economics major, for a term project. Football records and scores are from 1993 football season. load(&quot;data/athlet2.RData&quot;) data ## dscore dinstt doutstt htpriv vtpriv dapps htwrd vtwrd dwinrec dpriv ## 1 10 -409 -4679 0 0 -1038 1 1 0 0 ## 2 -14 NA -66 0 0 -7051 1 1 0 0 ## 3 23 -654 -637 0 0 6209 1 0 1 0 ## 4 8 -222 456 0 0 -129 1 1 0 0 ## 5 -12 -10 208 0 0 794 1 1 0 0 ## 6 7 494 17 0 0 411 0 0 0 0 ## 7 -21 2 2 0 0 -4363 1 1 0 0 ## 8 -5 96 -333 0 0 1144 1 0 1 0 ## 9 -3 223 2526 0 0 3956 0 0 0 0 ## 10 -32 -20 0 0 0 -641 0 1 -1 0 ## 11 9 66 0 0 0 -278 1 0 1 0 ## 12 1 56 -346 0 0 -2223 1 0 1 0 ## 13 7 556 717 0 0 -5217 1 0 1 0 ## 14 -20 169 -461 0 0 1772 0 1 -1 0 ## 15 35 -135 396 0 0 85 1 0 1 0 ## 16 35 -40 0 0 0 -988 1 0 1 0 ## 17 -25 24 0 0 0 -8140 1 1 0 0 ## 18 -9 90 0 0 0 8418 0 1 -1 0 ## 19 -33 27 900 0 0 -3273 0 0 0 0 ## 20 7 -89 -31 0 0 1906 1 0 1 0 ## 21 -3 536 2352 0 0 -151 1 1 0 0 ## 22 -6 13261 9111 1 0 -9936 1 1 0 1 ## 23 -29 13809 10076 1 0 -6265 0 1 -1 1 ## 24 14 -17631 -10589 0 1 1252 1 0 1 -1 ## 25 -18 14885 9983 1 0 -4529 1 1 0 1 ## 26 48 -15220 -11400 0 1 -318 1 0 1 -1 ## 27 -3 99 -29 0 0 -797 0 1 -1 0 ## 28 -3 -54 -88 0 0 -372 0 1 -1 0 ## 29 -3 -98 -4175 1 0 2460 1 1 0 1 ## 30 2 -304 2987 0 1 -3035 1 1 0 -1 We are especially interested in the variable, dscore, which represents the score differential, home team score - visiting team score. We would like to test whether a home team really has an advantage over the visiting team. The instructor will assign you to one of two teams. Team 1 will argue that the t-test is appropriate to this scenario. Team 2 will argue that the t-test is invalid. Take a few minutes to examine the data, then formulate your best argument. Should you perform a one-tailed test or a two-tailed test? What is the strongest argument for your answer? Execute the t-test and interpret every component of the output. Based on your output, suggest a different hypothesis that would have led to a different test result. Try executing the test to confirm that you are correct. "],["assumptions-behind-the-t-test.html", "6.7 Assumptions Behind the t-test", " 6.7 Assumptions Behind the t-test For the following scenarios, what is the strongest argument against the validity of a t-test? You have a sample of 50 CEO salaries, and you want to know whether the mean salary is greater than $1 million. A nonprofit organization measures the percentage of students that pass an 8th grade reading test in 40 neighboring California counties. You are interested in whether the percentage of students that pass in California is over 80% You have survey data in which respondents assess their own opinion of corgis, with options ranging from “1 - extreme disgust” to “5 - affection so intense it threatens my career.” You want to know whether people on the average like corgis more than 3, representing neutrality. "],["comparing-two-groups.html", "Unit 7 Comparing Two Groups", " Unit 7 Comparing Two Groups "],["learning-objectives-6.html", "7.1 Learning Objectives", " 7.1 Learning Objectives "],["class-announcements-5.html", "7.2 Class Announcements", " 7.2 Class Announcements Great work completing your final w203 test! There is no unit 7 homework! The Hypothesis Testing Lab is released today! Lab is due at Unit 09 Live Session (two weeks) Group lab in two parts: Part 1: Work as a team to engage the fundamentals of hypothesis tests Part 2: Apply these fundamentals to analyze 2020 election data and write a single, three-page analysis "],["roadmap-4.html", "7.3 Roadmap", " 7.3 Roadmap Rearview Mirror Statisticians create a population model to represent the world A population model has parameters we are interested in Ex: A parameter might represent the effect that a vitamin has on test performance A null hypothesis is a specific statement about a parameter Ex: The vitamin has zero effect on performance A hypothesis test is a procedure for rejecting or not rejecting a null, such the probability of a type 1 error is constrained. Today There are often multiple hypothesis tests you can apply to a scenario. Our primary concern is choosing a test with assumptions we can defend. Secondarily, we want to maximize power. Looking Ahead Next week, we start working with models for linear regression We will see how hypothesis testing is also used for regression parameters. "],["teamwork-discussion.html", "7.4 Teamwork Discussion", " 7.4 Teamwork Discussion Working on Data Science Teams Data science is a beautiful combination of team-work and individual-work Teams: Define research ambitions and scope Imagine/envision the landscape of what is possible Support, discuss, review and integrate individual contributions Individuals: Conduct the heads-down work that moves question answering forward The Problematic Psychology of Data Science People talk about the impostor syndrome – a feeling of inadequacy or interloping that is sometimes also associated with a fear of under-performing relative to the expectation of others on the team. These emotions are common through data science, academics, computer science. But, the emotions are also commonplace in journalism, film-making, and public speaking What might be generating these feelings? What Makes an Effective Team? This reading on effective teams summarizes academic research to argue: What really matters is less about who is on the team, and more about how the team works together. In your live session, your section might take 7 minutes to read this brief, reading the problem statement, the proposed solution, and the framework for team effectiveness (stopping at the section titled “Tool: Help teams determine their own needs.”) “Psychological safety refers to an individual’s perception of the consequences of taking an interpersonal risk. It is a belief that a team is safe for risk taking in the face of being seen as ignorant, incompetent, negative, or disruptive.” “In a team with high psychological safety, teammates feel safe to take risks around their team members. They feel confident that no one on the team will embarrass or punish anyone else for admitting a mistake, asking a question, or offering a new idea.” 7.4.1 We All Belong From your experience, can you give an example of taking a personal risk as part of a team? Can you describe your emotions when contemplating this risk? If you did take the risk, how did the reactions of your teammates affect you? Knowing the circumstances that generate feelings of anxiety – what steps can we take as a section, or a team, to recognize and respond to these circumstances? How can you add to the psychological safety of your peers in the section and lab teammates? Only 26% of data science jobs are held by women. Morgan Ames (a professor in the program, teaching w231), in The Charisma Machine (available for free through the library, and with the introduction chapter here) studies the One Laptop Per Child initiative, and argues that the failure of OLPC is (another) example of: Sociological barriers that make broad-based, inclusive collaboration in tech challenging. “One Laptop Per Child implicitly invokes the social imaginary of the technically precocious boy… This imaginary shows a “natural” mastery of technical toys as well as a particular kind of rebellious sensibility that enables technical tinkering—but is still exclusionary by connecting technical prowess to boys in particular.” “In contrast, I found that each [successful] student had a constellation of resources that encouraged them down this path: families that steered them toward creative and critical thinking, a focus on the importance of education, and in many cases another computer in the home. This account circumscribes the role of technology, [… and] instead highlights the importance of social worlds.” In the face of uneven challenges faced by under-represented peoples in data science, what actions can we take to produce an inclusive, integrated data science team? "],["team-kick-off.html", "7.5 Team Kick-Off", " 7.5 Team Kick-Off Lab 1 Teams Here are teams for Lab 1! Team Kick-Off Conversation In a 10 minute breakout with your team, please discuss the following questions: How much time will you invest in the lab each week? How often will you meet and for how long? How will you discuss, review, and integrate individual work into the team deliverable? What do you see as the biggest risks when working on a team? How can you contribute to an effective team dynamic? "],["a-quick-review.html", "7.6 A Quick Review", " 7.6 A Quick Review Review of Key Terms Define each of the following: Population Parameter Null Hypothesis Test Statistic Null Distribution Comparing Groups Review Take a moment to recall the tests you learned this week. Here is a quick cheat-sheet to their key assumptions. paired/unpaired parametric non-parametric unpaired unpaired t-test - metric var - i.i.d. - (not too un-)normal Wicoxon rank-sum ordinal var i.i.d. paired paired t-test metric var i.i.d. (not too un-)normal Wicoxon signed-rank metric var i.i.d. difference is symmetric sign test ordinal var i.i.d. "],["comparing-groups-r-exercise.html", "7.7 Comparing Groups R Exercise", " 7.7 Comparing Groups R Exercise The General Social Survey (GSS) is one of the longest running and extensive survey projects in the US. The full dataset includes over 1000 variables spanning demographics, attitudes, and behaviors. The file GSS_w203.RData contains a small selection of a variables from the 2018 GSS. To learn about each variable, you can enter it into the search bar at the GSS data explorer load(&#39;data/GSS_w203.RData&#39;) summary(GSS) ## rincome happy sexnow wwwhr emailhr ## $25000 or more: 851 very happy : 701 women :758 Min. : 0.00 Min. : 0.000 ## $20000 - 24999: 107 pretty happy :1307 man :640 1st Qu.: 3.00 1st Qu.: 0.000 ## $10000 - 14999: 94 not too happy: 336 transgender : 2 Median : 8.00 Median : 2.000 ## $15000 - 19999: 61 DK : 0 a gender not listed here: 1 Mean : 13.91 Mean : 7.152 ## lt $1000 : 33 IAP : 0 Don&#39;t know : 0 3rd Qu.: 20.00 3rd Qu.: 10.000 ## (Other) : 169 NA : 0 (Other) : 0 Max. :140.00 Max. :100.000 ## NA&#39;s :1033 NA&#39;s : 4 NA&#39;s :947 NA&#39;s :986 NA&#39;s :929 ## socrel socommun numpets tvhours major1 ## sev times a week:382 never :510 Min. : 0.000 Min. : 0.000 business administration: 138 ## sev times a mnth:287 once a month :243 1st Qu.: 0.000 1st Qu.: 1.000 education : 79 ## once a month :259 sev times a week:219 Median : 1.000 Median : 2.000 engineering : 54 ## sev times a year:240 sev times a year:196 Mean : 1.718 Mean : 2.938 nursing : 51 ## almost daily :217 sev times a mnth:174 3rd Qu.: 2.000 3rd Qu.: 4.000 health : 42 ## (Other) :171 (Other) :215 Max. :20.000 Max. :24.000 (Other) : 546 ## NA&#39;s :792 NA&#39;s :791 NA&#39;s :1201 NA&#39;s :793 NA&#39;s :1438 ## owngun ## yes :537 ## no :993 ## refused: 39 ## DK : 0 ## IAP : 0 ## NA : 0 ## NA&#39;s :779 You have a set of questions that you would like to answer with a statistical test. For each question: Choose the most appropriate test. List and evaluate the assumptions for your test. Conduct your test. Discuss statistical and practical significance. The Questions Do Americans with pets watch more or less TV than Americans without pets? Do economics majors watch more or less tv than computer science majors? Are Americans that own guns or Americans that don’t own guns more likely to have pets? Do Americans spend more time emailing or using the web? Are Americans with pets happier than Americans without pets? Do Americans spend more evenings with neighbors or with relatives? "],["ols-regression-estimates.html", "Unit 8 OLS Regression Estimates", " Unit 8 OLS Regression Estimates "],["learning-objectives-7.html", "8.1 Learning Objectives", " 8.1 Learning Objectives "],["class-announcements-6.html", "8.2 Class Announcements", " 8.2 Class Announcements Lab 1 is due next week. There is no HW 8. We will have HW 9 as usual. You’re doing great - keep it up! "],["roadmap-5.html", "8.3 Roadmap", " 8.3 Roadmap Rear-View Mirror Statisticians create a population model to represent the world. Sometimes, the model includes an “outcome” random variable \\(Y\\) and “input” random variables \\(X_1, X_2,...,X_k\\). The joint distribution of \\(Y\\) and \\(X_1, X_2,...,X_k\\) is complicated. The best linear predictor (BLP) is the canonical way to summarize the relationship. Today OLS regression is an estimator for the BLP We’ll discuss the mechanics of OLS Looking Ahead To make regression estimates useful, we need measures of uncertainty (standard errors, tests…). The process of building a regression model looks different, depending on whether the goal is prediction, description, or explanation. "],["regression-discussion.html", "8.4 Regression Discussion", " 8.4 Regression Discussion 8.4.1 Discussion Questions Suppose we have random variables \\(X\\) and \\(Y\\). Why do we care about the BLP? What assumptions are needed for OLS to consistently estimate the BLP? What assumptions are needed in terms of causality (\\(X\\) causes \\(Y\\), \\(Y\\) causes \\(X\\), etc.) in order to compute the regression of \\(Y\\) on \\(X\\)? 8.4.2 Reasoning by Analogies Here are some phrases about regression “in the population.” Convert each of them to its sample counterpart. Population :: Sample Error \\(\\epsilon\\) :: residuals :: \\(e_{i}\\) The BLP is the predictor that minimizes expected squared error. \\(\\beta_1 = \\frac{Cov[X,Y]}{V[X]}\\). \\(Cov[X, \\epsilon] = 0\\) \\(E[\\epsilon] = 0\\) The population moment conditions uniquely specify one line, which is the BLP. "],["coding-activityr-cheat-sheet.html", "8.5 Coding Activity:R Cheat Sheet", " 8.5 Coding Activity:R Cheat Sheet Suppose x and y are variables in dataframe d. To fit an ols regression of Y on X: mod &lt;- lm(y ~ x, data = d) To access coefficients from the model object: mod$coefficients or coef(mod) To access fitted values from the model object: mod$fitted or fitted(mod) or predict(mod) To access residuals from the model object: mod$residuals or resid(mod) To create a scatterplot that includes the regression line: plot(d[&#39;x&#39;], d[&#39;y&#39;]) abline(mod) or d %&gt;% ggplot() + aes(x = x, y = y) + geom_point() + geom_smooth(method = lm) "],["r-exercise.html", "8.6 R Exercise", " 8.6 R Exercise Real Estate in Boston The file hprice1.Rdata contains 88 observations of homes in the Boston area, taken from the real estate pages of the Boston Globe during 1990. This data was provided by Wooldridge. load(&#39;data/hprice1.RData&#39;) # provides 3 objects head(data) ## price assess bdrms lotsize sqrft colonial lprice lassess llotsize lsqrft ## 1 300.000 349.1 4 6126 2438 1 5.703783 5.855359 8.720297 7.798934 ## 2 370.000 351.5 3 9903 2076 1 5.913503 5.862210 9.200593 7.638198 ## 3 191.000 217.7 3 5200 1374 0 5.252274 5.383118 8.556414 7.225482 ## 4 195.000 231.8 3 4600 1448 1 5.273000 5.445875 8.433811 7.277938 ## 5 373.000 319.1 4 6095 2514 1 5.921578 5.765504 8.715224 7.829630 ## 6 466.275 414.5 5 8566 2754 1 6.144775 6.027073 9.055556 7.920810 Are there variables that would not be valid outcomes for an OLS regression? If so, why? Are there variables that would not be valid inputs for an OLS regression? If so, why? 8.6.1 Assess the Relationship between Price and Square Footage Suppose that you’re interested in knowing the relationship between price and square footage. Assess the assumptions of the Large-Sample Linear Model. Create a scatterplot of price and sqrft. Like every plot you make, ensure that the plot minimally has a title and meaningful axes. Find the correlation between the two variables. Recall the equation for the slope of the OLS regression line – here you can either use Variance and Covariance, or if you’re bold, the linear algebra. Compute the slope manually (without using lm()) Regress price on sqrft using the lm function. This will produce an estimate for the following model: \\[ price = \\beta_{0} + \\beta_{1} sqrft + e \\] Create a scatterplot that includes the fitted regression. Interpret what the coefficient means. State what features you are allowing to change and what features you’re requiring do not change. For each additional square foot, how much more (or less) is the house worth? Estimate a new model (and save it into another object) that includes the size of the lot and whether the house is a colonial. This will estimate the model: \\[ price = \\beta_{0} + \\beta_{1} sqrft + \\beta_{2} lotsize + \\beta_{3} colonial? + e \\] BUT BEFORE YOU DO, make a prediction: What do you think is going to happen to the coefficient that relates square footage and price? Will the coefficient increase, decrease, or stay the same? Compute the sample correlation between \\(X\\) and \\(e_i\\). What guarantees do we have from the book about this correlation? Does the data seem to bear this out? "],["ols-regression-inference.html", "Unit 9 OLS Regression Inference", " Unit 9 OLS Regression Inference "],["learning-objectives-8.html", "9.1 Learning Objectives", " 9.1 Learning Objectives "],["class-announcements-7.html", "9.2 Class Announcements", " 9.2 Class Announcements Congratulations on finishing your first lab! The next (and the last) lab is coming up in two weeks. Homework 09 has been assigned today, and it’s due in a week. "],["roadmap-6.html", "9.3 Roadmap", " 9.3 Roadmap Rear-View Mirror Statisticians create a population model to represent the world. Sometimes, the model includes an “outcome” random variable \\(Y\\) and “input” random variables \\(X_1, X_2,...,X_k\\). The joint distribution of \\(Y\\) and \\(X_1, X_2,...,X_k\\) is complicated. The best linear predictor (BLP) is the canonical way to summarize the relationship. OLS provides a point estimate of the BLP Today Robust Standard Error: quantify the uncertainty of OLS coefficients Hypothesis testing with OLS coefficients Bootstrapping Looking Ahead Regression is a foundational tool that can be applied to different contexts The process of building a regression model looks different, depending on whether the goal is prediction, description, or explanation. "],["uncertainty-in-ols.html", "9.4 Uncertainty in OLS", " 9.4 Uncertainty in OLS 9.4.1 Discussion Questions List as many differences between the BLP and the OLS line as you can. In the following regression table, explain in your own words what the standard error in parentheses means. outcome: sleep hours mg. melatonin 0.52 (0.31) 9.4.2 Understanding Uncertainty Under the relatively stricter assumptions of constant error variance, the variance of a slope coefficient is given by \\[ V(\\hat{\\beta_j}) = \\frac{\\sigma^2}{SST_j (1-R_j^2)} \\] A similar formulation is given in FOAS as definition 4.2.3, \\[ \\hat{V}_{C}[\\hat{\\beta}] = \\hat{\\sigma}^2 \\left( X^{T} X \\right)^{-1} \\rightsquigarrow \\frac{\\hat{\\sigma}^{2}}{\\left( X^{T}X\\right)} \\] Explain why each term makes the variance higher or lower: \\(\\sigma^2\\) is the variance of the error \\(\\epsilon\\) \\(SST_j\\) is (unscaled) variance of \\(X_j\\) \\(R_j^2\\) is \\(R^2\\) for a regression of \\(X_j\\) on the other \\(X\\)’s "],["r-exercise-1.html", "9.5 R Exercise", " 9.5 R Exercise Real Estate in Boston The file hprice1.RData contains 88 observations of homes in the Boston area, taken from the real estate pages of the Boston Globe during 1990. This data was provided by Wooldridge. load(&#39;data/hprice1.RData&#39;) # provides 3 objects Last week, we fit a regression of price on square feet. model_one &lt;- lm(price ~ sqrft, data = data) model_one ## ## Call: ## lm(formula = price ~ sqrft, data = data) ## ## Coefficients: ## (Intercept) sqrft ## 11.2041 0.1402 Questions Estimate a new model (and save it into another object) that includes the size of the lot and whether the house is a colonial. This will estimate the model: \\[ price = \\beta_{0} + \\beta_{1} sqrft + \\beta_{2} lotsize + \\beta_{3} colonial? + e \\] BUT BEFORE YOU DO, make a prediction: What do you think is going to happen to the coefficient that relates square footage and price? Will the coefficient increase, decrease, or stay the same? Will the uncertainty about the coefficient increase, decrease, or stay the same? Conduct an F-test that evaluates whether the model as a whole does better when the coefficients on colonial and lotsize are allowed to estimate freely, or instead are restricted to be zero (i.e. \\(\\beta_{2} = \\beta_{3} = 0\\). Use the function vcovHC from the sandwich package to estimate (a) the the heteroskedastic consistent (i.e. “robust”) variance covariance matrix; and (b) the robust standard errors for the intercept and slope of this regression. Recall, what is the relationship between the VCOV and SE in a regression? Perform a hypothesis test to check whether the population relationship between sqrft and price is zero. Use coeftest() with the robust standard errors computed above. Use the robust standard error and qt to compute a 95% confidence interval for the coefficient sqrft in the second model that you estimated. \\(price = \\beta_{0} + \\beta_{1} sqrft + \\beta_{2} lotsize + \\beta_{3} colonial\\). Bootstrap. The book very quickly talks about bootstrapping which is the process of sampling with replacement and fitting a model. The idea behind the bootstrap is that since the data is generated via an iid sample from the population, that you can simulate re-running your analysis by drawing repeated samples from the data that you have. Below is code that will conduct a boostrapping estimator of the uncertainty of the sqrft variable when lotsize and colonial are included in the model. bootstrap_sqft &lt;- function(d = data, number_of_bootstraps = 1000) { number_of_rows &lt;- nrow(d) coef_sqft &lt;- rep(NA, number_of_bootstraps) for(i in 1:number_of_bootstraps) { bootstrap_data &lt;- d[sample(x=1:number_of_rows, size=number_of_rows, replace=TRUE), ] estimated_model &lt;- lm(price ~ sqrft + lotsize + colonial, data = bootstrap_data) coef_sqft[i] &lt;- coef(estimated_model)[&#39;sqrft&#39;] } return(coef_sqft) } bootstrap_result &lt;- bootstrap_sqft(number_of_bootstraps = 1000) With this, it is possible to plot the distribution of these regression coefficients: ggplot() + aes(x = bootstrap_result) + geom_histogram() + labs( x = &#39;Estimated Coefficient&#39;, y = &#39;Count&#39;, title = &#39;Bootstrap coefficients for square footage&#39; ) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Compute the standard deviation of the bootstrapped regression coefficients. How does this compare to the robust standard errors you computed above? "],["descriptive-model-building.html", "Unit 10 Descriptive Model Building", " Unit 10 Descriptive Model Building "],["learning-objectives-9.html", "10.1 Learning Objectives", " 10.1 Learning Objectives "],["class-announcements-8.html", "10.2 Class Announcements", " 10.2 Class Announcements The Regression Lab begins next week. Your instructor will divide you into teams. As part of the lab, you will perform a statistical analysis using linear regression models. "],["roadmap-7.html", "10.3 Roadmap", " 10.3 Roadmap Rearview Mirror Statisticians create a population model to represent the world. The BLP is a useful way to summarize the relationship between one outcome random variable \\(Y\\) and input random varibles \\(X_1,...,X_k\\) OLS regression is an estimator for the Best Linear Predictor (BLP) We can capture the sampling uncertainty in an OLS regression with standard errors, and tests for model parameters. Today The research goal determines the strategy for building a linear model. Description means summarizing or representing data in a compact, human-understandable way. We will capture complex relationships by transforming data, including using indicator variables and interaction terms. Looking Ahead We will see how model building for explanation is different from building for description. The famous Classical Linear Model (CLM) allows us to apply regression to smaller samples. "],["discussion-1.html", "10.4 Discussion", " 10.4 Discussion 10.4.1 Three modes of model building Recall the three major modes of model building: Prediction, Description, Explanation. What is the appropriate mode for each of the following questions? What is going on? Why is something going on? What is going to happen? Think of a research question you are interested in. Which mode is it aligned with? 10.4.2 The statistical modeling process in different modes How does the modeling goal influence each of the following steps in the statistical modeling process? Choice of variables and transformation Choice of model (ols regression, neural nets, random forest, etc.) Model evaluation "],["r-activity-measuring-the-return-to-education.html", "10.5 R Activity: Measuring the return to education", " 10.5 R Activity: Measuring the return to education In labor economics, a key concept is returns to education. Our goal is description: what is the relationship between education and wages? We will proceed in two steps: First, we will discuss what the appropriate specifications are. Then we will estimate the different models to answer this question. We will use wage1 dataset in the wooldridge package in the following sections. #?wage1 #names(wage1) 10.5.1 Transformations 10.5.1.1 Applying and Interpreting Logarithms Which of the following specifications best capture the relationship between education and hourly wage? (Hint: Do a quick a EDA) level-level: \\(wage = \\beta_0 + \\beta_1 educ + u\\) Level-log: \\(wage = \\beta_0 + \\beta_1 \\ln(educ) + u\\) log-level: \\(\\ln(wage) = \\beta_0 + \\beta_1 educ + u\\) log-log: \\(\\ln(wage) = \\beta_0 + \\beta_1 \\ln(educ) + u\\) What is the interpretation of \\(\\beta_0\\) and \\(\\beta_1\\) in your selected specification? Can we use \\(R^2\\) or Adjusted \\(R^2\\) to choose between level-level or log-level specifications? Remember Doing a log transformation for any reason essentially implies a fundamentally different relationship between outcome (Y) and predictor (X) that we need to capture 10.5.1.2 Applying and Interpreting Polynomials The following specifications include two control variables: years of experience (exper) and years at current company (tenure). Do a quick EDA and select the specification that better suits our description goal. \\(wage = \\beta_0 + \\beta_1 educ + \\beta_2 exper + \\beta_3 tenure + u\\) \\(\\begin{aligned} wage &amp;= \\beta_0 + \\beta_1 educ + \\beta_2 exper + \\beta_3 exper^2 + \\\\ &amp; \\beta_4 tenure + \\beta_5 tenure^2 + u \\end{aligned}\\) How do you interpret the \\(\\beta\\) coefficients? 10.5.1.3 Applying and Interpreting Indicator variables and interaction terms In the following models, first, explain why the indicator variables or interaction terms have been included. Then identify the reference group (if any) and interpret all coefficients. \\(wage = \\beta_0 + \\beta_1 educ + \\beta_2 I(educ \\geq 12) + u\\) \\(wage = \\beta_0 + \\beta_1 educ + \\beta_2 female + u\\) \\(wage = \\beta_0 + \\beta_1 educ + \\beta_2 female + \\beta_3 educ*female + u\\) \\(\\begin{aligned} wage &amp;= \\beta_0 + \\beta_1 female + \\beta_2 I(educ = 2) + \\beta_3 I(educ = 3)\\\\ &amp;...+ \\beta_{20} I(educ = 20) + u\\\\ \\end{aligned}\\) 10.5.2 Estimation Estimating Returns to Education Answer the following questions using an appropriate hypothesis test. Is a year of education associated with changes to hourly wage? (Include experience and tenure without polynomial terms). Is the association between wage and experience / wage and tenure non-linear? Is there evidence for gender wage discrimination in the U.S.? Is there any evidence for a graduation effect on wage? Display all estimated models in a regression table, and discuss the robustness of your results. "],["explanatory-model-building.html", "Unit 11 Explanatory Model Building", " Unit 11 Explanatory Model Building "],["learning-objectives-10.html", "11.1 Learning Objectives", " 11.1 Learning Objectives "],["class-announcements-9.html", "11.2 Class Announcements", " 11.2 Class Announcements Lab 2-Regression Overview Setting: You are data scientists for a maker of products. Task: You select your own research question Your X should be an aspect of product design Your Y should be a metric of product success Deliverable: A statistical analysis that includes An introduction that motivates your research question A description of your model-building process A discussion of statistical assumptions that may be problematic A well-formatted regression table with a minimum of 3 specifications A conclusion that extracts key lessons from your statistical results The Report - Writing for a collaborating data scientist, what research question have you asked, what answers have you found, and how did you find them? Deliverable Name Week Due Grade Weight Research Proposal Week 12 10% Within-Team Review Week 12 5% Final Presentation Week 14 10% Final Report Week 14 75% Team Work Evaluation Most data science work happens on teams. Our educational goals include helping you improve in your role as a teammate. We’ll ask you to fill out a confidential evaluation regarding your team dynamics. Final Presentation Team will present their work in live session 14. Teams have between 10-15 min dedicated to discussing their work (depending on section size) Two-thirds of the time can be the team presenting BUT at least one-third should be asking and answering questions with your peers For example, if teams have 15 minutes total, then plan to present for no more than 10 minutes and structure 5 minutes of questions. "],["roadmap-8.html", "11.3 Roadmap", " 11.3 Roadmap Rearview Mirror Statisticians create a population model to represent the world. The BLP is a useful way to summarize relationships in a model, and OLS regression is a way to estimate the BLP. OLS regression is a foundational tool that can be applied to questions of description Today Questions of explanation require a substantially different modeling process. To answer causal questions, we must work within a causal theory OLS regression is sometimes appropriate for measuring a causal effect, But, only when the model estimated matches the causal theory. So, we must watch out for omitted variable bias, reverse causality, and outcome variables on the right hand side. Looking Ahead The famous Classical Linear Model (CLM) allows us to apply regression to smaller samples. We will address the pervasive issue of false discovery, and ways to be a responsible member of the scientific community. "],["discussion-2.html", "11.4 Discussion", " 11.4 Discussion 11.4.1 Path Diagrams \\[ \\begin{matrix} \\\\ \\text{Sleep} \\rightarrow \\text{Feelings of Stress} \\\\ \\\\ \\end{matrix} \\] How would the following fit into this causal path diagram? All the other factors in the world that also cause stress but don’t have a causal relationship with sleep. A factor: Coffee Intake What happens if you omit it in your regression? Reverse causality An outcome variable on the RHS: Job Performance What happens if you include it in your regression? 11.4.2 Omitted Variable Bias Recall the equation for omitted variable bias What specific regressions do \\(\\beta_2\\) and \\(\\gamma_1\\) come from? "],["r-exercise-2.html", "11.5 R Exercise", " 11.5 R Exercise Omitted Variable Bias in R The file htv.RData contains data from the 1991 National Longitudinal Survey of Youth, provided by Wooldridge. All people in the sample are males age 26 to 34. The data is interesting here, because it includes education, stored in the variable educ, and also a score on an ability test, stored in the variable abil. Assume that the true model is, Questions: 1- Are we able to directly measure ability? If so, how would you propose to measure it? 2- If not, what do we measure and how is this measurement related to ability? And there is a lot of evidence to suggest that standardized tests are not a very good proxy. But for now, let’s pretend that we really are measuring ability. 3- Using R, estimate (a) the true model, and (b) the regression of ability on education. Write down the expression for what omitted variable bias would be if you couldn’t measure ability. Add this omitted variable bias to the coefficient for education to see what it would be. 4- Now evaluate your previous result by fitting the model, \\[wage = \\alpha_0 + \\alpha_1 educ + w\\] Does the coefficient for the relationship between education and wages match what you estimated earlier? Why or why not? 5- Reflect on your results: What does the direction of omitted variable bias suggest about OLS estimates of returns to education? What does this suggest about the reported statistical significance of education? "],["discussion-3.html", "11.6 Discussion", " 11.6 Discussion The Direction of Omitted Variable Bias For each regression, estimate whether omitted variable bias is towards zero or away from zero. Regression Output Omitted Variable \\(\\widehat{grade} = 72.1 + 0.4\\ attendance\\) \\(time\\_studying\\) \\(\\widehat{lifespan} = 87.4 - 1.2\\ cigarettes\\) \\(exercise\\) \\(\\widehat{lifespan} = 87.4 - 1.2\\ cigarettes\\) \\(time\\_socializing\\) \\(\\widehat{wage} = 14.0 + 2.1\\ grad\\_education\\) \\(experience\\) \\(\\widehat{wage} = 14.0 + 2.1\\ grad\\_education\\) desire to effect \\(social\\_good\\) \\(\\widehat{literacy} = 54 + 12\\ network\\_access\\) \\(wealth\\) "],["the-classical-linear-model.html", "Unit 12 The Classical Linear Model ", " Unit 12 The Classical Linear Model "],["learning-objectives-11.html", "12.1 Learning Objectives", " 12.1 Learning Objectives "],["class-announcements-10.html", "12.2 Class Announcements", " 12.2 Class Announcements Lab 2 Deliverable and Dates Research Proposal (Today) Within-Team Review (Today) Final Report (Week 14) Final Presentation (Week 14) "],["roadmap-9.html", "12.3 Roadmap", " 12.3 Roadmap Rearview Mirror Statisticians create a population model to represent the world. The BLP is a useful summary for a relationship among random variables. OLS regression is an estimator for the Best Linear Predictor (BLP). For a large sample, we only need two mild assumptions to work with OLS To know coefficients are consistent To have valid standard errors, hypothesis tests Today The Classical Linear Model (CLM) allows us to apply regression to smaller samples. The CLM requires more to be true of the data generating process, to make coefficients, standard errors, and tests meaningful in small samples. Understanding if the data meets these requirements (often called assumptions) requires considerable care. Looking Ahead The CLM – and the methods that we use to evaluate the CLM – are the basis of advanced models (inter alia time-series) (Week 13) In a regression studies (and other studies), false discovery is a widespread problem. Understanding its causes can make you a better member of the scientific community. "],["the-classical-linear-model-1.html", "12.4 The Classical Linear Model", " 12.4 The Classical Linear Model 12.4.1 Comparing the Large Sample Model and the CLM Part I We say that in small samples, more needs be true of our data for OLS regression to “work.” What do we mean when we say “work”? If our goals are descriptive, how is a “working” estimator useful? If our goals are explanatory, how is a “working” estimator useful? If our goals are predictive, are the requirements the same? Part II Suppose that you’re interested in understanding how subsidized school meals benefit under-resourced students. Using the tools from 201, refine this question to a data science question. Suppose that to answer the question you have identified, there are two data sources: Individual-level data about income, nutrition and test scores, self-reported by individual families who have opted in to the study. Government data about school district characteristics, including district-level college achievement; county-level home prices, and state-level tax receipts. What are the tradeoffs to these different sources? Part III Suppose you use individual-level data (you have a large sample). Which of the large-sample assumptions do you expect are valid, and which are problematic? Say you use school-district-level data (you have a small sample). Which of the CLM assumptions do you expect are valid, and which do you expect are most problematic? Which dataset do you think will give you more precise estimates? "],["problems-with-the-clm-requirements.html", "12.5 Problems with the CLM Requirements", " 12.5 Problems with the CLM Requirements There are five requirements for the CLM IID Sampling Linear Conditional Expectation No Perfect Collinearity Homoskedastic Errors Normally Distributed Errors For each of these requirements: Identify one concrete way that the data might not satisfy the requirement. Identify what the consequence of failing to satisfy the requirement would be. Identify a path forward to satisfy the requirement. "],["r-exercise-3.html", "12.6 R Exercise", " 12.6 R Exercise library(tidyverse) library(wooldridge) library(car) library(lmtest) library(sandwich) library(stargazer) If you haven’t used the mtcars dataset, you haven’t been through an intro applied stats class! In this analysis, we will use the mtcars dataset which is a dataset that was extracted from the 1974 Motor Trend US magazine, and comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973-74 models). The dataset is automatically available when you start R. For more information about the dataset, use the R command: help(mtcars) data(mtcars) glimpse(mtcars) ## Rows: 32 ## Columns: 11 ## $ mpg &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8, 16.4, 17.3, 15.2, 10.4, 10.4, 14.7, … ## $ cyl &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8, 8, 8, 8, 4, 4, 4, 8, 6, 8, 4 ## $ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 167.6, 167.6, 275.8, 275.8, 275.8, 472.… ## $ hp &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180, 205, 215, 230, 66, 52, 65, 97, 150,… ## $ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92, 3.07, 3.07, 3.07, 2.93, 3.00, 3.23, … ## $ wt &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.440, 3.440, 4.070, 3.730, 3.780, 5.25… ## $ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18.30, 18.90, 17.40, 17.60, 18.00, 17.9… ## $ vs &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1 ## $ am &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1 ## $ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3, 3, 3, 3, 4, 5, 5, 5, 5, 5, 4 ## $ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2, 2, 4, 2, 1, 2, 2, 4, 6, 8, 2 Questions: Using the mtcars data, run a multiple linear regression to find the relationship between displacement (disp), gross horsepower (hp), weight (wt), and rear axle ratio (drat) on the miles per gallon (mpg). For each of the following CLM assumptions, assess whether the assumption holds. Where possible, demonstrate multiple ways of assessing an assumption. When an assumption appears violated, state what steps you would take in response. I.I.D. data Linear conditional expectation No perfect collinearity Homoskedastic errors Normally distributed errors In addition to the above, assess to what extent (imperfect) collinearity is affecting your inference. Interpret the coefficient on horsepower. Perform a hypothesis test to assess whether rear axle ratio has an effect on mpg. What assumptions need to be true for this hypothesis test to be informative? Are they? Choose variable transformations (if any) for each variable, and try to better meet the assumptions of the CLM (which also maintaining the readability of your model). (As time allows) report the results of both models in a nicely formatted regression table. "],["reproducible-research.html", "Unit 13 Reproducible Research ", " Unit 13 Reproducible Research "],["learning-objectives-12.html", "13.1 Learning Objectives", " 13.1 Learning Objectives "],["class-announcements-11.html", "13.2 Class Announcements", " 13.2 Class Announcements "],["roadmap-10.html", "13.3 Roadmap", " 13.3 Roadmap Rearview Mirror Today Looking Ahead "],["what-data-science-hopes-to-accomplish.html", "13.4 What data science hopes to accomplish", " 13.4 What data science hopes to accomplish As a data scientist, our goal is to learn about the world: Theorists and theologians build systems of explanations that are consistent with themselves Analysts build systems of explanations that are consistent with the past Scientists build systems of explanations that usefully predict events, or data, that hasn’t yet been seen "],["learning-from-data.html", "13.5 Learning from Data", " 13.5 Learning from Data As a data scientist, the way we learn about the world is through the streams of data that real world events produce Machine processes Political outcomes Customer actions The watershed moment in our field has been the profusion of data available, from many places, that is richer than at any other point in our past. In 251, and 266 we place structure on data series like audio, video and text that are transcendently rich In 261 we bring together flows of data that are generated at massive scales In 209 we ask, “How can we take data, and produce a new form of it that is most effectively understood by the human visual and interactive mind? "],["data-science-and-statistics.html", "13.6 Data Science and Statistics", " 13.6 Data Science and Statistics So why statistics? And why the way we’ve chosen to approach statistics in 203? "],["why-statistics-a-closing-argument-for-statistics.html", "13.7 Why Statistics?: A Closing Argument for Statistics", " 13.7 Why Statistics?: A Closing Argument for Statistics Business, policy, education and medical decisions are made by humans based on data A central task when we observe some pattern in data is to infer whether the pattern will occur in some novel context Statistics, as we practice it in 203, allows us to characterize: What we have seen What we could have seen Whether any guarantees exist about what we have seen What we can infer about the population So that we can either describe, explain or predict behavior. "],["course-goals.html", "13.8 Course Goals", " 13.8 Course Goals 13.8.1 Course Section III: Purpose-Driven Models Statistical models are unknowing transformations of data Because they’re built on the foundation of probability, we have certain guarantees what a model “says” Because they’re unknowing, the models themselves know-not what they say. As the data scientist, bring them alive to achieve our modeling goals In Lab 2 we have expanded our ability to parse the world using regression, built a model that accomplishes our goals, and done so in a way that brings the ability to test under a “null” scenario Key insight: regression is little more than conditional averages 13.8.2 Course Section II: Sampling Theory and Testing Under very general assumptions, sample averages follow a predictable, known, distribution – the Gaussian distribution This is true, even when the underlying probability distribution is very complex, or unknown! Due to this common distribution, we can produce reliable, general tests! In Lab 1 we computed simple statistics, and used guarantees from sampling theory to test whether these differences were likely to arise under a “null” scenario 13.8.3 Course Section I: Probability Theory Probability theory Underlies modeling and regression (Part III); Underlies sampling, inference, and testing (Part II) Every model built in every corner of data science We can: Model the complex world that we live in using probability theory; Move from a probability density function that is defined in terms of a single variable, into a function that is defined in terms of many variables Compute useful summaries – i.e. the BLP, expected value, and covariance – even with highly complex probability density functions. 13.8.4 Statistics as a Foundation for MIDS In w203, we hope to have laid a foundation in probability that can be used not only in statistical applications, but also in every other machine learning application that are likely to ever encounter "],["reproducibility-discussion.html", "13.9 Reproducibility Discussion", " 13.9 Reproducibility Discussion Green Jelly Beans What went wrong here? 13.9.1 Discussion Status Update You have a dataset of the number of Facebook status updates by day of the week. You run 7 different t-tests, one for posts on Monday (versus all other days), or for Tuesday (versus all other days), etc. Only the test for Sunday is significant, with a p-value of .045, so you throw out the other tests. Should you conclude that Sunday has a significant effect on number of posts? (How can you address this situation responsibly when you publish your results?) Such Update As before, you have a dataset of the number of Facebook status updates by day of the week. You do a little EDA and notice that Sunday seems to have more “status updates” than all other days, so you recode your “day of the week” variable into a binary one: Sunday = 1, All other days = 0. You run a t-test and get a p-value of .045. Should you conclude that Sunday has a significant effect on number of posts? Sunday Funday Suppose researcher A tests if Monday has an effect (versus all other days), Researcher B tests Tuesday (versus all other days), and so forth. Only Researcher G, who tests Sunday finds a significant effect with a p-value of .045. Only Researcher G gets to publish her work. If you read the paper, should you conclude that Sunday has a significant effect on number of posts? Sunday Repentence What if researcher G above is a sociologist that chooses to measure the effect of Sunday based on years of observing the way people behave on weekends? Researcher G is not interested in the other tests, because Sunday is the interesting day from her perspective, and she wouldn’t expect any of the other tests to be significant. Decreasing Effect Sizes Many observers have noted that as studies yielding statistically significant results are repeated, estimated effect sizes go down and often become insignificant. Why is this the case? "],["maximum-likelihood-estimation.html", "Unit 14 Maximum Likelihood Estimation", " Unit 14 Maximum Likelihood Estimation salvation mountain "],["learning-objectives-13.html", "14.1 Learning Objectives", " 14.1 Learning Objectives "],["class-announcements-12.html", "14.2 Class Announcements", " 14.2 Class Announcements "],["roadmap-11.html", "14.3 Roadmap", " 14.3 Roadmap Rearview Mirror: What We’ve Seen WLLN: \\(\\displaystyle\\lim_{n \\to \\infty} \\overline{X}_n \\overset{p}{=} E[X]\\) CLT \\(\\displaystyle\\lim_{n \\to \\infty} \\bar{X}_n \\overset{d}{=} N(E[X], \\text{Var}[X])\\) Today Use maximum likelihood to generate a good guess for model parameters; Use a confidence interval to indicate a range of plausible parameter values "],["what-is-a-model.html", "14.4 What is a model?", " 14.4 What is a model? A data science model is: A representation of the world built from random variables FOIS: “agnostic” models place minimal restrictions on joint distribution Parametric models (i.e. MLE) are models based on a family of distributions. \\(f_{Y|X}(y|\\mathbf{x}) \\sim g(y, \\mathbf{x}; \\mathbf{\\theta})\\) "],["estimation-1.html", "14.5 Estimation", " 14.5 Estimation We have the tools to use data to infer information about the (joint) distribution Because the joint distribution is complicated, we’ll usually estimate simpler summaries of the joint distribution – e.g. \\(E[X]\\), \\(V[X]\\), \\(E[Y|X]\\), \\(Cov[X,Y]\\) There are a number of techniques that you can use to develop an estimator for a parameter. These techniques vary in terms of the principle used to arrive at the estimator and the strength of the assumptions needed to support it. However, all of these estimators are statistics meaning they are functions of the data \\(\\{X_i\\}_{i=1}^n\\) "],["discussion-of-maximum-likelihood-estimation.html", "14.6 Discussion of Maximum Likelihood Estimation", " 14.6 Discussion of Maximum Likelihood Estimation What is the goal of estimating a parameter? Why is this something that we are interested in as data scientists? In your own words, describe how the method of maximum likelihood is used to estimate the unknown parameters. Why does a likelihood function have a \\(\\Pi\\) (product operator) within it? Is it possible to estimate using maximum likelihood without writing down a model for the data? What happens if your model for the data is wrong? Are your estimates for the parameters “incorrect”? Or, are they “correct” within the context of the model that you’ve written down? "],["optimization-in-r.html", "14.7 Optimization in R", " 14.7 Optimization in R The method of maximum likelihood requires an optimization routine. For a few very simple probability models, a closed-form solution exists and the MLE can be derived by hand. (This is also potentially the case for OLS regression.) But, instead lets use some machine learning to find the estimates that maximize the likelihood function. There are many optimizers (e.g. optimize, and optim). optimize is the simplest to use, but only works in one dimension. 14.7.1 Optimization Example: Optimum Price Suppose that a firm’s profit from selling a product is related to price, \\(p\\), and cost, \\(c\\), as follows: \\[ \\text{profit} = (p - p^2) - c + 100 \\] Explain how you would use calculus to find the maximizing price. Assume that cost is fixed. What is the firms revenue as p=0, cost = 2? What is it at p=10, cost = 2? Create a plot with the following characteristics: On the x-axis is a sequence (seq()) of prices from [0, 10]. On the y-axis is the revenue as a function of those prices. Hold cost constant at c=2. What does the best price seem to be? Solve this numerically in R, using the optimize() function. Take note: using the default arguments, will optimize try to find a maximum or a minimum? Check into the help documentation. profit &lt;- function(p, c) { r = (p - p^2) - c + 100 return(r) } profit(p=2, c=2) ## [1] 96 best_price &lt;- optimize( profit, # profit is the function lower = 0, upper = 1000, # this is the low and high we consider c = 2, # here, we&#39;re passing cost into profit maximum = TRUE) # we&#39;d like to maximize, not minimize best_price ## $maximum ## [1] 0.5 ## ## $objective ## [1] 98.25 "],["mle-for-poisson-random-variables.html", "14.8 MLE for Poisson Random Variables", " 14.8 MLE for Poisson Random Variables Suppose we use a camera to record an intersection for a particular length of time, and we write down the number of cars accidents in that interval. This process can be modeled by a Poisson random variable (now we are non-agnostic), that has a well-known probability mass function given by, \\[ f(x;\\lambda) = \\frac{\\lambda^x e^{-\\lambda}}{x!} \\] Here is an example of a string of outcomes generated by a Poisson RV, with parameter \\(\\lambda = 2\\). rpois(n = 10, lambda = 2) ## [1] 3 2 2 1 1 3 0 1 3 2 14.8.1 MLE for Poisson Random Variables: Data Suppose that we conduct an iid sample, and gather the following number of accidents. (It is a busy street!) data &lt;- c( 2, 6, 2, 1, 3, 3, 4, 4, 24, 1, 5, 4, 5, 1, 2, 2, 5, 2, 1, 5, 2, 1, 2, 9, 9, 1, 3, 2, 1, 1, 3, 1, 3, 2, 2, 4, 1, 1, 5, 3, 3, 2, 2, 1, 1, 1, 5, 1, 3, 1, 1, 1, 1, 2, 2, 4, 2, 1, 2, 2, 3, 1, 2, 6, 2, 2, 3, 2, 3, 5, 1, 3, 2, 5, 2, 1, 3, 2, 1, 2, 4, 2, 6, 1, 2, 2, 3, 5, 2, 1, 4, 2, 2, 1, 3, 2, 2, 4, 1, 1, 1, 1, 2, 3, 5, 1, 2, 2, 3, 1, 4, 1, 3, 2, 2, 2, 2, 2, 2, 3, 3, 1, 1, 2, 2, 4, 1, 5, 2, 7, 5, 2, 3, 2, 5, 3, 1, 2, 1, 1, 2, 3, 1, 5, 3, 4, 6, 3, 3, 2, 2, 1, 2, 2, 4, 2, 3, 4, 3, 1, 6, 3, 1, 2, 3, 2, 2, 3, 1, 1, 1, 1, 1, 10, 3, 2, 1, 1, 3, 2, 2, 3, 1, 1, 2, 2, 2, 4, 2, 2, 3, 3, 6, 1, 3, 2, 3, 2, 2, 2 ) table(data) ## data ## 1 2 3 4 5 6 7 9 10 24 ## 54 69 38 14 14 6 1 2 1 1 14.8.2 MLE Estimation Use the data that is stored in data, together with a Poisson model to estimate the \\(\\lambda\\) values that produce the “good” model from the Poisson family. That is, use MLE to estimate \\(\\lambda\\). Here is your work flow: Define your random variables. Write down the likelihood function for a sample of data that is generated by a Poisson process. To make the math easier, take the log of this likelihood function. Optimize this log-likelihood using calculus – what is the value of \\(\\lambda\\) that results? Compute this value, given the data that you have. Maximize this log-likelihood numerically, and report the value for \\(\\lambda\\) that produces the highest likelihood of seeing this data. Comment on your answers from parts 4 and 5. Are you surprised or not by what you see? poisson_ll &lt;- function(data, lambda) { ## fill this in: lambda # this is a placeholder, change this } search_space &lt;- seq(0,100, by = 0.1) plot( x = search_space, xlab = &#39;Search Space&#39;, y = poisson_ll(data=data, lambda=search_space), ylab = &#39;Log Likelihood&#39;, type = &#39;l&#39; ) # optimize(poisson_ll, lower = 0, upper = 100, data = data, maximum = TRUE) "],["confidence-intervals.html", "14.9 Confidence Intervals", " 14.9 Confidence Intervals This exercise is meant to demonstrate what the confidence level in a confidence interval represents. We will assume a standard normal population distribution and simulate what happens when we draw a sample and compute a confidence interval. Your task is to complete the following function so that it, simulates and storesn draws from a standard normal distribution based on those draws, computes a valid confidence interval with confidence level \\(\\alpha\\), a parameter that you pass to the function. Your function should return a vector of length 2, containing the lower bound and upper bound of the confidence interval. \\[ CI_{\\alpha} = \\overline{X} \\pm t_{\\alpha/2} \\cdot \\frac{s}{\\sqrt{n}} \\] where: \\(CI_\\alpha\\) is the confidence interval that you’re seeking to produce \\(\\overline{X}\\) is the sample average, \\(t_{\\alpha/2}\\) is your critical value (accessible through qt), and \\(s\\) is your sample standard deviation. Notice that you’ll need each of these pieces in the code that you’re about to write. sim_conf_int &lt;- function(n, alpha) { # Fill in your code to: # 1. simulate n draws from a standard normal dist. # 2. compute a confidence interval with confidence level alpha sample_draws &lt;- &#39;fill this in&#39; sample_mean &lt;- &#39;fill this in&#39; sample_sd &lt;- &#39;fill this in&#39; critical_t &lt;- &#39;fill this in&#39; ci_95 &lt;- &#39;fill this in&#39; return(ci_95) } sim_conf_int(n = 100, alpha = 0.25) ## [1] &quot;fill this in&quot; When your function is complete, you can use the following code to run your function 100 times and plot the results. many_confidence_intervals &lt;- function(num_simulations, n, alpha) { ## args: ## - num_simulations: the number of simulated confidence intervals ## - n: the number of observations in each simulation that will pass ## into your `sim_conf_int` function ## - alpha: the confidence interval that you will pass into ## your `sim_conf_int` function results &lt;- NULL for(i in 1:num_simulations) { interval = sim_conf_int(n, alpha) results = rbind(results, c(interval[1], interval[2], interval[1]&lt;0 &amp; interval[2]&gt;0)) } resultsdf = data.frame(results) names(resultsdf) = c(&quot;low&quot;, &quot;high&quot;, &quot;captured&quot;) return(resultsdf) } n = 20 confidence_intervals = many_confidence_intervals(100, n, .05) plot_many_confidence_intervals &lt;- function(c) { plot(NULL, type = &quot;n&quot;, xlim = c(1,100), xlab = &#39;Trial&#39;, ylim = c(min(c$low), max(c$high)), ylab=expression(mu),pch=19) abline(h = 0, col = &#39;gray&#39;) abline(h = qt(0.975, n-1)/sqrt(n), lty = 2, col = &#39;gray&#39;) abline(h = qt(0.025, n-1)/sqrt(n), lty = 2, col = &#39;gray&#39;) points(c$high, col = 2+c$captured, pch = 20) points(c$low, col = 2+c$captured, pch = 20) for(i in 1:nrow(c)) { lines(c(i,i), c(c$low[i],c$high[i]), col = 2+c$captured[i], pch = 19) } title(expression(paste(&quot;Simulation of t-Confidence Intervals for &quot;, mu, &quot; with Sample Size 20&quot;))) legend(0,-.65, legend = c(expression(paste(mu,&quot; Captured&quot;)), expression(paste(mu,&quot; Not Captured&quot;))), fill = c(3,2)) } # plot_many_confidence_intervals(confidence_intervals) How many of the simulated confidence intervals contain the true mean, zero? Suppose you run a single study. Based on what you’ve just written above, why is it incorrect to say that, “There is a 95% probability that the true mean is inside this (single) confidence interval”? "],["maximum-likelihood-example-printers.html", "14.10 Maximum Likelihood Example: Printers", " 14.10 Maximum Likelihood Example: Printers Part I Suppose that you’ve got a particular sequence of values: \\({1, 0, 0, 1, 0, 1, 1, 1, 1, 1}\\) that indicate whether a printer any particular time you try to print. You have data from the last 10 times you tried. Question: What is the probability (\\(p\\)) that the printer jams on the next print job? bbc, office space Part II The data resembles draws from a Bernoulli distribution. However, even if we want to model this as a Bernoulli distribution, we do not know the value of the parameter, \\(p\\). 1- Define your random variable. 2- Write down the likelihood function 3- If it will make the math easier, log the likelihood function. 4- Path 1: Maximize the likelihood using calculus 5- Path 2: Maximize using numeric methods. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
