[
  {
    "objectID": "02-random-variables.html",
    "href": "02-random-variables.html",
    "title": "2¬† Defining Random Variables",
    "section": "",
    "text": "2.1 Learning Objectives\nAt the end of this week‚Äôs course of study (which includes the async, sync, and homework) students should be able to\nThis week‚Äôs materials are theoretical tooling to build toward one of the first notable results of the course, conditional probability. This is the idea that, if we know that one event has occurred, we can make a conditional statement about the probability distribution for another, dependent distribution.",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Defining Random Variables</span>"
    ]
  },
  {
    "objectID": "02-random-variables.html#learning-objectives",
    "href": "02-random-variables.html#learning-objectives",
    "title": "2¬† Defining Random Variables",
    "section": "",
    "text": "Remember that random variable are neither random, or variables, but instead that they are objects thare are a foundation that we can use to reason about a world.\nUnderstand that the intuition developed by the use of set-theory probability maps into the more expressive space of random variables\nApply the appropriate mathematical transformations to move between joint, marginal, and conditional distributions.",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Defining Random Variables</span>"
    ]
  },
  {
    "objectID": "02-random-variables.html#introduction-to-the-materirals",
    "href": "02-random-variables.html#introduction-to-the-materirals",
    "title": "2¬† Defining Random Variables",
    "section": "2.2 Introduction to the Materirals",
    "text": "2.2 Introduction to the Materirals\nFrom the axioms of probability, it is possible to build a whole, expressive modeling system (that need not be grounded at all in the minutia of the world). With this probability model in place, we can describe how frequently events in the random variable will occur. When variable are dependent upon each other, we can utilize information that is encoded in this dependence in order to make predictions that are closer to the truth than predictions made without this information.\nThere is both a beauty and a tragedy when reasoning about random variables: we describe random variables using their joint density function.\n\nThe beauty is that by reasoning with such general objects ‚Äì the definitions that we create, and the theorems that we derive in this section of the course ‚Äì produce guarantees that hold in every case, no matter the function that stands in for the joint density function. We will compute several examples of specific functions to provide a chance to reason about these objects and how they ‚Äúwork‚Äù.\nThe tragedy is that in the ‚Äúreal world‚Äù, the world where we are going to eventually going to train and deploy our models, we are never provided with this joint density function. Because we don‚Äôt have access to the joint density function, in later weeks we will try to produce estimates using data. The simpler the estimate, the less data we need; the fuller the representation of the joint density function we desire, the more data we need.\n\nPerhaps this is the creation myth for probability theory: in a perfect world, we can produce a perfect result. But, in the ‚Äúfallen‚Äù world of data, we will only be able to produce approximations.",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Defining Random Variables</span>"
    ]
  },
  {
    "objectID": "02-random-variables.html#class-announcements",
    "href": "02-random-variables.html#class-announcements",
    "title": "2¬† Defining Random Variables",
    "section": "2.3 Class Announcements",
    "text": "2.3 Class Announcements\n\nHomework\n\nYou should have turned in your first homework. The solution set for this homework is scheduled to be released to you in Thursday at 2:00p. The solution set contains a full explanation of how we solved the questions posed to you. You can expect that feedback for this homework will be released back to you within seven days.\nYou can start working on your second homework when we are out of this class.\n\n\n\nStudy Groups\nIt is a very good idea for you to create a recurring time to work with a set of your classmates. Working together will help you solve questions more effectively, quickly, and will also help you to learn how to communicate what you do and do not understand about a problem to a group of collaborating data scientists. And, working together with a group will help you to find people who share data science interests with you.\n\n\nCourse Resources\nThere are several resources to support your learning. A learning object last week was that you would be introduced to each of these systems. Please continue to make sure that you have access to the:\n\nLibrary VPN to read all of the scholarly content in the known universe, including the course textbook.\nCourse LMS Page",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Defining Random Variables</span>"
    ]
  },
  {
    "objectID": "02-random-variables.html#using-definitions-of-random-variables",
    "href": "02-random-variables.html#using-definitions-of-random-variables",
    "title": "2¬† Defining Random Variables",
    "section": "2.4 Using Definitions of Random Variables",
    "text": "2.4 Using Definitions of Random Variables\n\n2.4.1 Random Varaible\nWhat is a random variable? Does this definition help you?\n\nA random variable is a function \\(X : \\Omega \\rightarrow \\mathbb{R},\\) such that \\(\\forall r \\in \\mathbb{R}, \\{\\omega \\in \\Omega: X(\\omega) \\leq r\\} \\in S\\).\n\nSomeone, please, read that without using a single ‚Äúomega‚Äù, \\(\\mathbb{R}\\), or other jargon terminology. Instead, someone read this aloud and tell us what each of the concepts mean.\n\n\n\n\n\n\nNote\n\n\n\nYou might notice that the \\(X(\\omega) \\leq r\\) feels kind of weird; why isn‚Äôt it just \\(X(\\omega) = r\\)? After all, this is a mapping from an outcome, \\(\\omega \\in \\Omega\\) to a real number, right? So, why not just be direct about it? The answer is a real deep-dive, and one that is better suited to a formal measure theory course.\nThe short answer, is that the we use this \\(\\leq r\\) range because we‚Äôre using a Boreal \\(\\sigma\\) algebra to constrain the sets that have probability measures. Why this constraint? If we don‚Äôt weird stuff can happen. Like, real weird things: you can split a sphere into pieces and create two new spheres of equal volume from the pieces (Banch-Tarski Paradox).\n\n\nThe goal of writing with math symbols like this is to be absolutely clear what concepts the author does and does not mean to invoke when they write a definition or a theorem. In a very real sense, this is a language that has specific meaning attached to specific symbols; there is a correspondence between the mathematical language and each of our home languages, but exactly what the relationship is needs to be defined into each student‚Äôs home language.\n\n\nWhat are the key things that random variables allow you to accomplish?\n\nSuppose that you were going to try to make a model that predicts the probability of winning ‚Äúbig money‚Äù on a slot machine. Big money might be that you get üçíüçíüçí. Can you do math with üçí?\nSuppose that you wanted to build a chatbort that uses a language model so that you don‚Äôt have to do your homework anymore. How would you go about it? Can you do math on words or concepts?\nSuppose you want to direct class support to students in 203, but their grades are scored [A, A-, ..., C+] and features include prior statistics classes grades, also scored A, A-, ..., C+].",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Defining Random Variables</span>"
    ]
  },
  {
    "objectID": "02-random-variables.html#pieces-of-a-random-variable",
    "href": "02-random-variables.html#pieces-of-a-random-variable",
    "title": "2¬† Defining Random Variables",
    "section": "2.5 Pieces of a Random Variable",
    "text": "2.5 Pieces of a Random Variable\n\nA random variable is a function \\(X : \\Omega \\rightarrow \\mathbb{R},\\) such that \\(\\forall r \\in \\mathbb{R}, \\{\\omega \\in \\Omega\\}: X(\\omega) \\leq r\\} \\in S\\).\n\nThere are two key pieces that must exist for every random variable. What are these pieces? The new piece is provided to us in Definition 1.2.1 Random Variable (on page 16). The older piece (from last week) that is now useful is a part of the Kolmogorov Axiom, the probabilty triple: \\((\\Omega, S, P)\\) where \\(\\Omega\\) is a sample space, \\(S\\) is an event space, and \\(P\\) is a probability-measure.\n\n\n\n\n\nSuppose that a random variable is simple and discrete. For concreteness, you could think of this random variable as the answer to the question, ‚ÄúIs the grass wet outside?‚Äù.\n\nWhat is the sample space?\nWhat is a sensible function that you might use to map from the sample space to real values?\nWhat is a sensible function that you might use to map from the sample space to real values? (A student well-seasoned in Maths might use (and define for the rest of the class) the concept of a bijective function).\nIf you simply had the values that the random variable function maps to are you guaranteed to be able to describe the entire sample space? Why or why not?\nHow would you go about determining the probability mass function for this random variable?\n\n\n\n2.5.1 Functions of Functions\n\nWhy do we say that random variables are functions? Is there some useful property of these being functions rather than any other quantity? What else could they be if not a function?\n\nWhat about a function of a random variable, which is a function of a function.\n\nLet \\(g : U \\rightarrow \\mathbb{R}\\) be some function, where \\(X(\\Omega) \\subseteq U \\subseteq \\mathbb{R}\\). Then, if \\(g \\circ X : \\Omega \\rightarrow \\mathbb{R}\\) is a random variable, we say that \\(g\\) is a function of X and write \\(g(X)\\) to denote the random variable \\(g \\circ X\\).\n\nIf a random variable is a function from the real world, or the sample space, or the outcome space to a real number, then what does it mean to define a function of a random variable?\n\nAt what point does this function work? Does this function change the sample space that is possible to observe? Or, does this function change the real-number that each outcome points to?\n\n\nSuppose that you are doing some image processing work. To keep things simple, that you are doing image classification in the style of the MNIST dataset.\n\nCan someone describe what this task is trying to accomplish?\nHas anyone done work like this?\n\nHowever, suppose that rather than having good clean indicators for whether a pixel is on or off, instead you have weak indicators ‚Äì there‚Äôs a lot of grey. A lot of the cells are marked in the range \\(0.2 - 0.3\\).\n\nHow might creating a function that re-maps this grey into more extreme values help your model?\nIs it possible to ‚Äúblur‚Äù events that are in the outcome space? Does this ‚Äúblurring‚Äù meet the requirements of a function of a random variable, as provided above?\n\n\n\n\n2.5.2 Probability Density Functions and Cumulative Distribution Functions\n\nWhat is a probability mass function?\nWhat do the Kolmogorov Axioms mean must be true about any probability mass function (pmf)?\n\n\nYou should try driving in Berkeley some time. It is a trip! Without being deliberately ageist, the city is full of ageing hippies driving beater Subaru Outbacks and making what seem to be stochastic right-or-left turns to buy incense, pottery, or just sourdough bread.\nSuppose that you are walking to campus, and you have to cross 10 crosswalks, each of which are spaced a block apart. Further, suppose that as you get closer to campus, there are fewer aging hippies, and therefore, there is decreasing risk that you‚Äôre hit by a Subaru as you cross the street. Specifically, and fortunately for our math, the risk of being hit decreases linearly with each block that you cross.\nFinally, campus provides you with the safety reports from last year, and reports that there were 55 student-Subaru incidents last year, out of 10,000 student-crosswalk crossings.\n\nWhat is the pmf for the probability that you are involved in a student-Subaru incident as you walk across these 10 blocks? What sample space, \\(\\Omega\\) is appropriate to represent this scenario?\nSuppose that you don‚Äôt leave your house ‚Äì this is a remote program after all! What is your cumulative probability of being involved in a student-subaru incident?\nWhat is the cumulative probability cmf for the probability that you are involved in a student-Subaru incident?\nSuppose that you live three blocks from campus, but your classmate lives five blocks from campus. What is the difference in the cumulative probability?\nHow would you describe the cumulative probability of being hit as you walk closer to campus? That is, suppose that you start 10 blocks away from campus, and are walking to get closer. Is your cumulative probability of being hit on your way to campus increasing or decreasing as you get closer to campus?\nHow would you describe the cumulative probability of being hit as you walk further from campus? That is, suppose that you start on campus, and you‚Äôre walking to a bar after classes. Is your cumulative probability of being hit on your way away from campus increasing or decreasing as you get further from campus?",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Defining Random Variables</span>"
    ]
  },
  {
    "objectID": "02-random-variables.html#discrete-continuous-random-variables",
    "href": "02-random-variables.html#discrete-continuous-random-variables",
    "title": "2¬† Defining Random Variables",
    "section": "2.6 Discrete & Continuous Random Variables",
    "text": "2.6 Discrete & Continuous Random Variables\nWhat, if anything is fundamentally different between discrete and continuous random variables? As a way of starting the conversation, consider the following cases:\n\nSuppose \\(X\\) is a random variable that describes the time a student spends on w203 homework 1.\n\nIf you have only granular measurement ‚Äì i.e.¬†the number of nights spent working on the homework ‚Äì is this discrete or continuous?\nIf you have the number of hours, is it discrete or continuous?\nIf you have the number of seconds? Or milliseconds?\n\nIs it possible that \\(P(X = a) = 0\\) for every point \\(a\\)? For example, that \\(P(X = 3600) = 0\\).\nDoes one of these measures have more information in it than another?\n\nHow are measurement choices that we make as designers of information capture systems ‚Äì i.e.¬†the machine processes, human processes, or other processes that we are going to work with as data scientists ‚Äì reflected in both the amount of information that is gathered, the type of information that is gathered, and the types of random variables that are manifest as a result?",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Defining Random Variables</span>"
    ]
  },
  {
    "objectID": "02-random-variables.html#moving-between-pdf-and-cdf",
    "href": "02-random-variables.html#moving-between-pdf-and-cdf",
    "title": "2¬† Defining Random Variables",
    "section": "2.7 Moving Between PDF and CDF",
    "text": "2.7 Moving Between PDF and CDF\nThe book defines pmf and cmf first as a way of developing intuition and a way of reasoning about these concepts. It then moves to defining continuous density functions, which is many ways are easier to work with although they lack the means of reasoning about them intuitively. Continuous distributions are defined in the book, and more generally, in terms of the cdf, which is the cumulative distribution function. There are technical reasons for this choice of definition, some of which are signed in the footnotes on the page where the book presents it.\nMore importantly for this course, in Definition 1.2.15 the book defines the relationship between cdf and pdf in the following way:\n\nFor a continuous random variable \\(X\\) with CDF \\(F\\), the probability density function of \\(X\\) is\n\\[\n  f(x) = \\left. \\frac{d F(u)}{du} \\right|_{u=x}, \\forall x \\in \\mathbb{R}.\n\\]\nThe implies, further, that for a random variable \\(X\\) with PDF \\(f\\), the cumulative density function of \\(X\\) is:\n\\[\nF(x) = \\int f(x) dx, \\forall x \\in \\mathbb{R}.\n\\]\n\n\nHow does this definition, which relates pdf and cdf by a means of differentiation and integration, fit with the ideas that we just developed in the context of walking to and from campus?\n\n\nSuppose that you learn than a particular random variable, \\(X\\) has the following function that describes its pdf, \\(f_{x}(x) = \\frac{1}{10}x\\). Also, suppose that you know that the smallest value that is possible for this random variable to obtain is 0.\n\nWhat is the CDF of \\(X\\)?\nWhat is the maximum possible value that \\(x\\) can obtain? How did you develop this answer, using the Kolmogorov axioms of probability?\nWhat is the cumulative probability of an outcome up to 0.5?\nWhat is the probability of an outcome between 0.25 and 0.75? Produce an answer to this in two ways:\n\nUsing the \\(PDF\\)\nUsing the \\(CDF\\)",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Defining Random Variables</span>"
    ]
  },
  {
    "objectID": "02-random-variables.html#joint-density",
    "href": "02-random-variables.html#joint-density",
    "title": "2¬† Defining Random Variables",
    "section": "2.8 Joint Density",
    "text": "2.8 Joint Density\nWorking with a single random variable helps to develop our understanding of how to relate the different features of a pdf and a cdf through differentiation and integration. However, there‚Äôs not really that much else that we can do; and, there is probably very little in our professional worlds that would look like a single random variable in isolation.\nWe really start to get to something useful when we consider joint density functions. Joint density functions describe the probability that both of two random variables. That is, if we are working with random variables \\(X\\) and \\(Y\\), then the joint density function provides a probability statement for \\(P(X \\cap Y)\\).\nIn this course, we might typically write this joint density function as \\(f_{X,Y}(x,y) = f(\\cdot)\\) where \\(f(\\cdot)\\) is the actual function that represents the joint probability. The \\(f(\\cdot)\\) means, essentially, ‚Äúsome function‚Äù where we just have not designated the specifics of the function; you might think of this as a generic function.\n\n2.8.1 Example: Uniform Joint Density\nSuppose that we know that two variables, \\(X\\) and \\(Y\\) are jointly uniformly distributed within the the support \\(x \\in [0,4], y \\in [0,4]\\). We have a requirement, imposed by the Kolmogorov Axioms that all probabilities must be non-zero, and that the total probability across the whole support must be one.\n\nCan you use these facts to determine answers to the following:\n\nWhat kind of shape does this joint pdf have?\nWhat is the specific function that describes this shape?\nIf you draw this shape on three axes, and \\(X\\), and \\(Y\\), and a \\(P(X,Y)\\), what does this plot look like?\nHow do you get from the joint density function, to a marginal density function for \\(X\\)?\nHow do you get form the joint density function, to a marginal density function for \\(Y\\)?\nHow do you get from these marginal density functions of \\(X\\) and \\(Y\\) back to the joint density? Is this always possible?\n\n\n\n\n2.8.2 Examples: Thinking Through Many Plots\n\n\n\n\n\n\n\n\n2.8.3 Triangle Math\nAfter considering the intuition for the triangle distribution, do the following: Write down the function that accords with the figure that you‚Äôre seeing above.1\n\nWhat is a full statement of the PDF of this image?\nWhat is the marginal distribution of \\(X\\), \\(f_{X}(x)\\)?\nWhat is the marginal distribution of \\(Y\\), \\(f_{Y}(y)\\)?\nUsing the definition of independence, are \\(X\\) and \\(Y\\) independent of each other?\nWhat is the CDF of \\(X\\), \\(F_{X}(x)\\)?\n\n\n\n2.8.4 Saddle Sores\nSuppose that you know that two random variables, \\(X\\) and \\(Y\\) are jointly distributed with the following pdf:\n\\[\nf_{X,Y}(x,y) =\n  \\begin{cases}\n    a * x^{2} * y^{2}, & 0 &lt; x &lt; 1, 0 &lt; y &lt; 1 \\\\\n    0, & \\text{otherwise.}\n  \\end{cases}\n\\]\nThis joint pdf is similar to the pdf that you can visualize above, under the distribution called ‚Äúsaddle‚Äù. The difference between this function and the image above is that the function bounds the with support of \\(x\\) and \\(y\\) on the range \\([0,1]\\). This is to make the math easier for us in the next step.\n\nCan you use these facts to determine the following?\n\nWhat value of \\(a\\) makes this a valid joint pdf?\nWhat is the marginal pdf of \\(x\\)? That is, what is \\(f_{x}(x)\\)?\nWhat is the conditional pdf of \\(X\\) given \\(Y\\)? That is, what is \\(f_{x|y}(x,y)\\)?\nGiven these facts, would you say that \\(X\\) and \\(Y\\) are dependent or independent?\nIf the support for this joint distribution were instead \\([0,4]\\) (rather than \\([0,1]\\)), how would the shape of the distribution change?",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Defining Random Variables</span>"
    ]
  },
  {
    "objectID": "02-random-variables.html#computing-different-distributions.",
    "href": "02-random-variables.html#computing-different-distributions.",
    "title": "2¬† Defining Random Variables",
    "section": "2.9 Computing Different Distributions.",
    "text": "2.9 Computing Different Distributions.\nSuppose that random variables \\(X\\) and \\(Y\\) are jointly continuous, with joint density function given by,\n\\[\nf(x,y) =\n  \\begin{cases}\n    c, & 0 \\leq x \\leq 1, 0 \\leq y \\leq x \\\\\n    0, & otherwise\n\\end{cases}\n\\]\nwhere \\(c\\) is a constant.\n\nDraw a graph showing the region of the X-Y plane with positive probability density.\nWhat is the constant \\(c\\)?\nCompute the marginal density function for \\(X\\). (Be sure to write a complete expression)\nCompute the conditional density function for \\(Y\\), conditional on \\(X=x\\). (Be sure to specify for what values of \\(x\\) this is defined)",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Defining Random Variables</span>"
    ]
  },
  {
    "objectID": "02-random-variables.html#conditional-probability",
    "href": "02-random-variables.html#conditional-probability",
    "title": "2¬† Defining Random Variables",
    "section": "2.10 Conditional Probability",
    "text": "2.10 Conditional Probability\nConditional probability is incredible. In fact, without exaggeration, almost all of data science is an exercise in making statements about conditional probability distributions. Don‚Äôt believe us?\n\nWhat is the goal of a ‚Äúcustomer churn‚Äù model or a conversion model?\nWhat is the goal of a language-completion model?\nWhat is the goal of flight-departures model?\n\n\nIf we possessed the whole information about a process; if we had the CDF that governed probability of occurrences, what kinds of statements would we be able to make? Would we even need data?\n\n\nUsing the distribution above, produce a statement of conditional probability, \\(f_{Y|X}(y|x)\\).",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Defining Random Variables</span>"
    ]
  },
  {
    "objectID": "02-random-variables.html#visualizing-distributions-via-simulation",
    "href": "02-random-variables.html#visualizing-distributions-via-simulation",
    "title": "2¬† Defining Random Variables",
    "section": "2.11 Visualizing Distributions Via Simulation",
    "text": "2.11 Visualizing Distributions Via Simulation\nTo this point in the course, we have focused on concepts in ‚Äúthe population‚Äù with no reference to samples. This is on purpose! We want to develop the theory that defines the best possible predictor if we knew everything (if we know formula of the function that maps from \\(\\omega \\rightarrow \\mathbb{R}\\), and we know the probability of each \\(\\omega \\in \\Omega\\) then we know everything). Beginning in week 5 of the course, we will talk about ‚Äúapproximating‚Äù (which we will call estimating) this best possible predictor with a limited sample of data.\nHowever, at this point, to help build your working understanding, or intuition, for what is happening, we are going to work on a way to simulate draws from a population. In some places, people might refer to these as Monte Carlo methods ‚Äì this is because the method was developed by von Neumann & Ulam during World War II, and they needed a way to talk about it using a code name. They chose Monte Carlo after a famous casino in Monaco.\n\n2.11.1 Example: The Uniform Distribution\n\nYou: ‚ÄúGosh. There sure are a lot of examples that use the uniform distribution. That must be a really important statistical distribution.‚Äù\nInstructor: ‚ÄúNah. Not really. We‚Äôre just using the uniform a bunch so that we don‚Äôt get too lost in doing math while we‚Äôre working with these concepts.‚Äù\n\nWe‚Äôll start with a simple uniform distribution, but then we‚Äôll make it a little more complex in a moment.\nWe can use R to simulate draws from a probability distribution function by providing it with the name of the distribution that we‚Äôre considering, the support of that distribution, or other features of the distribution. In the case of the uniform, the entire distribution is can be described just from it support.\nSo, suppose that you had a uniform distribution that had positive probability on the range \\([1.1, 4.3]\\). Why these? No particular reason. That is, suppose\n\\[\nf_{X}(x) =\n  \\begin{cases}\n    a & 1.1 \\leq x \\leq 4.3 \\\\\n    0 & otherwise\n  \\end{cases}\n\\]\nWhat does this distribution ‚Äúlook like‚Äù? Because it is a uniform, you might have a sense that it will be a horizontal line. But, what is the height of that line? Aha! We could do the math to figure it out, or we could generate an approximation using a simulation.\nIn the code below, we are going to create an object called samples_uniform that stores the results of the runif function call.\n\nsamples_uniform &lt;- runif(n=1000, min=1.1, max=4.3)\n\nWhat is happening inside runif?\nWhen you‚Äôre writing you own code, you can pull up the documentation for this (and any) function using a question mark, i.e.¬†?, followed by the function name ‚Äì ?runif.\nBut, we can speed this up slightly by simply telling you that n is the number of samples to take from the population; min is the low-end of the support, and max is the high-end of the support.\nIf we look into this object, we can see the results of the function call. Below, we will show the first \\(20\\) elements of the samples_uniform object.\n\nsamples_uniform[1:20]\n\n [1] 2.253276 3.474887 4.194507 3.791078 2.341289 2.754651 1.531660 1.369586\n [9] 2.694481 2.529188 2.693655 3.330845 1.599786 2.577170 2.236188 3.519088\n[17] 1.189404 1.400409 1.750101 2.453439\n\n\n(Notice that R is a \\(1\\) index language (python is a zero-index language).)\nWith this object created, we can plot a density of the data and then learn from this histogram what the pdf looks like.\n\nplot_full_data &lt;- ggplot() + \n  aes(x=1:length(samples_uniform), y=samples_uniform) + \n  geom_point()  + \n  labs(\n    title = 'Showing the Data', \n    y     = 'Sample Value', \n    x     = 'Index')\n\nplot_density &lt;- ggplot() + \n  aes(x=samples_uniform) + \n  geom_density(bw=0.1)   + \n  labs(\n    title = 'Showing the PDF', \n    y     = 'Probability of Drawing Value', \n    x     = 'Sample Value')\n\n(plot_full_data | (plot_density + coord_flip())) / \n  plot_density \n\n\n\n\n\n\n\n\nInteresting. From what we can see here, there does not appear to be any discernible pattern. This leaves us with two options: either, we might reduce the resolution that we‚Äôre using to view this pattern, or we might take more samples and hold the resolution constant. Below, two different plots show these differing approaches, and are very explicit about the code that creates them.\n\nsamples_uniform_moar &lt;- runif(n = 1000000, min = 1.1, max = 4.3)\n\n\nplot_low_res &lt;- ggplot()   + \n  aes(x = samples_uniform) + \n  geom_density(bw = 0.1)   + \n  lims(y = c(0,0.4))       + \n  labs(title = 'Low Res, Low Data')\n\nplot_high_res &lt;- ggplot()       + \n  aes(x = samples_uniform_moar) + \n  geom_density(bw = 0.01)       + \n  lims(y = c(0,0.4))            + \n  labs(title = 'High Res, More Data')\n\nplot_low_res | plot_high_res\n\n\n\n\n\n\n\n\n\n\n2.11.2 Example: The Normal Distribution\nFolks might have some prior beliefs about the Normal distribution. Don‚Äôt worry, we‚Äôll cover this later in the course. But, this is the distribution that you have in mind when you‚Äôre thinking of a ‚Äúbell curve‚Äù.\nWe can use the same method to visualize a normal distribution as we did for a uniform distribution. In this case, we would issue the call rnorm, together with the population parameters that define the population. At this point in the course, we do not expect that you will know these (and, actually memorizing these facts are not a core focus of the course), but you can look them up if you like. Truthfully, statistics wikipedia is very good.\nDo do you notice anything about the runif and the rnorm calls that we have identified? Both seem to name the distribution: \\(unif \\approx uniform\\) and \\(norm \\approx normal\\), but prepened with a r? This is for ‚Äúrandom draw‚Äù.\nBase R is loaded with a pile of basic statistics distributions, which you can look into using ?distributions.\n\nsamples_normal &lt;- rnorm(n = 100000, mean = 18, sd = 4)\n\nLike before, we could look at the first \\(20\\) of these samples.\n\nsamples_normal[1:20]\n\n [1] 24.497334 15.348211 26.349048 22.159840 26.591070 21.751742 21.880909\n [8] 17.109993 18.808287 18.666536 23.538855 17.679535 13.620293 17.360174\n[15]  6.916686 25.867951 17.688190  9.987599 11.467163 20.864328\n\n\nAnd, from here we could visualize this distribution.\n\nggplot() + \n  aes(x = samples_normal) + \n  geom_density() + \n  labs(title = 'Visualization of this Normal Distribution')\n\n\n\n\n\n\n\n\n\n2.11.2.1 Combining This Ability\n\nConsider three random variables \\(A, B, C\\). Suppose,\n\\[\n\\begin{aligned}    \n  A & \\sim Uniform(min=1.1, max=4.3) \\\\     \n  B & \\sim Normal(mean=18, sd=4)     \\\\     \n  C &= A + B  \\end{aligned}\n\\]\nAnd, suppose that \\(B\\) is a random variable that is described by the normal density that we considered earlier. Suppose that \\(A\\) and \\(B\\) are independent of each other.\nFinally, suppose that \\(C = A + 2B\\).\nWhat does \\(C\\) look like?\n\nAlthough this is a simple function applied to a random variable ‚Äì a legal move ‚Äì the math would be tedious. What if, instead, one used this simulation method to get a sense for the distribution?\n\nsamples_A &lt;- runif(n = 10000, min = 1.1, max = 4.3)\nsamples_B &lt;- rnorm(n = 10000, mean = 18, sd = 4)\n\nsamples_C &lt;- samples_A + samples_B\n\n\nplot_C &lt;- ggplot() + \n  aes(x = samples_C) + \n  geom_density()\n\nplot_C_and_A_and_B &lt;- ggplot()   + \n  geom_density(aes(x = samples_A), color = '#003262') + \n  geom_density(aes(x = samples_B), color = '#FDB515') + \n  geom_density(aes(x = samples_C), color = 'darkred')\n\nplot_C_and_A_and_B",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Defining Random Variables</span>"
    ]
  },
  {
    "objectID": "02-random-variables.html#review-of-terms",
    "href": "02-random-variables.html#review-of-terms",
    "title": "2¬† Defining Random Variables",
    "section": "2.12 Review of Terms",
    "text": "2.12 Review of Terms\nRemember some of the key terms we learned in the async:\n\nJoint Density Function\nConditional Distribution\nMarginal Distribution",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Defining Random Variables</span>"
    ]
  },
  {
    "objectID": "02-random-variables.html#footnotes",
    "href": "02-random-variables.html#footnotes",
    "title": "2¬† Defining Random Variables",
    "section": "",
    "text": "Notice, that in general, this kind of curve fitting isn‚Äôt really a common data science task. Instead, this is just a learning task that lets the class assess their understanding of the definitions of random variables.‚Ü©Ô∏é",
    "crumbs": [
      "Probabilty Theory",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Defining Random Variables</span>"
    ]
  }
]