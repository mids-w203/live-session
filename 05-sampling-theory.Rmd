```{r load unit 05 packages, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
library(tidyverse)
theme_set(theme_minimal())
```

# Learning from Random Samples 

![south hall](./images/south_hall.jpeg)

This week, we're coming into the big turn in the class, from probability theory to sampling theory. 

In the probability theory section of the course, we developed the *theoretically* **best** possible set of models. Namely, we said that if our goal is to produce a model that minimizes the Mean Squared Error that *expectation* and *conditional expectation* are as good as it gets. That is, if we only have the outcome series, $Y$, we cannot possibly improve upon $E[Y]$, the expectation of the random variable $Y$. If we have additional data on hand, say $X$ and $Y$, then the best model of $Y$ given $X$ is the conditional expectation, $E[Y|X]$.

We have also said that because this conditional expectation function might be complex, and hard to inform with data, that we might also be interested in a principled simplification of the conditional expectation function -- the simplification that requires our model be a line. 

With this simplification in mind, we derived the linear system that produces the minimum MSE:  the ratio of covariance between variables to variance of the predictor: 

$$
  \beta_{BLP} = \frac{Cov[Y,X]}{V[X]}. 
$$ 

We noted, quickly, that the simple case of only two variables -- an outcome and a single predictor -- generalizes nicely into the (potentially very) many dimensional case. If the many-dimensional $BLP$ is denoted as $g(\mathbf{X}) = b_{0} + b_{1}X_{1} + \dots + b_{k}X_{k}$, then we can arrive at the sloped between one particular predictor, $X_{k}$, and the outcome, $Y$, as: 

$$
  b_{k} = \frac{\partial g(\mathbf{X})}{\partial X_{k}}. 
$$




, toward the use of data to actually estimate approximations of the best models. 




## Goals, Framework, and Learning Objectives 

### Class Announcements

::: {.slide}
- You're done with probability theory. **Yay!**
- You're also done with your first test. **Double Yay!**
- We're going to have a second test in a few weeks. Then we're done testing for the semester **Yay?**
::: 

### Learning Objectives 

::: {.slide}
At the end of this week, students will be able to 

1. **Understand** what iid sampling is, and evaluate whether the assumption of iid sampling is sufficiently plausible to engage in frequentist modeling.
2. **Appreciate** that with iid sampling, summarizing functions of random variables are, themselves, random variables with probability distributions and values that they obtain.
3. **Recall** the definition of an estimator, 
4. **Recall** definition of an estimator, **state** and **understand** the desirable properties of estimators, and **evaluate** whether an estimator possesses those desirable properties. 
5. **Distinguish** between the concepts of {expectation & sample mean}, {variance & unbiased sample variance estimator, sampling-based variance in the sample mean}. 
:::

### Roadmap 

#### Where We're Going -- Coming Attractions

::: {.slide}
- We're going to start bringing data into our work 
- First, we're going to develop a testing framework that is built on sampling theory and reference distributions: these are the **frequentist tests**.
- Second, we're going to show that OLS regression is the sample estimator of the BLP. This means that OLS regression produces estimates of the BLP that have known convergence properties. 
- Third, we're going combine the frequentist testing framework with OLS estimation to produce a full regression testing framework.
::: 

#### Where We've Been -- Random Variables and Probability Theory

Statisticians create a model (also known as the population model) to represent the world. This model exists as joint probability densities that govern the probabilities that any series of events occurs at the same time. This joint probability of outcomes can be summarized and described with lower-dimensional summaries like the expectation, variance, covariance. While the expectation is a summary that contains information on about one marginal distribution (i.e. the outcome we are interested in) we can produce predictive models that update, or *condition* the expectation based on other random variables. This summary, the **conditional expectation** is the best possible (measured in terms of minimizing mean squared error) predictor of an outcome. We might simplify this conditional expectation predictor in many ways; the most common is to simplify to the point that the predictor is constrained to be a line or plane. This is known as the Best Linear Predictor. 

#### Where we Are

- We want to fit models -- use data to set their parameter values.
- A sample is a set of random variables
- Sample statistics are functions of a sample, and they are random variables
- Under iid and other assumptions, we get useful properties: 
  - Statistics may be consistent estimators for population parameters
  - The distribution of sample statistics may be asymptotically normal

## Key Terms and Assumptions

### IID
For each scenario, is the IID assumption plausible?

- Call a random phone number.  If someone answers, interview all persons in the household.  Repeat until you have data on 100 people.
- Call a random phone number, interview the person if they are over 30.  Repeat until you have data on 100 people.
- Record year-to-date price change for 20 largest car manufacturers.
- Measure net exports per GDP for all 195 countries recognized by the UN.

### Definitions

Define each of the following:

- Sample
- Sample Statistic
- Estimator
- Bias
- Efficiency 
- Consistency
- Convergence in Probability 
- Convergence in Distribution

## Understanding Sampling Distributions 

- Let $X$ be a Bernoulli random variable representing an unfair coin with $P(X=1) = 0.7$.
- You have an iid sample of size 2,  $(X_1,X_2)$.
- Compute the sampling distribution of $\overline X = \frac{X_1+X_2}{2}$.

```{r make axes, echo = FALSE, fig.height=4}
ggplot() + 
  aes(x = c(0,5), y = c(0,1)) + 
  geom_blank() + 
  labs(
    title = 'Distribution of Bernoulli RV', 
    x     = 'Value of X', 
    y     = 'Probability of that Value of X'
  )
```

**Questions:**

- Explain the difference between a population distribution and the sampling distribution of a statistic.
- As we toss more and more coins, $\overline X_{(100)} \rightarrow \overline X_{(10000)}$ what will the value of $\overline X$ get closer to? What law generates this, and why does this law generate this result?
- Why do we want to know things about the sampling distribution of a statistic?

## Uncertainty

**Which Result is Better?**

- Suppose that you measure salary data among individuals who try different strategies
- Report out in the following table: 

|                    | Early Rising | Mindfulness Retreat | MIDS Degree |
|--------------------|--------------|---------------------|-------------|
| Increase in Salary |    \$1020    |            \$5130   |      \$9200 |
| $SE$               |   (\$350)    |          (\$4560)   |             |
| $N =$              |  1,000       | 77                  | 700         |  

*(Standard errors in parentheses when available)*

## Standard Errors 

Errors in stating standard errors frequently occur. (We wanted to start with what might the most vapid, but also confusing statement possible!)

Standard errors are a statement about the sampling variance of the sample average. But, related to this concept are the ideas of the *Population Variance*, the *Plug-In Estimator for the Sample Variance*, the *Unbiased Sample Variance*, and, finally, the *Sampling Variance of the Sample Average* (i.e the *Standard Error*). 

How are each of these concepts related to one another, and how can we keep them all straight? As a group, fill out the following columns? 

| Population Concept    | Sample Estimator | Estimator Properties | Sampling Variance of Sample Estimator | 
|-----------------------|------------------|----------------------|---------------------------------------|
| Expected Value        |                  |                      |                                       | 
| Population Variance   |                  |                      |                                       | 
| Population Covariance |                  |                      |                                       | 
| CEF                   |                  |                      |                                       | 
| BLP                   |                  |                      |                                       | 

## Write Code to Demo the Central Limit Theorem (CLT)

### Motivating the Central Limit Theorem (CLT)

- Standard Errors tell us a lot about the uncertainty in our statistics
- But we want to say more: 
  - How confident are we that this vitamin has a positive effect?
  - How plausible is a mean income \$1000 below our estimate?
- For these questions, we need to know the sampling distribution of our statistic.
- How is this possible when we don't know the population distribution?

### Sampling from the Bernoulli Distribution in R 
- To demonstrate the CLT, we chose a Bernoulli distribution with parameter $p$.
  - This distribution is very simple
  - This distribution is non-normal, and can be very skewed depending on $p$.
  
- First, set `p=0.5` so your population distribution is symmetric.  Use a variable `n` to represent your sample size.  Initially, set `n=3`.

```{r set parameters}
n <- 3
p <- 0.5
```


### Useful R Commands

**sample() or rbinom()**

- R doesn't have a `bernoulli` function.
- To simulate draws from a Bernoulli variable, you can either: 
  - Use `sample` 
  - Or, use `rbinom` (the Bernoulli distribution is a special case of a binomial distribution.  In this function, `size` refers to a distribution parameter, not the number of draws.)


```{r ways to sample, results = 'hold'}
sample(x=0:1, size=n, replace=TRUE, prob=c(1-p, p))
rbinom(n=n, size=1, prob=p)
```

**replicate()** 

- To repeat an action, you can use `replicate`

```{r replicate log}
replicate(10, log(10))
```

```{r ggplot histogram, message=FALSE, fig.height=4}
ggplot() + 
  aes(x = rnorm(100)) + 
  geom_histogram(bins = '8') 
```

## Exercise
**Part 1**

Throughout this part, we will use fair coins (`p = 0.5`).  

1. Fill in the function below so that it simulates taking n draws from a Bernoulli distribution with parameter p.  This is like tossing n coins at the same time. Use the `mean` function to compute the sample mean -- the average of the number of heads that are showing.  Make sure that when you run it, you return values in $\{0,1/3,2/3,1\}$.

```{r define coin tossing function}
experiment <- function(n, p){

  }
```

2. The sample mean is a random variable. To understand it, use the visualization trick from a few weeks ago.  Use the `replicate` function to run the above experiment 1000 times, and plot a histogram of the results.

```{r conduct the experiment 1000 times}

```

3. If you replicate the experiment enough times, will the distribution ever look normal?  Why or why not?

```{r}

```

4. Use `sd()` to check the standard deviation of the sampling distribution of the mean for `number_of_coins = 3`.  What sample size is needed to decrease the standard deviation by a factor of 10?  Check that your answer is correct.

```{r}

```

**Part 2**

For this part, we'll continue to study a fair coin. 

5. Try different values for the sample size n, and examine the shape of the sampling distribution of the mean.  At what point does it look normal to you?

```{r}

```

**Part 3**

For this part, we'll study a very unfair coin. `p = 0.01`.  

This is an example of a highly skewed random variable.  That roughly means that one tail is a lot longer than the other.

For this activity, you can simply use your eyes to gauge how skewed a distribution is.  If you prefer, you can also use the skewness command in the univar package to measure skewness.  You may hear a rule of thumb that a skewness above 1 or below -1 is a highly skewed distribution.

6. Start with n=3 as before.  What do you notice about the shape of the sampling distribution?

```{r}

```

7. Try different values for the sample size n, and examine the shape of the sampling distribution of the mean.  At what point does it look normal to you?

```{r}

```


### Discussion Questions

1. How does the skewness of the population distribution affect the applicability of the Central Limit Theorem?  What lesson can you take for your practice of statistics?

2. Name a variable you would be interested in measuring that has a substantially skewed distribution.

3. One definition of a heavy tailed distribution is one with infinite variance.  For example, you can use the `rcauchy` command in R to take draws from a Cauchy distribution, which has heavy tails.  Do you think a "heavy tails" distribution will follow the CLT? What leads you to this intuition? 


